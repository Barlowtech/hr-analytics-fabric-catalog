<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Pattern Builder - HR Analytics Fabric Catalog</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Helvetica Neue', sans-serif;
            background: #0F172A;
            color: #F8FAFC;
            line-height: 1.6;
        }
        a { color: #0EA5E9; text-decoration: none; }
        a:hover { text-decoration: underline; }

        .container {
            max-width: 1920px;
            margin: 0 auto;
            height: 100vh;
            display: flex;
            flex-direction: column;
        }

        header {
            background: linear-gradient(135deg, #1E293B 0%, #0F172A 100%);
            border-bottom: 1px solid #334155;
            padding: 1.5rem;
            box-shadow: 0 1px 3px rgba(0,0,0,0.3);
        }

        .header-content {
            max-width: 1920px;
            margin: 0 auto;
        }

        h1 {
            font-size: 1.5rem;
            font-weight: 700;
            margin-bottom: 1rem;
            color: #0EA5E9;
        }

        .tabs {
            display: flex;
            gap: 0;
            border-bottom: 2px solid #334155;
        }

        .tab-btn {
            padding: 0.75rem 1.5rem;
            background: transparent;
            border: none;
            color: #94A3B8;
            font-size: 0.95rem;
            font-weight: 500;
            cursor: pointer;
            border-bottom: 3px solid transparent;
            transition: all 0.2s;
        }

        .tab-btn:hover {
            color: #F8FAFC;
        }

        .tab-btn.active {
            color: #0EA5E9;
            border-bottom-color: #0EA5E9;
        }

        .main-content {
            flex: 1;
            display: flex;
            overflow: hidden;
        }

        .tab-content {
            display: none;
            width: 100%;
            height: 100%;
            overflow: auto;
        }

        .tab-content.active {
            display: flex;
        }

        /* Browse Catalog Tab */
        #browse-tab {
            flex-direction: row;
        }

        .sidebar {
            width: 260px;
            background: #1E293B;
            border-right: 1px solid #334155;
            padding: 1.5rem;
            overflow-y: auto;
            flex-shrink: 0;
        }

        .main-area {
            flex: 1;
            padding: 2rem;
            overflow-y: auto;
            background: #0F172A;
        }

        .section-title {
            font-size: 0.85rem;
            font-weight: 600;
            text-transform: uppercase;
            color: #94A3B8;
            margin-top: 1.5rem;
            margin-bottom: 0.75rem;
            letter-spacing: 0.5px;
        }

        .section-title:first-child {
            margin-top: 0;
        }

        .search-box {
            width: 100%;
            padding: 0.75rem;
            background: #0F172A;
            border: 1px solid #334155;
            border-radius: 0.5rem;
            color: #F8FAFC;
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }

        .search-box:focus {
            outline: none;
            border-color: #0EA5E9;
            box-shadow: 0 0 0 2px rgba(14, 165, 233, 0.1);
        }

        .domain-filter {
            display: flex;
            flex-direction: column;
            gap: 0.5rem;
            margin-bottom: 1.5rem;
        }

        .domain-item {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 0;
            cursor: pointer;
            user-select: none;
        }

        .domain-item input {
            width: 18px;
            height: 18px;
            cursor: pointer;
        }

        .domain-item label {
            flex: 1;
            cursor: pointer;
            font-size: 0.9rem;
        }

        .domain-item .badge {
            font-size: 0.75rem;
            background: #334155;
            color: #94A3B8;
            padding: 0.2rem 0.5rem;
            border-radius: 3px;
            font-weight: 600;
        }

        .toggle-group {
            display: flex;
            flex-direction: column;
            gap: 0.5rem;
            margin-bottom: 1rem;
        }

        .toggle-item {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .toggle-item input {
            width: 18px;
            height: 18px;
            cursor: pointer;
        }

        .toggle-item label {
            cursor: pointer;
            font-size: 0.9rem;
        }

        .clear-all {
            width: 100%;
            padding: 0.75rem;
            background: transparent;
            border: 1px solid #334155;
            color: #0EA5E9;
            border-radius: 0.5rem;
            font-size: 0.9rem;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s;
        }

        .clear-all:hover {
            background: rgba(14, 165, 233, 0.1);
            border-color: #0EA5E9;
        }

        .catalog-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(350px, 1fr));
            gap: 1.5rem;
        }

        @media (max-width: 1200px) {
            .catalog-grid {
                grid-template-columns: repeat(2, 1fr);
            }
        }

        @media (max-width: 768px) {
            .sidebar {
                display: none;
            }
            .catalog-grid {
                grid-template-columns: 1fr;
            }
        }

        .card {
            background: #1E293B;
            border: 1px solid #334155;
            border-radius: 0.75rem;
            overflow: hidden;
            cursor: pointer;
            transition: all 0.3s;
            display: flex;
            flex-direction: column;
        }

        .card:hover {
            border-color: #0EA5E9;
            transform: translateY(-2px);
            box-shadow: 0 8px 16px rgba(14, 165, 233, 0.15);
        }

        .card-color-bar {
            height: 4px;
            width: 100%;
        }

        .card-content {
            padding: 1.5rem;
            flex: 1;
            display: flex;
            flex-direction: column;
        }

        .card-title {
            font-size: 1rem;
            font-weight: 600;
            margin-bottom: 0.75rem;
            color: #F8FAFC;
        }

        .card-summary {
            font-size: 0.85rem;
            color: #94A3B8;
            margin-bottom: 1rem;
            display: -webkit-box;
            -webkit-line-clamp: 3;
            -webkit-box-orient: vertical;
            overflow: hidden;
            flex: 1;
        }

        .card-badges {
            display: flex;
            gap: 0.75rem;
            flex-wrap: wrap;
            padding-top: 1rem;
            border-top: 1px solid #334155;
        }

        .badge {
            font-size: 0.75rem;
            font-weight: 600;
            padding: 0.35rem 0.75rem;
            border-radius: 0.35rem;
            white-space: nowrap;
        }

        .badge.complexity {
            background: #1F2937;
            color: #F3F4F6;
        }

        .badge.complexity.low {
            color: #10B981;
        }

        .badge.complexity.medium {
            color: #F59E0B;
        }

        .badge.complexity.high {
            color: #EF4444;
        }

        .badge.maturity {
            background: #1F2937;
            color: #F3F4F6;
        }

        .badge.maturity.ga {
            color: #10B981;
        }

        .badge.maturity.preview {
            color: #F59E0B;
        }

        .badge.maturity.emerging {
            color: #EF4444;
        }

        /* Modal */
        .modal {
            display: none;
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.8);
            z-index: 1000;
            overflow-y: auto;
            padding: 2rem;
        }

        .modal.active {
            display: flex;
            align-items: flex-start;
            justify-content: center;
            padding-top: 2rem;
            padding-bottom: 2rem;
        }

        .modal-content {
            background: #1E293B;
            border: 1px solid #334155;
            border-radius: 0.75rem;
            max-width: 900px;
            width: 100%;
            max-height: calc(100vh - 4rem);
            overflow-y: auto;
            position: relative;
        }

        .modal-header {
            padding: 2rem;
            border-bottom: 1px solid #334155;
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
        }

        .modal-title {
            font-size: 1.5rem;
            font-weight: 700;
            color: #0EA5E9;
        }

        .close-btn {
            background: transparent;
            border: none;
            color: #94A3B8;
            font-size: 1.5rem;
            cursor: pointer;
            padding: 0;
            width: 2rem;
            height: 2rem;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .close-btn:hover {
            color: #F8FAFC;
        }

        .modal-body {
            padding: 2rem;
        }

        .modal-section {
            margin-bottom: 2rem;
        }

        .modal-section-title {
            font-size: 1rem;
            font-weight: 600;
            color: #0EA5E9;
            margin-bottom: 1rem;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .two-column {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 2rem;
        }

        @media (max-width: 768px) {
            .two-column {
                grid-template-columns: 1fr;
            }
        }

        .list-item {
            margin-bottom: 0.75rem;
            color: #E2E8F0;
            line-height: 1.6;
        }

        .list-item strong {
            color: #0EA5E9;
        }

        .governance-box {
            background: rgba(245, 158, 11, 0.1);
            border-left: 4px solid #F59E0B;
            padding: 1rem;
            margin-bottom: 1rem;
            border-radius: 0.5rem;
            color: #F0F4FF;
        }

        .chips {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
        }

        .chip {
            background: #334155;
            color: #F8FAFC;
            padding: 0.4rem 0.8rem;
            border-radius: 0.35rem;
            font-size: 0.85rem;
            cursor: pointer;
            transition: all 0.2s;
        }

        .chip:hover {
            background: #475569;
            color: #0EA5E9;
        }

        .chip.clickable {
            cursor: pointer;
        }

        .chip.clickable:hover {
            background: #0EA5E9;
            color: #1E293B;
        }

        .modal-buttons {
            display: flex;
            gap: 1rem;
            margin-top: 2rem;
            padding-top: 1rem;
            border-top: 1px solid #334155;
        }

        .btn {
            padding: 0.75rem 1.5rem;
            border-radius: 0.5rem;
            border: 1px solid #334155;
            background: transparent;
            color: #F8FAFC;
            font-size: 0.9rem;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s;
        }

        .btn:hover {
            background: #334155;
        }

        .btn.primary {
            background: #0EA5E9;
            border-color: #0EA5E9;
            color: #0F172A;
        }

        .btn.primary:hover {
            background: #06B6D4;
            border-color: #06B6D4;
        }

        /* Pattern Builder Tab */
        #builder-tab {
            flex-direction: row;
        }

        .builder-layout {
            display: grid;
            grid-template-columns: 250px 1fr 350px;
            gap: 1.5rem;
            width: 100%;
            height: 100%;
        }

        @media (max-width: 1200px) {
            .builder-layout {
                grid-template-columns: 200px 1fr 300px;
                gap: 1rem;
            }
        }

        @media (max-width: 768px) {
            .builder-layout {
                grid-template-columns: 1fr;
            }
        }

        .pattern-list {
            background: #1E293B;
            border-right: 1px solid #334155;
            padding: 1rem;
            overflow-y: auto;
        }

        .pattern-item {
            background: #0F172A;
            border: 1px solid #334155;
            border-radius: 0.5rem;
            padding: 0.75rem;
            margin-bottom: 0.75rem;
            font-size: 0.85rem;
            cursor: pointer;
            transition: all 0.2s;
        }

        .pattern-item:hover {
            border-color: #0EA5E9;
        }

        .pattern-item-name {
            font-weight: 500;
            color: #F8FAFC;
            margin-bottom: 0.5rem;
        }

        .pattern-item-add {
            background: #0EA5E9;
            color: #0F172A;
            padding: 0.35rem 0.75rem;
            border-radius: 0.35rem;
            font-size: 0.75rem;
            font-weight: 600;
            cursor: pointer;
            border: none;
            width: 100%;
            transition: all 0.2s;
        }

        .pattern-item-add:hover {
            background: #06B6D4;
        }

        .pattern-item.incompatible {
            opacity: 0.5;
            cursor: not-allowed;
        }

        .pattern-item.incompatible .pattern-item-add {
            background: #EF4444;
        }

        .pattern-item.compatible {
            border-color: #10B981;
        }

        .pattern-item.warning {
            border-color: #F59E0B;
        }

        .builder-center {
            background: #1E293B;
            border: 1px solid #334155;
            border-radius: 0.75rem;
            padding: 1.5rem;
            overflow-y: auto;
            display: flex;
            flex-direction: column;
        }

        .builder-stack {
            flex: 1;
            display: flex;
            flex-direction: column;
            gap: 0.75rem;
        }

        .stack-pattern {
            background: #0F172A;
            border-left: 4px solid #0EA5E9;
            padding: 1rem;
            border-radius: 0.5rem;
            display: flex;
            justify-content: space-between;
            align-items: center;
            cursor: grab;
        }

        .stack-pattern:active {
            cursor: grabbing;
        }

        .stack-pattern-info {
            flex: 1;
        }

        .stack-pattern-name {
            font-weight: 600;
            color: #F8FAFC;
            font-size: 0.9rem;
            margin-bottom: 0.25rem;
        }

        .stack-pattern-domain {
            font-size: 0.75rem;
            color: #94A3B8;
        }

        .stack-pattern-remove {
            background: #EF4444;
            color: white;
            border: none;
            padding: 0.5rem 0.75rem;
            border-radius: 0.35rem;
            font-size: 0.75rem;
            cursor: pointer;
            transition: all 0.2s;
        }

        .stack-pattern-remove:hover {
            background: #DC2626;
        }

        .builder-right {
            background: #1E293B;
            border: 1px solid #334155;
            border-radius: 0.75rem;
            padding: 1.5rem;
            overflow-y: auto;
            display: flex;
            flex-direction: column;
        }

        .summary-table {
            width: 100%;
            font-size: 0.85rem;
            border-collapse: collapse;
            margin-bottom: 1.5rem;
        }

        .summary-table th {
            background: #0F172A;
            color: #0EA5E9;
            padding: 0.75rem;
            text-align: left;
            font-weight: 600;
            border-bottom: 1px solid #334155;
        }

        .summary-table td {
            padding: 0.75rem;
            border-bottom: 1px solid #334155;
            color: #E2E8F0;
        }

        .summary-table tr:hover {
            background: #0F172A;
        }

        .builder-buttons {
            display: flex;
            flex-direction: column;
            gap: 0.75rem;
            margin-top: auto;
            padding-top: 1rem;
            border-top: 1px solid #334155;
        }

        .builder-btn {
            padding: 0.75rem;
            border-radius: 0.5rem;
            border: 1px solid #334155;
            background: transparent;
            color: #F8FAFC;
            font-size: 0.9rem;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s;
            white-space: nowrap;
        }

        .builder-btn:hover {
            background: #334155;
        }

        .builder-btn.primary {
            background: #0EA5E9;
            border-color: #0EA5E9;
            color: #0F172A;
        }

        .builder-btn.primary:hover {
            background: #06B6D4;
        }

        .empty-state {
            text-align: center;
            color: #94A3B8;
            padding: 2rem;
        }

        /* Use Case Blueprints Tab */
        #blueprints-tab {
            flex-direction: column;
        }

        .blueprints-grid {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 2rem;
            padding: 2rem;
        }

        @media (max-width: 1200px) {
            .blueprints-grid {
                grid-template-columns: 1fr;
            }
        }

        .blueprint-card {
            background: #1E293B;
            border: 1px solid #334155;
            border-radius: 0.75rem;
            padding: 2rem;
            cursor: pointer;
            transition: all 0.3s;
            display: flex;
            flex-direction: column;
        }

        .blueprint-card:hover {
            border-color: #0EA5E9;
            transform: translateY(-2px);
            box-shadow: 0 8px 16px rgba(14, 165, 233, 0.15);
        }

        .blueprint-title {
            font-size: 1.1rem;
            font-weight: 600;
            color: #F8FAFC;
            margin-bottom: 0.75rem;
        }

        .blueprint-desc {
            font-size: 0.9rem;
            color: #94A3B8;
            margin-bottom: 1.5rem;
            line-height: 1.6;
            flex: 1;
        }

        .blueprint-meta {
            display: flex;
            gap: 1rem;
            flex-wrap: wrap;
            margin-bottom: 1.5rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid #334155;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.85rem;
        }

        .meta-label {
            color: #94A3B8;
        }

        .meta-value {
            color: #0EA5E9;
            font-weight: 600;
        }

        .blueprint-view {
            display: none;
        }

        .blueprint-view.active {
            display: block;
        }

        .blueprint-view-content {
            padding: 2rem;
        }

        .blueprint-back {
            background: transparent;
            border: 1px solid #334155;
            color: #0EA5E9;
            padding: 0.75rem 1.5rem;
            border-radius: 0.5rem;
            margin-bottom: 1.5rem;
            cursor: pointer;
            font-weight: 500;
        }

        .blueprint-back:hover {
            background: rgba(14, 165, 233, 0.1);
        }

        .flow-step {
            background: #0F172A;
            border-left: 4px solid #0EA5E9;
            padding: 1rem;
            margin-bottom: 0.75rem;
            border-radius: 0.5rem;
            cursor: pointer;
            transition: all 0.2s;
        }

        .flow-step:hover {
            background: #1E293B;
            border-left-color: #06B6D4;
        }

        .flow-step-pattern {
            font-weight: 600;
            color: #0EA5E9;
            font-size: 0.9rem;
        }

        .flow-step-desc {
            font-size: 0.85rem;
            color: #94A3B8;
            margin-top: 0.5rem;
        }

        .editable-notes {
            background: #0F172A;
            border: 1px solid #334155;
            border-radius: 0.5rem;
            padding: 1rem;
            color: #F8FAFC;
            min-height: 150px;
            margin-bottom: 1.5rem;
            font-family: system-ui, -apple-system, sans-serif;
            font-size: 0.9rem;
            line-height: 1.6;
        }

        .editable-notes:focus {
            outline: none;
            border-color: #0EA5E9;
        }

        /* Domain Colors */
        .domain-color-1 { background: #8B5CF6; }
        .domain-color-2 { background: #06B6D4; }
        .domain-color-3 { background: #F59E0B; }
        .domain-color-4 { background: #3B82F6; }
        .domain-color-5 { background: #10B981; }
        .domain-color-6 { background: #EC4899; }
        .domain-color-7 { background: #F97316; }
        .domain-color-8 { background: #6366F1; }

        /* Utility */
        .hidden { display: none; }
        .mt-2 { margin-top: 1rem; }
        .mb-2 { margin-bottom: 1rem; }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <div class="header-content">
                <h1>Pattern Builder - HR Analytics Fabric Catalog</h1>
                <div class="tabs">
                    <button class="tab-btn active" data-tab="browse">Browse Catalog</button>
                    <button class="tab-btn" data-tab="builder">Pattern Builder</button>
                    <button class="tab-btn" data-tab="blueprints">Use Case Blueprints</button>
                </div>
            </div>
        </header>

        <div class="main-content">
            <!-- TAB 1: Browse Catalog -->
            <div id="browse-tab" class="tab-content active">
                <div class="sidebar">
                    <input type="text" id="search-patterns" class="search-box" placeholder="Search patterns...">

                    <div class="section-title">Filter by Domain</div>
                    <div class="domain-filter" id="domain-filter"></div>

                    <div class="section-title">Complexity</div>
                    <div class="toggle-group" id="complexity-filter"></div>

                    <div class="section-title">Maturity</div>
                    <div class="toggle-group" id="maturity-filter"></div>

                    <button class="clear-all" id="clear-filters">Clear All Filters</button>
                </div>

                <div class="main-area">
                    <div class="catalog-grid" id="catalog-grid"></div>
                </div>
            </div>

            <!-- TAB 2: Pattern Builder -->
            <div id="builder-tab" class="tab-content">
                <div class="builder-layout">
                    <div class="pattern-list" id="builder-pattern-list"></div>
                    <div class="builder-center">
                        <div style="margin-bottom: 1rem;">
                            <h3 style="color: #0EA5E9; margin-bottom: 0.5rem;">Selected Patterns</h3>
                            <p style="font-size: 0.85rem; color: #94A3B8;">Drag to reorder, click to remove</p>
                        </div>
                        <div class="builder-stack" id="builder-stack"></div>
                        <div class="empty-state" id="empty-state" style="margin-top: 2rem;">No patterns added yet. Select from the left.</div>
                    </div>
                    <div class="builder-right">
                        <h3 style="color: #0EA5E9; margin-bottom: 1rem;">Stack Summary</h3>
                        <table class="summary-table" id="summary-table">
                            <thead>
                                <tr>
                                    <th>Metric</th>
                                    <th>Value</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Total Patterns</td>
                                    <td id="metric-count">0</td>
                                </tr>
                                <tr>
                                    <td>Avg Complexity</td>
                                    <td id="metric-complexity">-</td>
                                </tr>
                                <tr>
                                    <td>Avg Effort</td>
                                    <td id="metric-effort">-</td>
                                </tr>
                            </tbody>
                        </table>
                        <div id="warnings" style="margin-bottom: 1.5rem; color: #FCA5A5;"></div>
                        <div class="builder-buttons">
                            <button class="builder-btn primary" id="export-json-btn">Export Stack JSON</button>
                            <button class="builder-btn" id="generate-brief-btn">Generate Brief</button>
                            <button class="builder-btn" id="print-btn">Print/PDF</button>
                        </div>
                    </div>
                </div>
            </div>

            <!-- TAB 3: Use Case Blueprints -->
            <div id="blueprints-tab" class="tab-content">
                <div class="blueprints-grid" id="blueprints-grid"></div>
                <div class="blueprint-view" id="blueprint-view">
                    <div class="blueprint-view-content" id="blueprint-view-content"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Pattern Detail Modal -->
    <div class="modal" id="pattern-modal">
        <div class="modal-content">
            <div class="modal-header">
                <div>
                    <div class="modal-title" id="modal-pattern-name"></div>
                </div>
                <button class="close-btn">&times;</button>
            </div>
            <div class="modal-body">
                <div class="modal-section">
                    <div class="modal-section-title">Overview</div>
                    <div id="modal-pattern-description" style="color: #E2E8F0; line-height: 1.8;"></div>
                </div>

                <div class="modal-section">
                    <div class="modal-section-title">Pros & Cons</div>
                    <div class="two-column">
                        <div>
                            <h4 style="color: #10B981; margin-bottom: 0.75rem;">Pros</h4>
                            <ul id="modal-pattern-pros" style="list-style: none;"></ul>
                        </div>
                        <div>
                            <h4 style="color: #EF4444; margin-bottom: 0.75rem;">Cons</h4>
                            <ul id="modal-pattern-cons" style="list-style: none;"></ul>
                        </div>
                    </div>
                </div>

                <div class="modal-section">
                    <div class="modal-section-title">Usage Instructions</div>
                    <div id="modal-pattern-usage" style="color: #E2E8F0; line-height: 1.8; white-space: pre-wrap;"></div>
                </div>

                <div class="modal-section">
                    <div class="modal-section-title">Governance</div>
                    <div class="governance-box" id="modal-pattern-governance"></div>
                </div>

                <div class="modal-section">
                    <div class="modal-section-title">HR Analytics Use Cases</div>
                    <div class="chips" id="modal-pattern-usecases"></div>
                </div>

                <div class="modal-section">
                    <div class="modal-section-title">Compatible Patterns</div>
                    <div class="chips" id="modal-pattern-compatible"></div>
                </div>

                <div class="modal-section">
                    <div class="modal-section-title">Details</div>
                    <div class="list-item"><strong>Complexity:</strong> <span id="modal-pattern-complexity"></span></div>
                    <div class="list-item"><strong>Maturity:</strong> <span id="modal-pattern-maturity"></span></div>
                    <div class="list-item"><strong>Estimated Effort:</strong> <span id="modal-pattern-effort"></span></div>
                    <div class="list-item"><strong>Cost:</strong> <span id="modal-pattern-cost"></span></div>
                </div>

                <div class="modal-section">
                    <div class="modal-section-title">References</div>
                    <div id="modal-pattern-references"></div>
                </div>

                <div class="modal-buttons">
                    <button class="btn primary" id="export-pattern-btn">Export Pattern</button>
                    <button class="btn" onclick="document.getElementById('pattern-modal').classList.remove('active')">Close</button>
                </div>
            </div>
        </div>
    </div>

    <script>
        const PATTERNS = [{"id": "medallion-architecture", "name": "Medallion Architecture (Bronze-Silver-Gold)", "domain": "Data Organization and Structuring", "domainId": 1, "category": "Lakehouse Architecture", "summary": "Implements a three-layer medallion architecture (Bronze, Silver, Gold) for progressive data refinement in OneLake.", "description": "The medallion architecture separates raw data ingestion, cleansed data, and business-ready analytics data into distinct layers. For HR analytics, employee data moves through Bronze (raw), Silver (standardized), and Gold (business-ready) layers.", "fabricComponents": ["Lakehouse", "OneLake", "Spark Notebooks", "Delta Lake"], "pros": ["Provides clear separation of concerns with distinct data quality boundaries.", "Enables independent scaling and optimization of each layer.", "Facilitates governance by creating controlled access points."], "cons": ["Introduces operational complexity with three layers.", "Can increase storage costs if not properly optimized.", "Requires upfront investment in data modeling."], "usageInstructions": "1. Create three folders: Bronze, Silver, Gold. 2. Land raw data into Bronze. 3. Build transformations in Silver. 4. Create final tables in Gold. 5. Apply labels progressively. 6. Establish refresh schedules.", "governanceConsiderations": "Implement RLS at Gold layer and restrict Bronze/Silver access to engineers. Apply sensitivity labels to personal data. Maintain transformation logs for audits.", "peopleAnalyticsUseCases": ["Employee master repository moving through all layers.", "Payroll analytics data mart with restricted raw salary details.", "Organizational analytics foundation with normalized hierarchies."], "complexity": "High", "maturity": "GA", "compatibleWith": ["delta-lake-partitioning", "onelake-shortcuts", "spark-notebook-etl", "dataflow-gen2"], "incompatibleWith": [], "prerequisites": [], "tags": ["architecture", "data-organization", "lakehouse"], "referenceLinks": [{"label": "Medallion Architecture", "url": "https://docs.microsoft.com/en-us/fabric/onelake/medallion-lakehouse-architecture"}], "estimatedImplementationEffort": "3-4 weeks", "costImplications": "Storage scales with volume; optimize with VACUUM"}, {"id": "delta-lake-partitioning", "name": "Delta Lake Partitioning Strategy", "domain": "Data Organization and Structuring", "domainId": 1, "category": "Optimization", "summary": "Partitions Delta tables by business dimensions to optimize query performance and reduce scan costs.", "description": "Partitioning divides large tables into segments based on column values like date or department, enabling Spark to skip irrelevant partitions during queries.", "fabricComponents": ["Lakehouse", "Delta Lake", "Spark Notebooks"], "pros": ["Reduces query execution time through predicate pushdown.", "Reduces compute and storage costs by avoiding full scans.", "Simplifies data lifecycle management for old partitions."], "cons": ["Poorly chosen columns degrade performance.", "Over-partitioning requires frequent compaction.", "Adds complexity to pipeline logic."], "usageInstructions": "1. Analyze query patterns. 2. Select 1-3 partition columns. 3. Create table with PARTITIONED BY. 4. Ingest data with partition columns. 5. Run ANALYZE TABLE COMPUTE STATISTICS. 6. Monitor and optimize quarterly.", "governanceConsiderations": "Align partition columns with RLS policies. Document strategy in data catalog. Monitor partition drift. Ensure archived partitions follow retention policies.", "peopleAnalyticsUseCases": ["Payroll trend analysis across decade with fast access to recent periods.", "Attendance pattern retrieval without scanning all records.", "Time-travel org hierarchy analysis."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["medallion-architecture", "lakehouse-warehouse-selection", "spark-notebook-etl"], "incompatibleWith": [], "prerequisites": ["medallion-architecture"], "tags": ["optimization", "performance", "delta-lake"], "referenceLinks": [{"label": "Delta Lake Partitioning", "url": "https://docs.microsoft.com/en-us/fabric/data-engineering/lakehouse-shortcuts"}], "estimatedImplementationEffort": "1-2 weeks", "costImplications": "Reduces costs 50-80%; storage overhead minimal"}, {"id": "lakehouse-warehouse-selection", "name": "Lakehouse vs Warehouse Selection", "domain": "Data Organization and Structuring", "domainId": 1, "category": "Decision Framework", "summary": "Decision framework for choosing between Lakehouse and Warehouse based on workload characteristics.", "description": "Lakehouse works well for unstructured data and ML; Warehouse for structured relational analytics. Many use both in parallel.", "fabricComponents": ["Lakehouse", "Warehouse", "OneLake", "Spark Notebooks"], "pros": ["Optimal tool selection for different workloads.", "Lakehouse provides flexibility; Warehouse ensures consistency.", "Supports incremental adoption and migration."], "cons": ["Operating both increases operational overhead.", "Performance strategies differ significantly.", "Teams must understand distinct capabilities."], "usageInstructions": "1. Assess workload type. 2. Structured HR reporting \u2192 Warehouse. 3. Exploratory analysis \u2192 Lakehouse. 4. Semi-structured \u2192 Lakehouse. 5. If both needed: Lakehouse Bronze/Silver, Warehouse for reporting. 6. Use shortcuts.", "governanceConsiderations": "Warehouse provides stricter governance through schema enforcement. Restrict Warehouse to certified analytics. Use Lakehouse for non-sensitive exploration.", "peopleAnalyticsUseCases": ["Operational HR dashboards in Warehouse.", "Survey analysis and churn modeling in Lakehouse.", "ML pipelines in Lakehouse, results in Warehouse."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["medallion-architecture", "direct-lake-semantic-model"], "incompatibleWith": [], "prerequisites": [], "tags": ["architecture", "storage-selection"], "referenceLinks": [{"label": "Lakehouse Overview", "url": "https://docs.microsoft.com/en-us/fabric/data-engineering/lakehouse-overview"}], "estimatedImplementationEffort": "1 week assessment", "costImplications": "Lakehouse compute scales with transforms; Warehouse has fixed DWU pricing"}, {"id": "onelake-shortcuts", "name": "OneLake Shortcuts for Data Sharing", "domain": "Data Organization and Structuring", "domainId": 1, "category": "Data Integration", "summary": "Uses OneLake shortcuts as virtual references to data elsewhere without copying.", "description": "Shortcuts point to data in other Lakehouses, Warehouses, or external storage. Finance maintains employee master; HR and Recruiting create shortcuts to it.", "fabricComponents": ["OneLake", "Lakehouse", "Warehouse", "Shortcuts"], "pros": ["Eliminates duplication and maintains single source of truth.", "Zero-copy reduces costs; updates visible immediately.", "Simplifies cross-team collaboration."], "cons": ["Cross-workspace latency can degrade performance.", "Shortcuts obscure ownership and governance.", "Lineage becomes harder to debug."], "usageInstructions": "1. Identify source of truth tables. 2. Create Lakehouse in consumer team. 3. Right-click folder > New shortcut. 4. Select source table. 5. Query like normal tables. 6. Monitor performance.", "governanceConsiderations": "Shortcuts must point to governed tables. Establish data contracts for schema stability. Document in data catalog. Apply workspace permissions. Use for read-only reference data.", "peopleAnalyticsUseCases": ["Finance employee master accessed via shortcuts by HR and Recruiting.", "Shared org hierarchy referenced across teams.", "Reducing storage by shortcutting payroll data."], "complexity": "Low", "maturity": "GA", "compatibleWith": ["medallion-architecture", "lakehouse-warehouse-selection"], "incompatibleWith": [], "prerequisites": [], "tags": ["data-sharing", "shortcuts", "zero-copy"], "referenceLinks": [{"label": "OneLake Shortcuts", "url": "https://docs.microsoft.com/en-us/fabric/onelake/onelake-shortcuts"}], "estimatedImplementationEffort": "2-3 days", "costImplications": "Zero-copy saves significantly vs replication"}, {"id": "direct-lake-semantic-model", "name": "Direct Lake Semantic Model", "domain": "Data Organization and Structuring", "domainId": 1, "category": "Semantic Layer Design", "summary": "Creates semantic models that directly reference OneLake Delta tables in Direct Lake mode for real-time analytics.", "description": "Direct Lake bypasses VertiPaq import, providing freshness of DirectQuery with import speed. Gold-layer tables feed Power BI without duplication.", "fabricComponents": ["Semantic Model", "Power BI", "OneLake", "Delta Lake", "Lakehouse"], "pros": ["Real-time data access without import overhead.", "Eliminates storage duplication.", "Combines import performance with DirectQuery freshness."], "cons": ["Requires well-optimized Delta tables.", "Not all Power BI transformations supported.", "Optimization less transparent than import mode."], "usageInstructions": "1. Ensure Gold tables are optimized. 2. Create semantic model. 3. Select Direct Lake mode. 4. Browse and select Gold tables. 5. Create relationships. 6. Build reports. 7. Monitor performance.", "governanceConsiderations": "Direct Lake exposes lakehouse structure; govern before creating models. Control who modifies underlying tables. Apply sensitivity labels. Document contracts. Prevent accidental deletions.", "peopleAnalyticsUseCases": ["Real-time HR dashboards from Gold tables.", "Live payroll cost dashboards.", "Quick BI iteration using Direct Lake."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["medallion-architecture", "lakehouse-warehouse-selection", "delta-lake-partitioning"], "incompatibleWith": [], "prerequisites": ["medallion-architecture"], "tags": ["semantic-model", "direct-lake", "power-bi"], "referenceLinks": [{"label": "Direct Lake", "url": "https://docs.microsoft.com/en-us/power-bi/enterprise/directlake-overview"}], "estimatedImplementationEffort": "1-2 weeks", "costImplications": "Reduces import compute; scales with query volume"}, {"id": "hub-spoke-workspace", "name": "Hub-and-Spoke Workspace Design", "domain": "Data Organization and Structuring", "domainId": 1, "category": "Workspace Architecture", "summary": "Hub workspace contains shared reference data; spoke workspaces (HR, Recruiting, Finance) build domain-specific analytics.", "description": "Central Hub maintains employee master, org structure, cost centers. Spokes use shortcuts to reference Hub and build domain analytics.", "fabricComponents": ["Workspaces", "OneLake", "Lakehouse", "Shortcuts"], "pros": ["Centralizes reference data governance.", "Enables autonomous spoke teams.", "Simplifies permission management."], "cons": ["Cross-workspace dependencies add complexity.", "Hub requires dedicated team.", "Network latency for shortcuts."], "usageInstructions": "1. Create Hub workspace. 2. Populate with reference tables. 3. Create Spoke workspaces. 4. Spoke teams create shortcuts to Hub. 5. Spoke builds own layers. 6. Establish data review board. 7. Document dependencies.", "governanceConsiderations": "Hub requires clear ownership and change management. Strict Hub permissions: stewards only. Enforce RLS at Spoke level. Document contracts. Monitor dependencies.", "peopleAnalyticsUseCases": ["Central HR Hub with Recruiting, Compensation spokes.", "Finance Hub with HR spoke for cost allocation.", "Executive Hub with HR spoke for talent alignment."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["medallion-architecture", "onelake-shortcuts"], "incompatibleWith": [], "prerequisites": ["medallion-architecture"], "tags": ["workspace-design", "hub-spoke"], "referenceLinks": [{"label": "Workspace Management", "url": "https://docs.microsoft.com/en-us/fabric/admin/workspaces"}], "estimatedImplementationEffort": "3-4 weeks", "costImplications": "Hub shared across spokes; separate Spoke costs"}, {"id": "spark-notebook-etl", "name": "Spark Notebook ETL Pipelines", "domain": "Data Transformation and Processing", "domainId": 2, "category": "ETL Development", "summary": "PySpark notebooks for complex Bronze-to-Silver and Silver-to-Gold transformations with full programming power.", "description": "Notebooks provide flexibility for complex logic, iterative development, and large-scale transformations. Payroll notebooks standardize fields, fill missing dates, calculate tenure.", "fabricComponents": ["Notebook", "Spark", "PySpark", "Lakehouse", "Delta Lake"], "pros": ["Ultimate flexibility for complex business logic.", "Cell-level execution enables step-by-step debugging.", "Automatic scaling to large datasets."], "cons": ["Requires Python/Scala expertise.", "No visual lineage or profiling.", "Harder to govern."], "usageInstructions": "1. Create notebook. 2. Read Bronze: df = spark.read.table(). 3. Transform (standardize, validate, enrich). 4. Write to Silver. 5. Test on sample data. 6. Schedule as job. 7. Add error handling.", "governanceConsiderations": "Implement code review for notebooks. Version control via Git. Restrict modify permissions. Log all transformations. Document assumptions.", "peopleAnalyticsUseCases": ["Complex payroll ETL with reconciliation.", "Employee movement tracking via snapshots.", "Unified talent dataset from multiple sources."], "complexity": "High", "maturity": "GA", "compatibleWith": ["medallion-architecture", "delta-lake-partitioning", "incremental-watermark"], "incompatibleWith": [], "prerequisites": ["medallion-architecture"], "tags": ["etl", "spark", "pyspark", "transformation"], "referenceLinks": [{"label": "Spark Notebooks", "url": "https://docs.microsoft.com/en-us/fabric/data-engineering/notebook-overview"}], "estimatedImplementationEffort": "2-5 days per pipeline", "costImplications": "Scales with complexity and volume"}, {"id": "dataflow-gen2", "name": "Dataflow Gen2 Low-Code Transformations", "domain": "Data Transformation and Processing", "domainId": 2, "category": "ETL Development", "summary": "Power Query Online visual ETL for simple-to-moderate transformations without coding.", "description": "Graphical UI for filter, merge, group, enrich. Employee master ingest, remove duplicates, rename columns, output to Lakehouse.", "fabricComponents": ["Dataflow Gen2", "Power Query Online", "Lakehouse", "Power BI"], "pros": ["Reduces time-to-delivery for standard transforms.", "Built-in data profiling and quality checks.", "Native Power BI integration."], "cons": ["Limited to moderately complex logic.", "Performance degrades on large datasets.", "Harder to version control and CI/CD."], "usageInstructions": "1. Create Dataflow Gen2. 2. Connect to source. 3. Apply transforms: Remove Duplicates, Filter, Rename. 4. Group/summarize if needed. 5. Merge with references. 6. Preview and validate. 7. Configure destination. 8. Schedule refresh.", "governanceConsiderations": "Document formulas clearly. Establish change approval. Monitor refresh times. Use as preferred entry for business users. Implement labels on outputs.", "peopleAnalyticsUseCases": ["Weekly employee snapshot: ingest, dedup, filter, output.", "Department cost center mapping and aggregation.", "Applicant data prep for recruiting analytics."], "complexity": "Low", "maturity": "GA", "compatibleWith": ["medallion-architecture", "data-quality-validation"], "incompatibleWith": [], "prerequisites": [], "tags": ["etl", "low-code", "power-query"], "referenceLinks": [{"label": "Dataflow Gen2", "url": "https://docs.microsoft.com/en-us/fabric/data-factory/create-first-dataflow-gen2"}], "estimatedImplementationEffort": "1-3 days", "costImplications": "Lower compute for simple transforms; scales with frequency"}, {"id": "incremental-watermark", "name": "Incremental Loading with Watermarks", "domain": "Data Transformation and Processing", "domainId": 2, "category": "ETL Efficiency", "summary": "Captures only new/modified records since last run using watermark columns to reduce refresh time.", "description": "Store max last_modified_date in control table; query only records > previous watermark. Nightly refresh hours reduces to minutes.", "fabricComponents": ["Spark Notebook", "Lakehouse", "Delta Lake", "Control Tables"], "pros": ["Reduces refresh from hours to minutes.", "Scales elegantly with constant change volume.", "Enables real-time/near-real-time analytics."], "cons": ["Requires reliable source change tracking.", "Complex to handle late-arriving data.", "Difficult recovery from failures."], "usageInstructions": "1. Create control table for watermarks. 2. Read previous watermark. 3. Query source with filter. 4. Merge into Silver. 5. Update watermark. 6. Monitor for late data.", "governanceConsiderations": "Govern source change-tracking columns. Document watermark logic. Implement monitoring. Archive watermarks. Establish reset procedures.", "peopleAnalyticsUseCases": ["Employee transactions with daily incremental loads.", "Real-time headcount dashboard.", "Employee history capturing changes."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["medallion-architecture", "spark-notebook-etl", "scd-type-2"], "incompatibleWith": [], "prerequisites": ["spark-notebook-etl", "medallion-architecture"], "tags": ["incremental", "watermark", "etl"], "referenceLinks": [{"label": "Delta Merge", "url": "https://docs.microsoft.com/en-us/fabric/data-engineering/delta-optimization"}], "estimatedImplementationEffort": "3-5 days", "costImplications": "Reduces 80-90% vs full reload"}, {"id": "scd-type-2", "name": "Slowly Changing Dimensions Type 2", "domain": "Data Transformation and Processing", "domainId": 2, "category": "Dimension Management", "summary": "Maintains historical versions with validity dates enabling time-travel analysis of attribute changes.", "description": "New rows created for changes with effective_date, end_date. Promotion creates new employee row; old row marked ended. Enables salary progression analysis.", "fabricComponents": ["Spark Notebook", "Delta Lake", "Lakehouse"], "pros": ["Enables temporal analysis and past-state reconstruction.", "Maintains historical context for metrics.", "Supports full audit trail."], "cons": ["Storage overhead from historical versions.", "Complex merge logic required.", "Analytics queries become more complex."], "usageInstructions": "1. Design dimension with surrogate key, effective_date, end_date, current_flag. 2. Initial load. 3. On update: END previous, INSERT new. 4. Merge into dimension. 5. Validate no overlaps.", "governanceConsiderations": "Document merge logic thoroughly. Implement validation checks. Archive old dimensions. Use selectively. Establish retention policies.", "peopleAnalyticsUseCases": ["Career progression analysis with salary growth.", "Org change analysis with historical reporting lines.", "Compensation cohort analysis."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["medallion-architecture", "spark-notebook-etl", "incremental-watermark"], "incompatibleWith": [], "prerequisites": ["spark-notebook-etl", "medallion-architecture"], "tags": ["scd", "slowly-changing", "temporal"], "referenceLinks": [{"label": "SCD Patterns", "url": "https://en.wikipedia.org/wiki/Slowly_changing_dimension"}], "estimatedImplementationEffort": "3-4 days", "costImplications": "Storage 20-50% higher for history"}, {"id": "cdc-change-capture", "name": "Change Data Capture (CDC) for Auditing", "domain": "Data Transformation and Processing", "domainId": 2, "category": "Data Auditing", "summary": "Records before/after values of all modifications with user and timestamp for compliance audit trails.", "description": "Salary update triggers record logging old/new values. Audit table captures INSERT/UPDATE/DELETE with metadata. Supports investigations.", "fabricComponents": ["Warehouse", "Spark Notebook", "Audit Tables"], "pros": ["Complete audit trail for compliance.", "Enables real-time alerting on sensitive changes.", "Supports efficient incremental processing."], "cons": ["Increases write overhead and latency.", "Audit tables grow very large.", "Edge cases require careful handling."], "usageInstructions": "1. Create audit table. 2. Trigger on updates records changes. 3. Archive records >7 years. 4. Expose to compliance via Power BI. 5. Set up alerts.", "governanceConsiderations": "Restrict to compliance/audit only. Establish retention policies. Document audited fields. Archive to cold storage. Use for investigation support.", "peopleAnalyticsUseCases": ["Compliance auditing of salary changes.", "Detecting unauthorized modifications.", "Payroll reconciliation tracking."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["sensitivity-labels", "scd-type-2", "data-quality-validation"], "incompatibleWith": [], "prerequisites": [], "tags": ["cdc", "audit", "compliance"], "referenceLinks": [{"label": "CDC Patterns", "url": "https://docs.microsoft.com/en-us/fabric/data-warehouse/change-data-capture"}], "estimatedImplementationEffort": "2-3 days", "costImplications": "Storage 10-30% overhead; consider tiered storage"}, {"id": "dbt-integration", "name": "dbt Integration for Data Transformation", "domain": "Data Transformation and Processing", "domainId": 2, "category": "ETL Development", "summary": "SQL-based dbt models with version control, testing, documentation enabling software engineering discipline.", "description": "dbt projects organize SQL transformations with tests, lineage, documentation. Junior analysts contribute familiar SQL; dbt handles plumbing.", "fabricComponents": ["Warehouse", "Lakehouse", "dbt", "Git", "Spark SQL"], "pros": ["Brings software engineering to analytics.", "Modular SQL enables junior analyst contribution.", "Auto-generates lineage documentation."], "cons": ["Requires dbt and YAML knowledge.", "Performance tuning less transparent.", "Limited to SQL transformations."], "usageInstructions": "1. Create dbt project. 2. Configure Warehouse target. 3. Write SQL models. 4. Define tests. 5. Run dbt run. 6. Commit to git. 7. Configure CI/CD.", "governanceConsiderations": "Structure by medallion layers. Implement code review. Use dbt tests for quality. Document business context. Manage breaking changes carefully.", "peopleAnalyticsUseCases": ["HR transformation logic with employee, role, compensation models.", "Collaborative analysis pipeline with code review.", "Rapid metrics iteration."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["medallion-architecture", "data-quality-validation", "spark-notebook-etl"], "incompatibleWith": [], "prerequisites": ["medallion-architecture"], "tags": ["dbt", "sql", "transformation", "version-control"], "referenceLinks": [{"label": "dbt Docs", "url": "https://docs.getdbt.com/"}], "estimatedImplementationEffort": "1-2 weeks", "costImplications": "Same as SQL compute; reduced overhead from reuse"}, {"id": "data-quality-validation", "name": "Data Quality Validation Framework", "domain": "Data Transformation and Processing", "domainId": 2, "category": "Quality Assurance", "summary": "Automated checks after ETL for anomalies, nulls, schema violations, business rule violations preventing bad data propagation.", "description": "Validation tests NULL values in required fields, validates salary ranges, confirms dept codes match reference, detects duplicates. Failures pause processes.", "fabricComponents": ["Spark Notebook", "dbt", "Great Expectations", "Lakehouse"], "pros": ["Catches issues at source before propagation.", "Builds analyst trust in data.", "Enables quick root-cause analysis."], "cons": ["Requires upfront effort to define rules.", "Can over-reject valid data.", "Adds latency to pipelines."], "usageInstructions": "1. Define rules: required_fields, ranges, valid values. 2. After load, run validation. 3. Check violations vs threshold. 4. Alert if exceeded. 5. Implement with dbt tests or Great Expectations.", "governanceConsiderations": "Document rules with justification. Version in code. Archive results for audits. Use as data contract documentation. Establish escalation procedures.", "peopleAnalyticsUseCases": ["Payroll validation before Finance handoff.", "Org hierarchy consistency checking.", "HRIS reconciliation."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["medallion-architecture", "spark-notebook-etl", "dbt-integration"], "incompatibleWith": [], "prerequisites": ["medallion-architecture"], "tags": ["data-quality", "validation", "testing"], "referenceLinks": [{"label": "dbt Tests", "url": "https://docs.getdbt.com/docs/building-a-dbt-project/tests"}], "estimatedImplementationEffort": "2-3 weeks", "costImplications": "Validation is 5-10% of ETL compute"}, {"id": "purview-data-map", "name": "Microsoft Purview Data Map", "domain": "Data Governance and Security", "domainId": 3, "category": "Data Catalog", "summary": "Creates comprehensive data catalog in Purview mapping all assets, lineage, classifications, and sensitive data locations.", "description": "Purview scans Fabric workspace discovering tables, columns, lineage. Classifications mark PII (SSN, salary). Stewards govern assets and ownership.", "fabricComponents": ["Microsoft Purview", "Fabric", "Lakehouse", "Warehouse"], "pros": ["Provides complete asset inventory and lineage.", "Automates sensitive data discovery.", "Enables steward governance at scale."], "cons": ["Requires significant setup and configuration.", "Scans can be resource-intensive.", "Learning curve for Purview concepts."], "usageInstructions": "1. Connect Fabric to Purview. 2. Configure scans. 3. Run scans. 4. Review classifications. 5. Assign stewards. 6. Create business glossary. 7. Document assets.", "governanceConsiderations": "Use Purview as source of truth for data governance. Integrate classifications with sensitivity labels. Assign stewards for critical assets. Track data quality metrics. Update regularly.", "peopleAnalyticsUseCases": ["Discover all HR data assets across organization.", "Track payroll data lineage from source to reports.", "Identify PII exposure and mitigation paths."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["sensitivity-labels", "row-level-security", "cdc-change-capture"], "incompatibleWith": [], "prerequisites": [], "tags": ["data-catalog", "governance", "purview", "lineage"], "referenceLinks": [{"label": "Microsoft Purview", "url": "https://docs.microsoft.com/en-us/purview/"}], "estimatedImplementationEffort": "3-4 weeks", "costImplications": "Purview licensing plus scan compute"}, {"id": "sensitivity-labels", "name": "Sensitivity Labels for Data Classification", "domain": "Data Governance and Security", "domainId": 3, "category": "Data Protection", "summary": "Applies sensitivity labels (Highly Confidential, Confidential, Internal) to tables and columns triggering data masking and access controls.", "description": "Label SSN and salary columns as Highly Confidential; Purview enforces masking and restricts query results. Labels cascade to reports and exports.", "fabricComponents": ["Sensitivity Labels", "Purview", "Lakehouse", "Warehouse", "Power BI"], "pros": ["Automated protection based on content classification.", "Enforcement applies across Fabric and Power BI.", "Cascades to exports and reports."], "cons": ["Initial labeling requires effort.", "Label enforcement can break some use cases.", "Requires governance process."], "usageInstructions": "1. Define label taxonomy. 2. Create labels in Security & Compliance. 3. Apply to tables/columns. 4. Configure label policies. 5. Test masking. 6. Monitor usage.", "governanceConsiderations": "Establish consistent label taxonomy. Assign classification responsibility. Monitor label compliance. Update as data changes. Train users.", "peopleAnalyticsUseCases": ["Mark SSN, salary, health data as Highly Confidential.", "Restrict export of labeled data.", "Auto-mask salary in development environment."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["purview-data-map", "row-level-security", "dynamic-data-masking"], "incompatibleWith": [], "prerequisites": [], "tags": ["sensitivity-labels", "classification", "pii", "protection"], "referenceLinks": [{"label": "Sensitivity Labels", "url": "https://docs.microsoft.com/en-us/purview/sensitivity-labels"}], "estimatedImplementationEffort": "2-3 weeks", "costImplications": "Minimal overhead; licensing included in Purview"}, {"id": "row-level-security", "name": "Row-Level Security (RLS) at Gold Layer", "domain": "Data Governance and Security", "domainId": 3, "category": "Access Control", "summary": "Restricts query results based on user identity/role so managers see only their team's data and employees see personal data.", "description": "RLS rules evaluated at query time. HR admin sees all employees; manager sees only direct reports. Prevents accidental overexposure.", "fabricComponents": ["Warehouse", "Semantic Model", "Power BI", "RLS Roles"], "pros": ["Query-time enforcement is performant.", "Prevents overexposure through accidental queries.", "Works across Warehouse and Power BI."], "cons": ["RLS logic can become complex.", "Debugging RLS issues is difficult.", "Semantic model must support RLS columns."], "usageInstructions": "1. Identify RLS dimension (e.g., manager_id, department). 2. Create Warehouse views with RLS. 3. In semantic model, define RLS roles. 4. Map users to roles. 5. Test query results per role. 6. Assign roles to users.", "governanceConsiderations": "RLS is critical for HR data. Test thoroughly before production. Document RLS logic. Monitor for overexposure. Update when org changes.", "peopleAnalyticsUseCases": ["Managers see direct reports salary; executives see all.", "Employees see personal data only.", "HR sees department; Finance sees cost center salary."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["medallion-architecture", "sensitivity-labels", "certified-semantic-model"], "incompatibleWith": [], "prerequisites": [], "tags": ["rls", "row-level-security", "access-control"], "referenceLinks": [{"label": "Row-Level Security", "url": "https://docs.microsoft.com/en-us/power-bi/enterprise/service-admin-rls"}], "estimatedImplementationEffort": "2-3 weeks", "costImplications": "Minimal compute overhead"}, {"id": "dynamic-data-masking", "name": "Dynamic Data Masking for Development/Test", "domain": "Data Governance and Security", "domainId": 3, "category": "Data Protection", "summary": "Masks sensitive data values in non-production environments so developers see realistic data without exposure.", "description": "Replace salary values with 0, truncate SSN to last 4 digits, replace names with 'Employee-123'. Development team tests with masked data.", "fabricComponents": ["Warehouse", "Spark Notebook", "Data Masking Policies"], "pros": ["Enables realistic testing without sensitive data exposure.", "Reduces security incidents from dev environment breaches.", "Supports faster dev cycles without data sanitization."], "cons": ["Masking logic can affect performance.", "Developers frustrated by realistic data lack.", "Complex to mask consistently."], "usageInstructions": "1. Create dev/test environments. 2. Define masking rules for sensitive columns. 3. Apply masks on data refresh. 4. Validate masking prevents identification. 5. Monitor compliance.", "governanceConsiderations": "Mask all sensitive data in non-prod. Maintain consistent masking across all clones. Document masking rules. Monitor mask effectiveness.", "peopleAnalyticsUseCases": ["Dev database with masked SSN, salary for developers.", "Test environment with realistic structure but masked values.", "Compliance environment with redacted data."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["sensitivity-labels", "row-level-security"], "incompatibleWith": [], "prerequisites": [], "tags": ["data-masking", "pii-protection", "development"], "referenceLinks": [{"label": "Data Masking", "url": "https://docs.microsoft.com/en-us/sql/relational-databases/security/dynamic-data-masking"}], "estimatedImplementationEffort": "2-3 weeks", "costImplications": "Minimal overhead for masking operations"}, {"id": "abac-access-control", "name": "Attribute-Based Access Control (ABAC)", "domain": "Data Governance and Security", "domainId": 3, "category": "Access Control", "summary": "Access decisions based on user attributes (department, role, location) plus resource attributes enabling fine-grained, scalable permissions.", "description": "Role/department attributes determine access. HR team member + HR resource = access. Scales better than managing individual user permissions.", "fabricComponents": ["Workspace Permissions", "Semantic Model Roles", "Azure AD Groups"], "pros": ["Scales to large organizations.", "Changes managed through attributes, not user lists.", "Reduces permission management overhead."], "cons": ["Requires attribute governance.", "Complex logic can be hard to audit.", "Debugging attribute-based denials difficult."], "usageInstructions": "1. Define access attributes. 2. Populate attributes in Azure AD. 3. Create Azure AD groups by attributes. 4. Assign groups to Workspace/Model roles. 5. Test access per attribute combination. 6. Monitor attribute changes.", "governanceConsiderations": "Attribute definitions require business input. Master attributes in Azure AD. Audit attribute changes. Regular access reviews. Update as org changes.", "peopleAnalyticsUseCases": ["Department attribute controls workspace access.", "Role attribute determines semantic model permissions.", "Location attribute gates data access.", "Manager attribute enables RLS."], "complexity": "High", "maturity": "GA", "compatibleWith": ["row-level-security", "workspace-permission-governance"], "incompatibleWith": [], "prerequisites": [], "tags": ["abac", "attribute-based", "access-control"], "referenceLinks": [{"label": "ABAC Concepts", "url": "https://docs.microsoft.com/en-us/azure/active-directory/conditional-access/overview"}], "estimatedImplementationEffort": "4-6 weeks", "costImplications": "Azure AD licensing; minimal compute"}, {"id": "workspace-permission-governance", "name": "Workspace Permission Governance", "domain": "Data Governance and Security", "domainId": 3, "category": "Access Control", "summary": "Manages workspace role assignments (Admin, Member, Contributor, Viewer) through approval workflows preventing unauthorized access creep.", "description": "Access requests go through approval. Quarterly access reviews. Admins audit who has what role. Revoke unused access promptly.", "fabricComponents": ["Workspaces", "Roles", "Azure AD", "Access Reviews"], "pros": ["Prevents unauthorized access accumulation.", "Audit trail of who approves access.", "Regular reviews catch stale access."], "cons": ["Adds overhead to access provisioning.", "Review fatigue with many users.", "Requires disciplined process."], "usageInstructions": "1. Define role matrix. 2. Establish request process. 3. Configure approval workflow. 4. Quarterly access review. 5. Deprovision unused access. 6. Audit log.", "governanceConsiderations": "Clear role definitions. Documented request process. Approval authority defined. Review frequency set. Audit logged.", "peopleAnalyticsUseCases": ["Request approval for workspace access.", "Quarterly review of HR Analytics workspace roles.", "Audit trail for compliance."], "complexity": "Low", "maturity": "GA", "compatibleWith": ["abac-access-control", "purview-data-map"], "incompatibleWith": [], "prerequisites": [], "tags": ["workspace-permissions", "governance", "access-control"], "referenceLinks": [{"label": "Workspace Roles", "url": "https://docs.microsoft.com/en-us/fabric/admin/roles"}], "estimatedImplementationEffort": "2-3 weeks", "costImplications": "Minimal overhead"}, {"id": "directlake-power-bi", "name": "Direct Lake Power BI Semantic Models", "domain": "Business Intelligence and Reporting", "domainId": 4, "category": "BI Modeling", "summary": "Power BI semantic models using Direct Lake connectivity to Fabric Gold tables for real-time BI without data import.", "description": "Gold tables feed Power BI directly via Direct Lake. No nightly imports. BI analysts refresh tables instantly. Dashboards always show latest data.", "fabricComponents": ["Power BI", "Semantic Model", "Direct Lake", "Lakehouse"], "pros": ["Real-time data for dashboards.", "No import bottleneck.", "Simplified data pipeline."], "cons": ["Requires optimized Gold tables.", "Less transformation flexibility.", "Network latency possible."], "usageInstructions": "1. Create semantic model in Workspace. 2. Connect to Gold tables via Direct Lake. 3. Define relationships. 4. Create measures. 5. Build reports. 6. Publish. 7. Monitor performance.", "governanceConsiderations": "Gold tables must be governed. Control who modifies lakehouse. Apply RLS at semantic model level. Document data contracts.", "peopleAnalyticsUseCases": ["Real-time HR dashboard from Gold tables.", "Executive payroll dashboard.", "Live recruiting pipeline dashboard."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["medallion-architecture", "direct-lake-semantic-model", "delta-lake-partitioning"], "incompatibleWith": [], "prerequisites": ["medallion-architecture"], "tags": ["power-bi", "direct-lake", "semantic-model"], "referenceLinks": [{"label": "Direct Lake BI", "url": "https://docs.microsoft.com/en-us/power-bi/enterprise/directlake-best-practices"}], "estimatedImplementationEffort": "1-2 weeks", "costImplications": "Reduces Premium capacity cost vs import"}, {"id": "storage-mode-selection", "name": "Storage Mode Selection (Import/DirectQuery/Dual)", "domain": "Business Intelligence and Reporting", "domainId": 4, "category": "BI Modeling", "summary": "Chooses optimal storage mode per table based on size, update frequency, and performance requirements.", "description": "Import small lookup tables for speed. DirectQuery large slow-changing fact tables. Dual mode combines both for flexibility.", "fabricComponents": ["Power BI", "Semantic Model", "Warehouse", "Lakehouse"], "pros": ["Optimizes performance and refresh time.", "Reduces Premium capacity utilization.", "Flexibility for heterogeneous requirements."], "cons": ["Increases modeling complexity.", "DirectQuery can be slow without optimization.", "Users must understand modes."], "usageInstructions": "1. Analyze table size and query frequency. 2. Small/frequently accessed = Import. 3. Large/slow-changing = DirectQuery. 4. Mixed = Dual. 5. Monitor refresh times. 6. Adjust modes based on perf.", "governanceConsiderations": "Document storage mode decisions. Monitor refresh failures. Update source query optimization. Test DirectQuery performance.", "peopleAnalyticsUseCases": ["Import employee dimension; DirectQuery large payroll facts.", "Dual mode org hierarchy with frequent ref lookup.", "Import cost center lookup; DirectQuery salary facts."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["directlake-power-bi", "certified-semantic-model"], "incompatibleWith": [], "prerequisites": [], "tags": ["storage-mode", "power-bi", "performance"], "referenceLinks": [{"label": "Storage Modes", "url": "https://docs.microsoft.com/en-us/power-bi/transform-model/desktop-storage-mode"}], "estimatedImplementationEffort": "1-2 weeks", "costImplications": "Optimizes capacity utilization"}, {"id": "composite-model", "name": "Composite Models (Multi-Source Mashing)", "domain": "Business Intelligence and Reporting", "domainId": 4, "category": "BI Modeling", "summary": "Combines tables from multiple sources (Lakehouse, Warehouse, SQL, Excel) in single semantic model for unified analytics.", "description": "Employee master from Lakehouse, payroll from Warehouse, survey data from Excel. Composite model joins across sources.", "fabricComponents": ["Power BI", "Semantic Model", "Composite Model", "Multiple Sources"], "pros": ["Unified analytics across sources.", "Reduces data movement.", "Flexible source management."], "cons": ["Query complexity increases.", "Cross-source joins can be slow.", "Debugging difficult."], "usageInstructions": "1. Create semantic model. 2. Add tables from multiple sources. 3. Define relationships across sources. 4. Create measures. 5. Test query performance. 6. Monitor.", "governanceConsiderations": "Document source integration logic. Monitor cross-source join performance. Establish data ownership across sources.", "peopleAnalyticsUseCases": ["Employee Lakehouse table joined with payroll Warehouse table.", "Org Lakehouse joined with survey results from Excel.", "Performance Lakehouse joined with compensation Warehouse."], "complexity": "High", "maturity": "GA", "compatibleWith": ["storage-mode-selection", "certified-semantic-model"], "incompatibleWith": [], "prerequisites": [], "tags": ["composite-model", "multi-source", "power-bi"], "referenceLinks": [{"label": "Composite Models", "url": "https://docs.microsoft.com/en-us/power-bi/transform-model/desktop-composite-models"}], "estimatedImplementationEffort": "2-3 weeks", "costImplications": "Higher compute for cross-source joins"}, {"id": "certified-semantic-model", "name": "Certified Semantic Models", "domain": "Business Intelligence and Reporting", "domainId": 4, "category": "BI Governance", "summary": "BI team certifies semantic models ensuring consistent definitions, quality metrics, and single source of truth for organization.", "description": "Gold-layer semantic models certified by BI team mark metrics as authoritative. Analysts use certified models for consistency.", "fabricComponents": ["Power BI", "Semantic Model", "Workspace"], "pros": ["Ensures metric consistency across dashboards.", "Reduces metric duplication.", "Facilitates self-service BI."], "cons": ["Requires strong BI governance.", "Slows new model deployment.", "Can bottleneck innovation."], "usageInstructions": "1. Build semantic model. 2. Document metrics and definitions. 3. Have BI team certify. 4. Mark as Certified. 5. Analysts build reports. 6. Update as needed.", "governanceConsiderations": "Establish certification criteria. Document metric definitions. Review before certification. Update certification on changes.", "peopleAnalyticsUseCases": ["Certified headcount metric used across dashboards.", "Certified FTE calculation model.", "Certified cost per hire metric."], "complexity": "Low", "maturity": "GA", "compatibleWith": ["directlake-power-bi", "storage-mode-selection", "composite-model"], "incompatibleWith": [], "prerequisites": [], "tags": ["certified-model", "governance", "metrics"], "referenceLinks": [{"label": "Certified Models", "url": "https://docs.microsoft.com/en-us/power-bi/collaborate-share/service-certify-datasets"}], "estimatedImplementationEffort": "2-3 weeks", "costImplications": "Minimal overhead"}, {"id": "paginated-reports", "name": "Paginated Reports for Formal Documents", "domain": "Business Intelligence and Reporting", "domainId": 4, "category": "Report Development", "summary": "Power BI paginated reports for pixel-perfect formal documents like payroll statements, regulatory reports, and audit certifications.", "description": "Paginated reports in Power BI for formatted output. Employee tax statements, OFCCP compliance reports, board summaries.", "fabricComponents": ["Power BI", "Paginated Reports", "Warehouse", "Semantic Model"], "pros": ["Pixel-perfect formatting for formal documents.", "Supports complex layouts and headers.", "Suitable for printing and distribution."], "cons": ["Slower to develop than dashboards.", "Limited interactivity.", "Requires RDL knowledge."], "usageInstructions": "1. Create paginated report in Power BI. 2. Define parameters. 3. Design layout. 4. Connect to data source. 5. Format for printing. 6. Test output. 7. Schedule.", "governanceConsiderations": "Formal reports require approval. Document generation logic. Maintain version history. Ensure data accuracy.", "peopleAnalyticsUseCases": ["Employee tax statements generated monthly.", "OFCCP compliance report certification.", "Board summary with specific formatting.", "Payroll audit report."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["storage-mode-selection", "certified-semantic-model"], "incompatibleWith": [], "prerequisites": [], "tags": ["paginated-reports", "formal-documents"], "referenceLinks": [{"label": "Paginated Reports", "url": "https://docs.microsoft.com/en-us/power-bi/paginated-reports/paginated-reports-report-builder-power-bi"}], "estimatedImplementationEffort": "2-4 weeks", "costImplications": "Minimal overhead; scales with report volume"}, {"id": "metrics-scorecard", "name": "Power BI Metrics Scorecards", "domain": "Business Intelligence and Reporting", "domainId": 4, "category": "Report Development", "summary": "Visual display of key metrics with goals, trends, and out-of-range alerts enabling executive dashboards.", "description": "Headcount goal vs actual, cost per hire vs target, time-to-fill trend. Scorecard shows metric, trend, variance from goal.", "fabricComponents": ["Power BI", "Metrics", "Semantic Model"], "pros": ["Executive-friendly visualization.", "Quick identification of variances.", "Supports scorecards at any granularity."], "cons": ["Requires careful metric selection.", "Not suited for deep analysis.", "Goal management overhead."], "usageInstructions": "1. Select metrics. 2. Define goals. 3. Create scorecard visual. 4. Connect to semantic model. 5. Format for execs. 6. Publish.", "governanceConsiderations": "Goals reviewed and approved. Metric definitions consistent. Regular updates. Executive alignment on priorities.", "peopleAnalyticsUseCases": ["Executive dashboard with headcount, cost, turnover metrics.", "Department scorecard with regional goals.", "Recruiting metrics scorecard with time-to-fill, cost-per-hire."], "complexity": "Low", "maturity": "GA", "compatibleWith": ["certified-semantic-model", "directlake-power-bi"], "incompatibleWith": [], "prerequisites": [], "tags": ["metrics-scorecard", "executive-dashboard"], "referenceLinks": [{"label": "Metrics Scorecard", "url": "https://docs.microsoft.com/en-us/power-bi/create-reports/service-metrics-cards"}], "estimatedImplementationEffort": "1-2 weeks", "costImplications": "Minimal overhead"}, {"id": "deployment-pipelines", "name": "Power BI Deployment Pipelines", "domain": "Business Intelligence and Reporting", "domainId": 4, "category": "DevOps", "summary": "Automated promotion of BI artifacts from development through staging to production enabling controlled releases.", "description": "Dev \u2192 Stage \u2192 Prod. Semantic models and reports tested in stage before prod deployment. Reduces errors in production.", "fabricComponents": ["Power BI", "Deployment Pipelines", "Semantic Model", "Reports"], "pros": ["Reduces deployment errors.", "Enables testing before production.", "Audit trail of changes."], "cons": ["Setup complexity.", "Requires discipline in dev/stage separation.", "Can slow development cycles."], "usageInstructions": "1. Create three workspaces: Dev, Stage, Prod. 2. Set up pipeline. 3. Develop in Dev. 4. Deploy to Stage. 5. Test. 6. Deploy to Prod. 7. Monitor.", "governanceConsiderations": "Change approval for Prod. Test in Stage. Monitor Prod performance. Rollback procedures. Post-mortem on failures.", "peopleAnalyticsUseCases": ["Develop dashboard in Dev; promote through Stage to Prod.", "Semantic model change testing before production.", "New report rollout with staging validation."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["certified-semantic-model", "directlake-power-bi"], "incompatibleWith": [], "prerequisites": [], "tags": ["deployment-pipelines", "devops", "power-bi"], "referenceLinks": [{"label": "Deployment Pipelines", "url": "https://docs.microsoft.com/en-us/power-bi/create-reports/deployment-pipelines-overview"}], "estimatedImplementationEffort": "1-2 weeks", "costImplications": "Minimal overhead"}, {"id": "batch-inference-pipeline", "name": "Batch Inference Pipelines", "domain": "Machine Learning and Traditional AI", "domainId": 5, "category": "ML Operationalization", "summary": "Regular batch scoring of employee records against trained ML models producing predictions (attrition risk, salary range) at scale.", "description": "Weekly job scores all active employees with churn model. Output predictions to Gold layer. HR uses for retention focus.", "fabricComponents": ["Spark Notebook", "MLflow", "Feature Store", "Lakehouse"], "pros": ["Scalable scoring for thousands of employees.", "Scheduled inference keeps predictions current.", "Batch approach efficient for throughput."], "cons": ["Latency from batch schedule.", "Storage for predictions grows.", "Model monitoring required."], "usageInstructions": "1. Load feature store. 2. Load trained model from MLflow. 3. Score features. 4. Format output. 5. Write to Gold. 6. Schedule daily/weekly. 7. Monitor scores.", "governanceConsiderations": "Document model assumptions. Monitor prediction distributions. Audit high-risk predictions. Update regularly.", "peopleAnalyticsUseCases": ["Weekly churn risk scoring of all employees.", "Salary range prediction for compensation analysis.", "Performance rating prediction."], "complexity": "High", "maturity": "GA", "compatibleWith": ["feature-store-delta", "mlflow-model-registry", "medallion-architecture"], "incompatibleWith": [], "prerequisites": ["feature-store-delta"], "tags": ["ml", "batch-inference", "predictions"], "referenceLinks": [{"label": "Batch Scoring", "url": "https://docs.microsoft.com/en-us/fabric/data-science/model-serving"}], "estimatedImplementationEffort": "3-4 weeks", "costImplications": "Spark compute scales with employee count"}, {"id": "feature-store-delta", "name": "Feature Store Implementation", "domain": "Machine Learning and Traditional AI", "domainId": 5, "category": "ML Infrastructure", "summary": "Centralized repository of engineered features (tenure_years, salary_percentile) for reuse across ML models reducing redundancy.", "description": "Feature computation once, reuse everywhere. Tenure_years calculated once from hire_date; all models use same value.", "fabricComponents": ["Delta Lake", "Lakehouse", "Feature Store", "MLflow"], "pros": ["Features computed once, reused everywhere.", "Ensures consistency across models.", "Facilitates collaboration."], "cons": ["Setup overhead.", "Feature staleness if not refreshed.", "Storage growth."], "usageInstructions": "1. Design features. 2. Compute features in Spark. 3. Store in Delta tables. 4. Register in feature store. 5. Models reference feature store. 6. Refresh regularly.", "governanceConsiderations": "Document feature logic. Version feature definitions. Monitor freshness. Retire unused features.", "peopleAnalyticsUseCases": ["Tenure, salary percentile, performance rating features.", "Department, manager, location features.", "Historical aggregations: avg salary by dept."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["medallion-architecture", "batch-inference-pipeline", "mlflow-model-registry"], "incompatibleWith": [], "prerequisites": ["medallion-architecture"], "tags": ["feature-store", "ml-infrastructure"], "referenceLinks": [{"label": "Feature Stores", "url": "https://docs.microsoft.com/en-us/fabric/data-science/data-science-overview"}], "estimatedImplementationEffort": "2-3 weeks", "costImplications": "Storage for feature tables; compute for refresh"}, {"id": "mlflow-model-registry", "name": "MLflow Model Registry", "domain": "Machine Learning and Traditional AI", "domainId": 5, "category": "ML Governance", "summary": "Centralized repository for ML models with versioning, staging (Dev/Prod), and metadata enabling model lifecycle management.", "description": "Models registered in MLflow. Dev version tested; Prod version deployed. Version history for rollback. Metadata documents model purpose.", "fabricComponents": ["MLflow", "Spark Notebook", "Model Registry"], "pros": ["Version control for models.", "Staging enables testing.", "Metadata enables governance.", "Easy rollback."], "cons": ["Requires MLflow setup.", "Deployment automation needed.", "Model monitoring overhead."], "usageInstructions": "1. Train model. 2. Register in MLflow. 3. Set Staging=Dev. 4. Test in Stage. 5. Transition to Prod. 6. Deploy. 7. Monitor.", "governanceConsiderations": "Document model purpose, assumptions, metrics. Code review before Prod. Monitor performance. Track versions.", "peopleAnalyticsUseCases": ["Churn model versions tracked and tested.", "Salary prediction model with Prod version.", "Performance rating model with rollback capability."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["batch-inference-pipeline", "feature-store-delta", "model-drift-detection"], "incompatibleWith": [], "prerequisites": ["feature-store-delta"], "tags": ["mlflow", "model-registry", "ml-governance"], "referenceLinks": [{"label": "MLflow Registry", "url": "https://docs.microsoft.com/en-us/fabric/data-science/model-serving"}], "estimatedImplementationEffort": "1-2 weeks", "costImplications": "Minimal overhead for registry"}, {"id": "model-drift-detection", "name": "Model Drift Detection and Monitoring", "domain": "Machine Learning and Traditional AI", "domainId": 5, "category": "ML Monitoring", "summary": "Automated monitoring of model performance metrics detecting drift when predictions no longer match reality triggering retraining.", "description": "Monitor churn model prediction accuracy. If accuracy drops below 70%, alert to retrain. Detect input data distribution changes.", "fabricComponents": ["Spark Notebook", "MLflow", "Monitoring", "Delta Lake"], "pros": ["Early detection of model decay.", "Automated alerts trigger action.", "Historical performance tracking."], "cons": ["Requires ground truth labels.", "Monitoring setup overhead.", "Retraining may be expensive."], "usageInstructions": "1. Define performance metrics. 2. Set baseline and alert thresholds. 3. Score model on new data. 4. Compute metrics. 5. Alert if drift detected. 6. Retrain if needed.", "governanceConsiderations": "Document drift thresholds. Establish retraining SLAs. Track model lifecycle. Post-mortem on failures.", "peopleAnalyticsUseCases": ["Monitor churn model accuracy; retrain monthly.", "Detect salary prediction model degradation.", "Alert on input data distribution shifts."], "complexity": "High", "maturity": "Preview", "compatibleWith": ["batch-inference-pipeline", "mlflow-model-registry"], "incompatibleWith": [], "prerequisites": ["mlflow-model-registry"], "tags": ["model-drift", "monitoring", "ml-ops"], "referenceLinks": [{"label": "Model Monitoring", "url": "https://docs.microsoft.com/en-us/fabric/data-science/model-serving"}], "estimatedImplementationEffort": "2-3 weeks", "costImplications": "Compute for drift detection and retraining"}, {"id": "fairness-bias-evaluation", "name": "Fairness and Bias Evaluation", "domain": "Machine Learning and Traditional AI", "domainId": 5, "category": "ML Ethics", "summary": "Evaluates ML models for bias in predictions across demographic groups (gender, race) ensuring fair and compliant hiring/promotion decisions.", "description": "Churn model predictions should not systematically disfavor any demographic. Test for disparate impact. Document findings.", "fabricComponents": ["Spark Notebook", "Fairness Toolkit", "Delta Lake"], "pros": ["Ensures compliance with bias regulations.", "Detects systematic unfairness.", "Supports ethical decision-making."], "cons": ["Requires labeled demographic data.", "No perfect fairness definition.", "Trade-offs between fairness metrics."], "usageInstructions": "1. Get ground truth + demographics. 2. Run fairness analysis. 3. Compare metrics by group. 4. Document findings. 5. Adjust model if needed. 6. Retest.", "governanceConsiderations": "Privacy-conscious demographic collection. Document assumptions. Legal review. Regular reassessment as data changes.", "peopleAnalyticsUseCases": ["Evaluate churn model for gender bias.", "Assess promotion recommendation fairness.", "Audit salary prediction by race/ethnicity."], "complexity": "High", "maturity": "Emerging", "compatibleWith": ["batch-inference-pipeline", "mlflow-model-registry"], "incompatibleWith": [], "prerequisites": ["mlflow-model-registry"], "tags": ["fairness", "bias", "ethics", "ml"], "referenceLinks": [{"label": "Fairness Analysis", "url": "https://docs.microsoft.com/en-us/azure/machine-learning/concept-fairness"}], "estimatedImplementationEffort": "3-4 weeks", "costImplications": "Analysis compute; typically small"}, {"id": "champion-challenger", "name": "Champion-Challenger Model Testing", "domain": "Machine Learning and Traditional AI", "domainId": 5, "category": "ML Experimentation", "summary": "A/B testing of new model versions (Challenger) against current production model (Champion) in controlled experiments.", "description": "Run new churn model on 10% of employees; compare predictions to current model. If Challenger performs better, promote.", "fabricComponents": ["Spark Notebook", "MLflow", "Delta Lake", "Experimentation"], "pros": ["Safe testing of new models.", "Data-driven promotion decisions.", "Controlled rollout reduces risk."], "cons": ["Requires holdout population.", "Delayed rollout of improvements.", "Complex experiment management."], "usageInstructions": "1. Designate Champion. 2. Train Challenger. 3. Split users: 90% Champion, 10% Challenger. 4. Run experiment. 5. Compare metrics. 6. Promote if better. 7. Full rollout.", "governanceConsiderations": "Ethical experiment design. User consent where needed. Results documentation. Promotion approval process.", "peopleAnalyticsUseCases": ["Test new churn model on subset before full deployment.", "A/B test salary prediction improvements.", "Validate performance rating model changes."], "complexity": "High", "maturity": "GA", "compatibleWith": ["mlflow-model-registry", "batch-inference-pipeline"], "incompatibleWith": [], "prerequisites": ["mlflow-model-registry"], "tags": ["champion-challenger", "ab-testing", "experimentation"], "referenceLinks": [{"label": "A/B Testing", "url": "https://docs.microsoft.com/en-us/azure/machine-learning/concept-responsible-ml"}], "estimatedImplementationEffort": "2-3 weeks", "costImplications": "Compute for dual model scoring"}, {"id": "rag-fabric-grounded", "name": "RAG (Retrieval-Augmented Generation) Fabric-Grounded", "domain": "Generative AI and Conversational Interfaces", "domainId": 6, "category": "Generative AI", "summary": "LLM-powered chatbot that retrieves employee data, org structure, policies from Fabric and generates answers grounded in actual data.", "description": "'What are John's direct reports?' retrieves from org table, passes to LLM which answers. 'Top 5 highest salaries?' retrieves from payroll, generates list.", "fabricComponents": ["Azure OpenAI", "Semantic Model", "Lakehouse", "RAG"], "pros": ["Answers grounded in actual data, not hallucinated.", "Natural language interface to data.", "Reduces manual report requests."], "cons": ["LLM cost with heavy usage.", "Latency from retrieval + generation.", "Requires prompt engineering."], "usageInstructions": "1. Set up vector index on Gold tables. 2. Configure retrieval logic. 3. Connect to LLM API. 4. Test prompts. 5. Integrate with chat interface. 6. Monitor usage.", "governanceConsiderations": "Retrieved data must respect RLS. Sensitive salary data must be masked. Log all queries for audit. Limit user query volume.", "peopleAnalyticsUseCases": ["HR chatbot answering org structure questions.", "Employee compensation bot with salary band info.", "Policy chatbot with benefits/leave policies."], "complexity": "High", "maturity": "Preview", "compatibleWith": ["medallion-architecture", "row-level-security", "semantic-model-certification-pipeline"], "incompatibleWith": [], "prerequisites": ["medallion-architecture", "row-level-security"], "tags": ["rag", "generative-ai", "chatbot"], "referenceLinks": [{"label": "RAG Pattern", "url": "https://docs.microsoft.com/en-us/azure/ai-services/openai/concepts/retrieval-augmented-generation"}], "estimatedImplementationEffort": "4-6 weeks", "costImplications": "OpenAI API costs plus storage for vectors"}, {"id": "secure-chat-rls", "name": "Secure Conversational Interface with RLS", "domain": "Generative AI and Conversational Interfaces", "domainId": 6, "category": "Generative AI", "summary": "Conversational AI interface enforcing row-level security so employees see only authorized data and managers see team data.", "description": "Employee chatbot enforces that employees see personal data only; managers see team data. RLS applied at retrieval.", "fabricComponents": ["Chatbot Framework", "RLS", "Semantic Model", "Azure AD"], "pros": ["Natural interface with built-in security.", "Reduces query errors from RLS confusion.", "Improves user experience."], "cons": ["RLS enforcement in retrieval complex.", "Debugging RLS issues difficult.", "Performance overhead."], "usageInstructions": "1. Implement chatbot. 2. Identify user identity. 3. Apply RLS filter to retrieval. 4. Answer questions respecting RLS. 5. Log queries. 6. Monitor.", "governanceConsiderations": "RLS must be correctly enforced. Audit RLS-filtered queries. Prevent RLS bypass through prompt injection.", "peopleAnalyticsUseCases": ["Employee chatbot showing personal salary only.", "Manager chatbot with team-scoped salary data.", "Executive chatbot with company-wide access."], "complexity": "High", "maturity": "Preview", "compatibleWith": ["row-level-security", "rag-fabric-grounded"], "incompatibleWith": [], "prerequisites": ["row-level-security"], "tags": ["secure-chat", "rls", "conversational"], "referenceLinks": [{"label": "Secure Chatbots", "url": "https://docs.microsoft.com/en-us/power-virtual-agents/"}], "estimatedImplementationEffort": "4-6 weeks", "costImplications": "Chatbot platform + API costs"}, {"id": "azure-ai-foundry-integration", "name": "Azure AI Foundry Integration", "domain": "Generative AI and Conversational Interfaces", "domainId": 6, "category": "Generative AI", "summary": "Integrates Azure AI Foundry (formerly Cognitive Services) for NLP, document understanding, and entity extraction on HR documents.", "description": "Extract key info from resumes, offer letters, termination docs. Classify employee feedback as positive/negative. Structure unstructured data.", "fabricComponents": ["Azure AI Foundry", "Form Recognizer", "Text Analytics", "Lakehouse"], "pros": ["Pre-trained models for common NLP tasks.", "Document understanding with form parsing.", "Reduces custom ML effort."], "cons": ["API costs scale with usage.", "Limited customization compared to custom ML.", "Latency for real-time extraction."], "usageInstructions": "1. Connect Azure AI Foundry. 2. Select service (Form Recognizer, Text Analytics). 3. Call API on documents. 4. Store results in Lakehouse. 5. Reference in analytics.", "governanceConsiderations": "PII in documents must be protected. API calls logged. Retention policy for extracted data.", "peopleAnalyticsUseCases": ["Resume parsing to extract skills, experience.", "Employee feedback analysis: sentiment classification.", "Offer letter extraction: salary, start date.", "Performance review entity extraction."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["medallion-architecture", "spark-notebook-etl"], "incompatibleWith": [], "prerequisites": [], "tags": ["azure-ai", "nlp", "document-understanding"], "referenceLinks": [{"label": "Azure AI Foundry", "url": "https://docs.microsoft.com/en-us/azure/ai-services/"}], "estimatedImplementationEffort": "2-3 weeks", "costImplications": "Azure AI Foundry API costs per request"}, {"id": "copilot-studio-grounded", "name": "Copilot Studio with Fabric Data Grounding", "domain": "Generative AI and Conversational Interfaces", "domainId": 6, "category": "Generative AI", "summary": "Power Platform Copilot Studio integration with Fabric data enabling custom copilots grounded in HR analytics without custom coding.", "description": "Drag-drop copilot builder connecting to Fabric semantic models. Answers grounded in org data. No coding required.", "fabricComponents": ["Copilot Studio", "Power Platform", "Semantic Model", "Fabric"], "pros": ["Low-code copilot creation.", "Native Fabric integration.", "Reduced development time.", "Power Platform ecosystem."], "cons": ["Limited to Power Platform capabilities.", "Customization limited compared to custom code.", "Cost per interaction."], "usageInstructions": "1. Create copilot in Studio. 2. Connect to Fabric semantic model. 3. Define intents. 4. Map to Fabric queries. 5. Test. 6. Deploy.", "governanceConsiderations": "Semantic model permissions enforced. Audit conversations. Sensitive data restrictions.", "peopleAnalyticsUseCases": ["HR chatbot answering org questions.", "Recruiter copilot with candidate data.", "Employee self-service copilot."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["certified-semantic-model", "direct-lake-semantic-model"], "incompatibleWith": [], "prerequisites": [], "tags": ["copilot-studio", "low-code", "generative-ai"], "referenceLinks": [{"label": "Copilot Studio", "url": "https://docs.microsoft.com/en-us/power-virtual-agents/"}], "estimatedImplementationEffort": "2-3 weeks", "costImplications": "Copilot Studio licensing"}, {"id": "semantic-search-vectors", "name": "Semantic Search with Vector Embeddings", "domain": "Generative AI and Conversational Interfaces", "domainId": 6, "category": "Generative AI", "summary": "Vector embeddings of HR data (job descriptions, policies, performance reviews) enabling semantic similarity search.", "description": "Embed job descriptions, search 'find roles similar to engineer.' Embed policies, search 'parental leave policy.'", "fabricComponents": ["Vector Store", "Embeddings API", "Lakehouse", "Semantic Search"], "pros": ["Semantic similarity beyond keyword match.", "Supports RAG and discovery use cases.", "Natural language search."], "cons": ["Storage overhead for vectors.", "Vector quality depends on embedding model.", "Refresh complexity."], "usageInstructions": "1. Embed HR content. 2. Store vectors in vector DB. 3. On query, embed user query. 4. Search vector space. 5. Return top matches. 6. Feed to LLM.", "governanceConsiderations": "Vectors must preserve privacy of embedded content. Refresh vectors when source updates.", "peopleAnalyticsUseCases": ["Job description semantic search: find similar roles.", "Policy search: natural language policy questions.", "Performance review search: find similar feedback."], "complexity": "High", "maturity": "Preview", "compatibleWith": ["rag-fabric-grounded", "medallion-architecture"], "incompatibleWith": [], "prerequisites": [], "tags": ["semantic-search", "vectors", "embeddings"], "referenceLinks": [{"label": "Vector Search", "url": "https://docs.microsoft.com/en-us/azure/search/vector-search-overview"}], "estimatedImplementationEffort": "3-4 weeks", "costImplications": "Vector DB storage plus embedding API costs"}, {"id": "llm-auto-narrative", "name": "LLM Auto-Generated Narratives", "domain": "Generative AI and Conversational Interfaces", "domainId": 6, "category": "Generative AI", "summary": "LLM automatically generates narrative descriptions of dashboards, trends, and insights reducing reporting burden.", "description": "Dashboard shows headcount down 5% YoY. LLM generates: 'Headcount declined 5% year-over-year to 1,200 from 1,263.'", "fabricComponents": ["Azure OpenAI", "Power BI", "Lakehouse"], "pros": ["Reduces manual report writing.", "Generates consistent narratives.", "Scales insights to many users."], "cons": ["LLM cost with scale.", "Narrative quality depends on prompts.", "May need human review for critical reports."], "usageInstructions": "1. Extract dashboard metrics. 2. Format for LLM. 3. Call LLM with prompt template. 4. Generate narrative. 5. Insert in report. 6. Review.", "governanceConsiderations": "LLM outputs must be reviewed before publication. Sensitive data in narratives must be protected.", "peopleAnalyticsUseCases": ["Dashboard narrative generation for executive summary.", "Trend description in reports.", "Anomaly narration.", "Insight summarization."], "complexity": "Medium", "maturity": "Preview", "compatibleWith": ["medallion-architecture", "directlake-power-bi"], "incompatibleWith": [], "prerequisites": [], "tags": ["llm", "auto-narrative", "generative-ai"], "referenceLinks": [{"label": "LLM Integration", "url": "https://docs.microsoft.com/en-us/azure/ai-services/openai/"}], "estimatedImplementationEffort": "2-3 weeks", "costImplications": "OpenAI API costs per narrative generation"}, {"id": "hr-ai-guardrails", "name": "HR-Specific AI Guardrails and Safety", "domain": "Generative AI and Conversational Interfaces", "domainId": 6, "category": "Generative AI", "summary": "Guardrails preventing AI from making inappropriate recommendations on hiring, termination, or compensation decisions.", "description": "Prevent salary recommendations based on protected attributes. Prevent discriminatory hiring suggestions. Audit all recommendations.", "fabricComponents": ["Azure OpenAI", "Guardrails Framework", "Auditing"], "pros": ["Prevents discriminatory AI outputs.", "Ensures compliance with employment law.", "Reduces liability.", "Builds user trust."], "cons": ["Guardrails overhead.", "May block valid use cases.", "Guardrail effectiveness hard to measure."], "usageInstructions": "1. Define guardrails (no gender/race in salary), 2. Configure content filter. 3. Monitor LLM outputs. 4. Log violations. 5. Audit. 6. Retrain if needed.", "governanceConsiderations": "Legal review of guardrails. Regular guardrail testing. Audit trail of all recommendations. Transparency with users.", "peopleAnalyticsUseCases": ["Salary recommendation guardrail: no gender/race consideration.", "Hiring recommendation guardrail: prevent age bias.", "Termination recommendation: require human approval."], "complexity": "High", "maturity": "Emerging", "compatibleWith": ["rag-fabric-grounded", "fairness-bias-evaluation"], "incompatibleWith": [], "prerequisites": [], "tags": ["guardrails", "ai-safety", "compliance"], "referenceLinks": [{"label": "Responsible AI", "url": "https://docs.microsoft.com/en-us/azure/machine-learning/concept-responsible-ml"}], "estimatedImplementationEffort": "4-6 weeks", "costImplications": "Guardrails framework plus monitoring overhead"}, {"id": "data-activator-reflex", "name": "Data Activator (Reflex) for Auto-Actions", "domain": "Alerting, Automation, and Operational Intelligence", "domainId": 7, "category": "Automation", "summary": "Automatic workflows triggered by data anomalies: high turnover in department \u2192 auto-email recruiter; forecast miss \u2192 escalate to VP.", "description": "Monitor headcount by department. If any dept loses >10% of staff in a month, auto-trigger recruiting workflow. Alert manager.", "fabricComponents": ["Data Activator", "Reflex", "Semantic Model", "Power Automate"], "pros": ["Responds to anomalies automatically.", "Reduces manual monitoring.", "Scales to many metrics."], "cons": ["False positives trigger wasted workflows.", "Complex rule logic is hard to debug.", "Costs with action volume."], "usageInstructions": "1. Create rule on metric. 2. Define trigger condition. 3. Select action (email, Power Automate). 4. Test. 5. Deploy. 6. Monitor.", "governanceConsiderations": "Rules must be approved. Test before prod. Monitor false positives. Log all actions. Disable poorly performing rules.", "peopleAnalyticsUseCases": ["High turnover alert \u2192 send recruiter email.", "Salary anomaly detection \u2192 escalate to manager.", "Hiring target miss \u2192 VP alert.", "Compliance violation \u2192 legal escalation."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["medallion-architecture", "certified-semantic-model"], "incompatibleWith": [], "prerequisites": ["medallion-architecture"], "tags": ["data-activator", "reflex", "automation"], "referenceLinks": [{"label": "Data Activator", "url": "https://docs.microsoft.com/en-us/fabric/real-time-analytics/data-activator/data-activator-introduction"}], "estimatedImplementationEffort": "1-2 weeks per reflex", "costImplications": "Minimal overhead; Power Automate action costs"}, {"id": "power-automate-triggers", "name": "Power Automate Workflows with Data Triggers", "domain": "Alerting, Automation, and Operational Intelligence", "domainId": 7, "category": "Automation", "summary": "Power Automate workflows triggered by HR data changes automating notifications, approvals, and downstream processes.", "description": "When new hire is added, automatically send welcome email, provision accounts, schedule onboarding. New termination \u2192 disable access.", "fabricComponents": ["Power Automate", "Warehouse", "Lakehouse", "Connectors"], "pros": ["Visual workflow automation.", "Wide connector ecosystem.", "Reduces manual tasks."], "cons": ["Performance at scale is limited.", "Complex logic becomes hard to maintain.", "Cost per workflow run."], "usageInstructions": "1. Create flow. 2. Set Fabric data trigger. 3. Add actions (email, API call). 4. Test. 5. Enable. 6. Monitor.", "governanceConsiderations": "Flows must be approved. Sensitive actions require confirmation. Audit trail of executions. Disable unused flows.", "peopleAnalyticsUseCases": ["New hire trigger \u2192 welcome email, account provisioning.", "Termination trigger \u2192 disable access, exit survey.", "Promotion trigger \u2192 new org reporting, email announcement.", "Review due date \u2192 email reminder to managers."], "complexity": "Low", "maturity": "GA", "compatibleWith": ["medallion-architecture", "data-activator-reflex"], "incompatibleWith": [], "prerequisites": [], "tags": ["power-automate", "workflows", "automation"], "referenceLinks": [{"label": "Power Automate", "url": "https://docs.microsoft.com/en-us/power-automate/"}], "estimatedImplementationEffort": "1-2 days per workflow", "costImplications": "Power Automate licensing; per-action costs"}, {"id": "metric-summarization-engine", "name": "Metric Summarization Engine", "domain": "Alerting, Automation, and Operational Intelligence", "domainId": 7, "category": "Operational Intelligence", "summary": "Automated daily/weekly email summaries of key metrics with trends and anomalies delivered to executives without manual compilation.", "description": "Daily HR metrics digest: headcount, turnover, hiring pipeline, cost summary. Auto-generated, delivered to execs 7am.", "fabricComponents": ["Lakehouse", "Semantic Model", "Automation", "Email"], "pros": ["Executives get key metrics automatically.", "Reduces report compilation time.", "Consistent delivery schedule."], "cons": ["Metric selection bias (may miss important changes).", "Email fatigue if too frequent.", "Setup complexity."], "usageInstructions": "1. Define metrics. 2. Create automated query. 3. Format results. 4. Schedule (daily/weekly). 5. Email to distribution list. 6. Monitor engagement.", "governanceConsiderations": "Metrics reviewed by leadership. Delivery list managed. Format consistent. Performance tracked.", "peopleAnalyticsUseCases": ["Daily HR dashboard metrics summary.", "Weekly turnover report with anomalies.", "Monthly compensation report to executives.", "Quarterly talent metrics digest."], "complexity": "Low", "maturity": "GA", "compatibleWith": ["medallion-architecture", "directlake-power-bi"], "incompatibleWith": [], "prerequisites": [], "tags": ["summarization", "automation", "metrics"], "referenceLinks": [{"label": "Scheduled Queries", "url": "https://docs.microsoft.com/en-us/fabric/"}], "estimatedImplementationEffort": "1-2 weeks", "costImplications": "Minimal compute; scheduling overhead"}, {"id": "sla-freshness-monitoring", "name": "SLA Freshness Monitoring", "domain": "Alerting, Automation, and Operational Intelligence", "domainId": 7, "category": "Operational Intelligence", "summary": "Monitoring pipeline SLAs ensuring data is refreshed within defined window (midnight refresh by 6am) with alerts on violations.", "description": "Gold layer must refresh by 6am. If refresh doesn't complete, alert ops team. Track SLA compliance month-over-month.", "fabricComponents": ["Data Factory", "Monitoring", "Alerting"], "pros": ["Ensures users get fresh data on time.", "Quick detection of pipeline failures.", "SLA compliance visibility."], "cons": ["SLA setup requires discipline.", "False alerts from planned maintenance.", "Debugging SLA misses complex."], "usageInstructions": "1. Define SLA (e.g., refresh by 6am). 2. Monitor pipeline completion. 3. Alert if missed. 4. Track compliance %. 5. Post-mortem on misses.", "governanceConsiderations": "SLAs set realistically. Escalation procedures defined. Maintenance windows coordinated. Compliance reported.", "peopleAnalyticsUseCases": ["Gold layer refresh by 6am SLA monitoring.", "Payroll data SLA: reconcile by 9am.", "Recruiting pipeline SLA: updates daily.", "Executive dashboard SLA: available by 7am."], "complexity": "Low", "maturity": "GA", "compatibleWith": ["medallion-architecture", "data-activator-reflex"], "incompatibleWith": [], "prerequisites": [], "tags": ["sla", "freshness", "monitoring"], "referenceLinks": [{"label": "Pipeline Monitoring", "url": "https://docs.microsoft.com/en-us/fabric/"}], "estimatedImplementationEffort": "1-2 weeks", "costImplications": "Minimal overhead for monitoring"}, {"id": "escalation-routing", "name": "Automated Escalation and Routing", "domain": "Alerting, Automation, and Operational Intelligence", "domainId": 7, "category": "Automation", "summary": "Smart routing of alerts to appropriate teams based on severity and data ownership (high turnover \u2192 dept head; data quality issue \u2192 data team).", "description": "High turnover alert goes to HR head. Data quality issue goes to data team. Ownership-based routing ensures right person acts.", "fabricComponents": ["Alerting", "Routing Engine", "Power Automate"], "pros": ["Right person gets right alert.", "Reduces response time.", "Prevents alert fatigue."], "cons": ["Routing logic becomes complex.", "Ownership must be maintained.", "Depends on accurate severity classification."], "usageInstructions": "1. Define alert types. 2. Map to owners. 3. Set routing rules. 4. Configure notifications. 5. Test. 6. Monitor effectiveness.", "governanceConsiderations": "Ownership assignments maintained. Routing rules reviewed. Escalation paths clear. Response SLAs defined.", "peopleAnalyticsUseCases": ["High turnover \u2192 department head.", "Data quality issue \u2192 data steward.", "Salary anomaly \u2192 compensation manager.", "Compliance issue \u2192 HR legal."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["data-activator-reflex", "power-automate-triggers"], "incompatibleWith": [], "prerequisites": [], "tags": ["escalation", "routing", "alerting"], "referenceLinks": [{"label": "Routing Logic", "url": "https://docs.microsoft.com/en-us/power-automate/"}], "estimatedImplementationEffort": "2-3 weeks", "costImplications": "Minimal overhead"}, {"id": "cross-workspace-shortcuts", "name": "Cross-Workspace Data Sharing via Shortcuts", "domain": "Data Sharing and Distribution", "domainId": 8, "category": "Data Distribution", "summary": "Shortcuts enable different workspaces (HR, Finance, Recruiting) to share data without copying, maintaining single source of truth.", "description": "Finance workspace has authoritative employee master and cost centers. HR and Recruiting create shortcuts, always get latest data.", "fabricComponents": ["OneLake", "Shortcuts", "Workspaces"], "pros": ["Single source of truth.", "Zero-copy sharing.", "Automatic updates visible."], "cons": ["Cross-workspace latency.", "Complex ownership management.", "Lineage harder to track."], "usageInstructions": "1. Source workspace has data. 2. Consumer workspace creates shortcut. 3. Reference shortcut like local table. 4. Monitor performance.", "governanceConsiderations": "Source table ownership clear. Data contracts documented. Shortcut access controlled. Performance monitored.", "peopleAnalyticsUseCases": ["Finance employee master shared to HR via shortcuts.", "Recruiting access cost center via Finance shortcut.", "All teams access central org structure."], "complexity": "Low", "maturity": "GA", "compatibleWith": ["onelake-shortcuts", "hub-spoke-workspace"], "incompatibleWith": [], "prerequisites": [], "tags": ["shortcuts", "cross-workspace", "sharing"], "referenceLinks": [{"label": "Cross-Workspace Shortcuts", "url": "https://docs.microsoft.com/en-us/fabric/onelake/onelake-shortcuts"}], "estimatedImplementationEffort": "1-2 days", "costImplications": "Zero-copy saves significantly"}, {"id": "cross-tenant-sharing", "name": "Cross-Tenant Data Sharing (B2B Scenarios)", "domain": "Data Sharing and Distribution", "domainId": 8, "category": "Data Distribution", "summary": "Sharing data across Azure AD tenants enabling partner organizations to access shared analytics while maintaining security.", "description": "Parent company shares org structure and benchmark data with subsidiary. Subsidiary accesses via external shortcuts securely.", "fabricComponents": ["OneLake", "Shortcuts", "Tenants", "Azure AD"], "pros": ["Enables partner collaboration.", "Maintains security across tenants.", "Zero-copy for cross-tenant data."], "cons": ["Complex permission management.", "Cross-tenant latency.", "Requires Azure AD trust."], "usageInstructions": "1. Source tenant grants access. 2. Target tenant creates shortcut. 3. Authenticate across tenants. 4. Reference shortcut.", "governanceConsiderations": "Explicit cross-tenant contracts. Legal agreements. Access regularly reviewed. Sensitive data restricted.", "peopleAnalyticsUseCases": ["Parent company shares benchmarks with subsidiary.", "Shared services org shares data with business units.", "JV shares hiring data with partners."], "complexity": "High", "maturity": "Preview", "compatibleWith": ["onelake-shortcuts"], "incompatibleWith": [], "prerequisites": [], "tags": ["cross-tenant", "b2b", "sharing"], "referenceLinks": [{"label": "Tenant Collaboration", "url": "https://docs.microsoft.com/en-us/fabric/"}], "estimatedImplementationEffort": "4-6 weeks", "costImplications": "Minimal overhead; security complexity"}, {"id": "semantic-model-certification-pipeline", "name": "Semantic Model Certification Pipeline", "domain": "Data Sharing and Distribution", "domainId": 8, "category": "BI Governance", "summary": "Automated certification workflow where data teams publish semantic models and BI team verifies quality before marking Certified.", "description": "Data team publishes employee dimension model. BI team runs tests, checks metadata, certifies if passed. Analysts use certified models.", "fabricComponents": ["Semantic Model", "Power BI", "CI/CD", "Approval Workflow"], "pros": ["Ensures semantic model quality.", "Reduces model duplication.", "Governance automation."], "cons": ["Slows model time-to-value.", "Certification bottleneck.", "Requires clear criteria."], "usageInstructions": "1. Data team publishes model. 2. Auto-run quality checks. 3. BI team reviews. 4. Approve/reject. 5. Publish as Certified. 6. Analysts use.", "governanceConsiderations": "Certification criteria clear. Review time SLA defined. Criteria version controlled. Regular audit of certified models.", "peopleAnalyticsUseCases": ["Employee dimension certification before BI use.", "Payroll semantic model certification.", "Org structure model quality gates.", "Metrics model certification."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["certified-semantic-model", "deployment-pipelines"], "incompatibleWith": [], "prerequisites": [], "tags": ["semantic-model", "certification", "governance"], "referenceLinks": [{"label": "Model Certification", "url": "https://docs.microsoft.com/en-us/power-bi/collaborate-share/service-certify-datasets"}], "estimatedImplementationEffort": "2-3 weeks", "costImplications": "Minimal overhead; certification automation"}, {"id": "api-exposure-management", "name": "REST API Exposure and Management", "domain": "Data Sharing and Distribution", "domainId": 8, "category": "Data Distribution", "summary": "Expose curated analytics data via REST APIs enabling external systems (HRIS, payroll, recruiting platforms) to consume Fabric data.", "description": "REST API endpoints for employee master, org structure, payroll data. External HRIS calls API to fetch org updates. Managed API throttling.", "fabricComponents": ["Warehouse", "SQL Endpoints", "API Management", "REST API"], "pros": ["Programmatic data access.", "Enables external integrations.", "API governance and throttling.", "Reduces data replication."], "cons": ["API security overhead.", "Performance tuning needed.", "Version management complexity."], "usageInstructions": "1. Create API Management instance. 2. Expose Warehouse via API. 3. Define endpoints. 4. Set throttling. 5. Manage keys. 6. Monitor usage.", "governanceConsiderations": "API authentication required. Rate limiting enforced. Data access logged. Sensitive data restricted at API level.", "peopleAnalyticsUseCases": ["API for employee master to external HRIS.", "Org structure API to recruiting platforms.", "Payroll data API to finance systems.", "Compensation band API to offer letter system."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["lakehouse-warehouse-selection", "row-level-security"], "incompatibleWith": [], "prerequisites": [], "tags": ["api", "rest", "data-distribution"], "referenceLinks": [{"label": "API Management", "url": "https://docs.microsoft.com/en-us/azure/api-management/"}], "estimatedImplementationEffort": "2-3 weeks", "costImplications": "API Management licensing plus query compute"}, {"id": "dataset-subscription-alerting", "name": "Dataset Subscription and Change Alerts", "domain": "Data Sharing and Distribution", "domainId": 8, "category": "Data Distribution", "summary": "Subscribers receive alerts when datasets change or refresh, enabling downstream systems to react to data updates.", "description": "Recruiting platform subscribes to org structure changes. When org structure table updates, recruiting system receives alert, refetches data.", "fabricComponents": ["Event Grid", "Webhooks", "Lakehouse", "Subscribers"], "pros": ["Push-based data distribution.", "Subscribers notified of changes.", "Reduces polling.", "Real-time integration."], "cons": ["Webhook management overhead.", "Failure handling complexity.", "Debugging integration issues."], "usageInstructions": "1. Configure Event Grid. 2. Define dataset change events. 3. Create webhooks for subscribers. 4. Subscribers listen. 5. Notify on change. 6. Handle failures.", "governanceConsiderations": "Subscription management. Event audit trail. Webhook security. Retry policies defined.", "peopleAnalyticsUseCases": ["Org structure change alerts to recruiting system.", "Employee master update alerts to payroll.", "Compensation change alerts to benefits system.", "Headcount change alerts to planning tools."], "complexity": "Medium", "maturity": "Preview", "compatibleWith": ["medallion-architecture", "data-activator-reflex"], "incompatibleWith": [], "prerequisites": [], "tags": ["subscriptions", "webhooks", "event-driven"], "referenceLinks": [{"label": "Event Grid", "url": "https://docs.microsoft.com/en-us/azure/event-grid/overview"}], "estimatedImplementationEffort": "2-3 weeks", "costImplications": "Event Grid pricing plus webhook compute"}, {"id": "encryption-at-rest-cmk", "name": "Encryption at Rest with Customer-Managed Keys", "domain": "Data Governance and Security", "domainId": 3, "category": "Encryption", "summary": "Implements encryption at rest using customer-managed keys (CMK) stored in Azure Key Vault for OneLake workspace data. Ensures TLS 1.2+ for all data in transit and maintains FIPS 140-2 compliance for sensitive HR data.", "description": "Customer-Managed Key (CMK) encryption provides organizational control over encryption key lifecycle and rotation. In financial services HR analytics, CMK encryption via Azure Key Vault ensures that OneLake workspace data is encrypted with keys managed by your organization, not Microsoft. All data in transit uses TLS 1.2 or higher. This approach meets regulatory requirements for key custody, enables key rotation policies, supports audit logging for key access, and provides compliance with FIPS 140-2 standards for handling highly sensitive employee financial and personal data. Integration with Entra ID service principals allows role-based access control over decryption operations.", "fabricComponents": ["Azure Key Vault", "OneLake", "Fabric Workspace Settings", "Entra ID Service Principal"], "pros": ["Provides organizational control over encryption keys with full audit trail of key access and rotations.", "Enables compliance with regulations requiring customer-managed encryption (SOX, GDPR, PIPEDA, HIPAA).", "Supports break-glass emergency key access protocols and disaster recovery scenarios."], "cons": ["Increases operational complexity requiring key rotation and lifecycle management procedures.", "Key Vault service calls add latency to Fabric operations, typically <10ms but noticeable at scale.", "Requires careful IAM design to prevent accidental key lockout that could make data unrecoverable."], "usageInstructions": "1. Create an Azure Key Vault resource in the same region as Fabric capacity. 2. Generate RSA 3072-bit or 4096-bit CMK in Key Vault. 3. Grant Fabric capacity system-assigned managed identity Key Wrap/Unwrap permissions on the key. 4. In Fabric Workspace Settings, select 'Customer Managed Key' and specify the Key Vault URI. 5. Configure key rotation policy (annual minimum). 6. Enable Key Vault audit logging and monitor access. 7. Test disaster recovery failover procedures quarterly.", "governanceConsiderations": "Establish a key management committee with business and security stakeholders. Document key rotation procedures and emergency access protocols. Implement access reviews quarterly for Key Vault permissions. Ensure backup keys are stored in a geographically separate region. Monitor failed decryption attempts and investigate anomalies. Integrate key rotation events into change management processes.", "peopleAnalyticsUseCases": ["Salary survey analysis where payroll data is encrypted with organization-controlled CMK, enabling internal audits while maintaining key custody.", "Executive compensation dashboards requiring SOX compliance where CMK encryption demonstrates regulatory control over sensitive executive personal data.", "International employee equity tracking where Canadian employee data uses Canadian Key Vault instance and US data uses US Key Vault for data residency compliance."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["sensitivity-labels", "purview-data-map", "dynamic-data-masking", "row-level-security"], "incompatibleWith": [], "prerequisites": [], "tags": ["encryption", "security", "compliance", "key-management", "financial-services"], "referenceLinks": [{"label": "Encryption at Rest in Microsoft Fabric", "url": "https://learn.microsoft.com/en-us/fabric/enterprise/encryption-at-rest"}, {"label": "Azure Key Vault Overview", "url": "https://learn.microsoft.com/en-us/azure/key-vault/general/overview"}, {"label": "FIPS 140-2 Compliance in Azure", "url": "https://learn.microsoft.com/en-us/compliance/regulatory/offering-fips-140-2"}], "estimatedImplementationEffort": "2-3 weeks", "costImplications": "Azure Key Vault: $0.34/10k operations; CMK key storage $1/month. Key Vault calls add minimal overhead."}, {"id": "dlp-policy-enforcement", "name": "Data Loss Prevention Policy Enforcement", "domain": "Data Governance and Security", "domainId": 3, "category": "Data Protection", "summary": "Configures Microsoft Purview Data Loss Prevention (DLP) policies to detect and block unauthorized export or sharing of sensitive information types including SSN, salary data, and bank account numbers.", "description": "Data Loss Prevention (DLP) policies in Microsoft Purview automatically detect sensitive information types within Fabric and Power BI, then enforce preventive or detective controls. For HR analytics, DLP policies identify patterns matching US Social Security Numbers, salary ranges, employee IDs, and bank account numbers. When detected, policies can block exports to CSV/Excel, prevent uploads to personal OneDrive, restrict sharing to external domains, require approval for sensitive queries, or log incidents to Azure Sentinel for SOC review. DLP works alongside sensitivity labels to provide multi-layered protection. Policies can be scoped to specific workspaces, datasets, or semantic models. Integration with Sensitivity Labels and Power BI Workspace settings ensures consistent enforcement across analytics surfaces.", "fabricComponents": ["Microsoft Purview DLP", "Sensitivity Labels", "Power BI", "OneLake"], "pros": ["Detects sensitive data automatically using built-in and custom regex patterns without manual classification.", "Blocks unauthorized export at the point of action (export, copy, print) reducing insider threat risk.", "Provides audit trail of blocked/allowed actions and integrates with SIEM for incident response workflows."], "cons": ["Can trigger false positives on benign data patterns, requiring regular tuning and user exceptions.", "DLP policies are primarily detective in Power BI; preventive enforcement requires specific workspace integration.", "Regex patterns for custom sensitive data types require security team expertise and ongoing maintenance."], "usageInstructions": "1. Open Microsoft Purview Compliance Center and navigate to Data Loss Prevention > Policies. 2. Create new policy with template 'Financial Info' or 'PII' as baseline. 3. Add custom sensitive info types: SSN regex (^\\d{3}-\\d{2}-\\d{4}$), salary range (\\$[0-9]{3,4}[KM]). 4. Configure rule actions: 'Restrict access' for Power BI, 'Require justification' for exports, 'Send alert to admin'. 5. Set policy scope to HR workspace and datasets. 6. Enable incident reporting to Azure Sentinel. 7. Test policy with sample data before production rollout. 8. Review blocked incidents monthly.", "governanceConsiderations": "Establish a DLP governance committee with HR, security, and legal teams. Document all custom sensitive data type patterns and business justification. Implement exception request workflow with business and compliance approval. Monitor false positive rate and adjust thresholds quarterly. Ensure DLP incidents are correlated with user access logs and audit trails. Train HR analytics team on compliant export procedures.", "peopleAnalyticsUseCases": ["Automated blocking of salary equity analysis exports containing aggregated ranges to unauthorized recipients, while allowing HR team to export to approved HR department network shares.", "Detection and alert when employee benefit election data containing SSN is copied to clipboard or exported to personal email, triggering incident response investigation.", "Prevention of executive compensation dashboard drill-through exports to external consultants without pre-approval workflow, enforcing approval for sensitive roles."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["sensitivity-labels", "row-level-security", "dynamic-data-masking", "audit-siem-integration"], "incompatibleWith": [], "prerequisites": ["sensitivity-labels"], "tags": ["data-loss-prevention", "dlp", "compliance", "data-protection", "security"], "referenceLinks": [{"label": "Data Loss Prevention in Microsoft Purview", "url": "https://learn.microsoft.com/en-us/purview/dlp-learn-about-dlp"}, {"label": "DLP Policy Creation and Management", "url": "https://learn.microsoft.com/en-us/purview/create-test-tune-dlp-policy"}, {"label": "Sensitive Information Types in Purview", "url": "https://learn.microsoft.com/en-us/purview/sensitive-information-type-learn-about"}], "estimatedImplementationEffort": "3-4 weeks", "costImplications": "Included in Purview premium license (part of Microsoft 365 E5 or standalone). DLP policy evaluation adds <1% query latency."}, {"id": "pii-tokenization", "name": "PII Tokenization and Pseudonymization", "domain": "Data Transformation and Processing", "domainId": 2, "category": "Privacy Engineering", "summary": "Replaces Personally Identifiable Information (PII) with deterministic tokens using Microsoft Presidio and PySpark, enabling data sharing and analytics without exposing actual personal data.", "description": "PII tokenization uses deterministic hashing to replace sensitive personal identifiers with tokens while maintaining the ability to link records. Microsoft Presidio, an AI-powered data protection service, automatically detects PII entities (names, addresses, phone numbers, emails) within notebooks and datasets. Identified PII is replaced with tokens, with the mapping stored securely in Azure Key Vault. For HR analytics, this enables sharing anonymized talent data with third-party vendors, consultants, and development teams without exposing actual employee identity. Deterministic tokenization (same PII always maps to same token) preserves join semantics. PySpark transformations within Fabric Notebooks perform tokenization at scale. Different token formats support different use cases: hash tokens for aggregate analytics, UUID tokens for record-level joins. Reverse lookup tokens require Key Vault authorization for decryption.", "fabricComponents": ["Fabric Notebooks", "PySpark", "Microsoft Presidio", "Azure Key Vault", "Lakehouse"], "pros": ["Enables secure data sharing with business partners and vendors by removing actual identity without losing join capability.", "Detects PII automatically using AI models rather than relying on manual regex patterns, catching complex PII patterns.", "Supports different tokenization strategies per use case: hash-only tokens for analytics, reversible tokens for authorized users with Key Vault access."], "cons": ["Deterministic tokenization vulnerable to frequency analysis if PII distribution is skewed (e.g., only 3 VP names).", "Presidio requires pre-trained models and tuning to avoid false positives/negatives in organization-specific contexts.", "Large-scale tokenization can impact notebook execution time; requires parallel PySpark processing for efficiency."], "usageInstructions": "1. Install Presidio in Fabric Notebook: pip install presidio-analyzer presidio-anonymizer. 2. Load employee data in PySpark DataFrame. 3. Configure Presidio analyzer with entity patterns: PERSON, PHONE_NUMBER, EMAIL_ADDRESS. 4. Create tokenization mapping: use Presidio to detect PII, generate deterministic SHA-256 hash token or UUID. 5. Store mapping in Key Vault secret with access restricted to governance team. 6. Replace PII with tokens using Presidio anonymizer. 7. Save tokenized dataset to Silver/Gold layer. 8. Log all tokenization operations with user, timestamp, entity count. 9. Test reverse lookup process with authorized user.", "governanceConsiderations": "Establish PII tokenization review board with privacy, security, and HR stakeholders. Document which entity types are tokenized and which remain for legitimate analytics. Implement access controls requiring Key Vault authorization for reverse lookups. Monitor tokenization logs for suspicious decryption patterns. Conduct quarterly audits of Presidio configuration to ensure no drift in entity detection. Maintain separate tokenization keys per data consumer or use case.", "peopleAnalyticsUseCases": ["Sharing anonymized employee experience survey data with external consulting firm to analyze cultural trends without exposing employee names, emails, or departments.", "Enabling development team to use production-like employee hierarchy and org structure data for testing without accessing real employee identities, using UUID tokens in place of names.", "Providing recruitment analytics to third-party recruitment platform showing skills and experience patterns without revealing actual employee identity, enabling vendor to match internal talent to opportunities."], "complexity": "High", "maturity": "Emerging", "compatibleWith": ["medallion-architecture", "spark-notebook-etl", "data-quality-validation", "dlp-policy-enforcement"], "incompatibleWith": [], "prerequisites": [], "tags": ["pii", "anonymization", "privacy", "data-sharing", "presidio"], "referenceLinks": [{"label": "Microsoft Presidio Overview", "url": "https://learn.microsoft.com/en-us/presidio/"}, {"label": "Presidio with Fabric Notebooks", "url": "https://github.com/microsoft/presidio"}, {"label": "Data Anonymization Techniques", "url": "https://learn.microsoft.com/en-us/compliance/regulatory/gdpr-data-subject-rights"}], "estimatedImplementationEffort": "4-6 weeks", "costImplications": "Presidio: open-source, no licensing cost. Key Vault storage: minimal. Spark compute: standard Fabric notebook rates."}, {"id": "data-retention-lifecycle", "name": "Automated Data Retention and Purge Pipeline", "domain": "Data Transformation and Processing", "domainId": 2, "category": "Lifecycle Management", "summary": "Implements scheduled automated pipelines that evaluate data age against retention rules, execute secure deletion with audit trails, and support legal hold overrides for compliance.", "description": "Data retention and purge pipelines ensure that employee data is not retained longer than required by law or business policy. For HR analytics, employee records must be retained per employment law (typically 3-7 years depending on jurisdiction), then securely deleted. Automated pipelines check record creation/modification dates against configurable retention policies, identify data eligible for purge, execute secure deletion via soft-delete flags or physical removal, and generate audit logs. Legal holds prevent deletion during litigation or regulatory investigations. Data Factory pipelines coordinate with Fabric notebooks to identify eligible records using SQL queries (SELECT * WHERE modified_date < DATEADD(year,-3,GETDATE())), mark for deletion, execute deletion operations, and log to audit tables. Soft-delete approach (marking records inactive) preserves referential integrity and enables recovery. Hard-delete (physical removal from Delta Lake) reduces storage and improves query performance but requires careful execution.", "fabricComponents": ["Data Factory Pipeline", "Fabric Notebooks", "OneLake", "Delta Lake", "Lakehouse"], "pros": ["Automates compliance with data retention regulations (GDPR right-to-erasure, employment law, SOX records retention).", "Reduces storage costs by removing obsolete data and improves query performance on smaller active datasets.", "Maintains complete audit trail of deleted records and reasons, supporting forensic investigations and compliance audits."], "cons": ["Incorrectly configured retention rules can cause accidental data loss; requires thorough testing and change management approval.", "Soft-delete approaches maintain referential integrity but require query filters to exclude deleted records; hard-delete is faster but riskier.", "Legal holds must be tracked separately; coordination between legal, HR, and data teams is required to manage hold status."], "usageInstructions": "1. Define retention policies in configuration table: entity type, retention period (years), jurisdiction, legal hold indicator. 2. Create Data Factory pipeline with Copy Activity to identify records eligible for purge using SQL: SELECT * WHERE datediff(year, modified_date, getdate()) >= retention_years AND legal_hold = 0. 3. Use Fabric Notebook to mark records as deleted (UPDATE table SET is_deleted = 1 WHERE id IN (...)) instead of hard delete initially. 4. Execute soft-delete daily/weekly per schedule. 5. After 30-day recovery period, run hard delete: VACUUM table_name RETAIN 0 HOURS; DELETE FROM table WHERE is_deleted = 1. 6. Log all operations: user, timestamp, record count, reasons. 7. Send audit report to compliance/legal team monthly.", "governanceConsiderations": "Establish data retention committee with legal, HR, compliance, and data teams. Document retention policies for each data entity type by jurisdiction. Implement approval workflow for legal hold changes. Set up audit alerts for large purge operations. Conduct quarterly audits of purge logs to verify correctness. Test disaster recovery to ensure deleted data cannot be recovered from backups without explicit authorization. Maintain retention policy version control.", "peopleAnalyticsUseCases": ["Automatically delete contingent worker records 2 years after separation date and contract end, enabling compliance with employment laws while reducing data footprint.", "Purge applicant data after hiring decision per FCRA requirements (typically 3 years), except for records under litigation hold from employment disputes.", "Soft-delete employee benefit elections from terminated employees after 7 years but preserve audit trail for pension calculations and benefit inquiries, with legal hold preventing deletion during claims."], "complexity": "High", "maturity": "Emerging", "compatibleWith": ["medallion-architecture", "spark-notebook-etl", "delta-lake-partitioning", "audit-siem-integration"], "incompatibleWith": [], "prerequisites": ["medallion-architecture"], "tags": ["retention", "lifecycle", "compliance", "gdpr", "data-deletion"], "referenceLinks": [{"label": "Azure Data Factory Scheduling and Triggers", "url": "https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers"}, {"label": "GDPR Data Retention and Right to Erasure", "url": "https://learn.microsoft.com/en-us/compliance/regulatory/gdpr-data-subject-rights"}, {"label": "Delta Lake VACUUM and DELETE Operations", "url": "https://learn.microsoft.com/en-us/fabric/onelake/delta-lake-overview"}], "estimatedImplementationEffort": "5-7 weeks", "costImplications": "Data Factory pipeline runs: $0.50 per execution. Fabric compute for Notebook purge jobs: standard rates. Storage savings from deletion: 10-30% reduction in employee data volume."}, {"id": "anonymization-k-anonymity", "name": "Statistical Anonymization for HR Analytics", "domain": "Data Governance and Security", "domainId": 3, "category": "Privacy Engineering", "summary": "Implements k-anonymity and differential privacy techniques to ensure published HR analytics cannot identify individuals through aggregation attacks or frequency analysis.", "description": "K-anonymity ensures that any combination of quasi-identifiers (age, department, salary band) appears in at least k records, preventing record linkage attacks. Differential privacy adds carefully calibrated noise to query results, enabling accurate aggregate statistics while preventing inference of individual values. For HR analytics, aggregation-only views enforce k-anonymity by requiring HAVING COUNT(*) >= 5 clauses on all queries, preventing disclosure of rare populations (e.g., the sole executive in a location). Noise injection adds Laplace or Gaussian noise proportional to sensitivity, with epsilon (privacy budget) controlling noise magnitude. Views in Fabric Warehouse or SQL Analytics Endpoint implement these controls. PySpark notebooks implement noise injection for ad-hoc analyses. Practical applications include publishing salary bands by role (not individuals), showing headcount by department (minimum 5 per group), and publishing engagement scores by location-function groups.", "fabricComponents": ["Fabric Warehouse", "SQL Analytics Endpoint", "PySpark Notebooks", "Lakehouse"], "pros": ["Provides formal mathematical guarantees against re-identification attacks, meeting GDPR and other privacy regulations' proportionality requirements.", "Enables publishing aggregate HR analytics to broad audiences (all employees, board, public) without risk of individual inference.", "Can be implemented as database views and functions, integrating seamlessly into existing analytics without changing consuming applications."], "cons": ["K-anonymity vulnerable to semantic attacks if quasi-identifiers are not carefully chosen; requires domain expertise to identify all identifying combinations.", "Noise injection reduces statistical utility; high privacy budgets (epsilon) may allow inference while low budgets (epsilon <0.1) add significant noise.", "Maintaining k-anonymity becomes harder as dataset grows and rare populations appear; may force conservative suppression of legitimate insights."], "usageInstructions": "1. Identify quasi-identifiers in HR data (age, salary band, department, location, tenure, role). 2. Create aggregation-only views with HAVING COUNT(*) >= 5. 3. Test for k=5 anonymity: SELECT department, role, COUNT(*) FROM employees GROUP BY department, role HAVING COUNT(*) >= 5. 4. For salary analytics, implement noise injection in Python: create base aggregate (SELECT department, avg(salary) as avg_sal FROM employees GROUP BY department), calculate sensitivity (max salary difference), generate Laplace noise with epsilon=0.5, add noise to aggregates. 5. Create security principal with view-only permissions to anonymized views. 6. Publish anonymized views via Power BI semantic model with field-level security. 7. Document epsilon budgets and k values for each published analysis.", "governanceConsiderations": "Establish privacy analytics working group with data scientists, privacy officers, and HR stakeholders. Document quasi-identifier sets and privacy threat model per analysis. Set epsilon budgets based on acceptable privacy-utility tradeoff. Review all published aggregates for k-anonymity before release. Track total epsilon consumption across all published analyses to prevent privacy budget exhaustion. Implement technical controls preventing ad-hoc SQL query access to raw employee data.", "peopleAnalyticsUseCases": ["Publishing company-wide salary equity analysis to all employees showing average salary by department-level-location, with k=10 minimum group size, enabling transparency without revealing individual salaries.", "Generating anonymized performance distribution histograms for board review with noise injection ensuring no distribution shape discloses individual outliers.", "Creating recruitment funnel analytics published externally showing aggregate conversion rates by source-role with k=5 minimums, preventing inference of individual candidate outcomes."], "complexity": "High", "maturity": "Emerging", "compatibleWith": ["medallion-architecture", "row-level-security", "dynamic-data-masking", "purview-data-map"], "incompatibleWith": [], "prerequisites": ["medallion-architecture", "row-level-security"], "tags": ["anonymization", "k-anonymity", "differential-privacy", "privacy", "compliance"], "referenceLinks": [{"label": "K-Anonymity and Privacy Techniques", "url": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/privacy-techniques"}, {"label": "Differential Privacy in Azure", "url": "https://learn.microsoft.com/en-us/purview/compliance-privacy-basics"}, {"label": "GDPR Data Minimization Principles", "url": "https://learn.microsoft.com/en-us/compliance/regulatory/gdpr-data-minimization"}], "estimatedImplementationEffort": "6-8 weeks", "costImplications": "No licensing cost; implemented in Fabric Warehouse at standard query rates. Noise injection PySpark notebooks at standard compute rates."}, {"id": "audit-siem-integration", "name": "Audit Log Export and SIEM Integration", "domain": "Data Governance and Security", "domainId": 3, "category": "Monitoring", "summary": "Forwards Fabric activity logs and Power BI audit logs to Azure Sentinel for centralized SOC monitoring, anomaly detection, and forensic investigation.", "description": "Audit log integration ensures all Fabric and Power BI activities (data access, semantic model changes, sharing, export, deletion) are captured, centralized, and analyzed for security incidents. Fabric activity logs (workspace creation, item access, refresh execution, query patterns) flow to Log Analytics Workspace. Power BI activity logs (report view, dataset refresh, sharing changes, export) are forwarded to the Unified Audit Log via the Office 365 Management API. Azure Sentinel consumes these logs, applies correlation rules to detect anomalies (bulk data export, after-hours access, privilege escalation), enriches with identity and threat intelligence, and triggers incidents for SOC investigation. Custom KQL (Kusto Query Language) queries detect HR-specific threats: unusual bulk report exports, access to sensitive employee data by non-HR users, policy violation patterns. Retention ensures logs are available for 1-2 years for forensic analysis and compliance audits.", "fabricComponents": ["Azure Sentinel", "Log Analytics Workspace", "Power BI Activity Log", "Unified Audit Log", "Azure Monitor"], "pros": ["Centralizes audit logs from Fabric and Power BI in single searchable repository enabling cross-system correlation and forensic investigations.", "Detects insider threats and compliance violations in real-time through rule-based alerting and anomaly detection, enabling rapid response.", "Provides evidence trail for regulatory audits and incident response, meeting SOX, HIPAA, and other audit requirements."], "cons": ["Azure Sentinel ingestion costs scale with log volume; busy Fabric environments can generate 1000+ GB/month of logs, increasing licensing costs.", "Lag between activity and Sentinel processing (typically 1-5 minutes) means real-time detection is limited; historical detection takes longer.", "False positive rates in correlation rules can overwhelm SOC; requires significant tuning and stakeholder coordination."], "usageInstructions": "1. Create Log Analytics Workspace in Azure. 2. Enable Fabric activity logging: Workspace Settings > Audit and Compliance > Enable Activity Logging. 3. Create Data Connector in Log Analytics: Fabric Activity > Diagnostic Settings > Send to Log Analytics Workspace. 4. Enable Power BI audit logging: Power BI Admin Portal > Audit and Compliance > Turn on audit log search. 5. Configure Office 365 Management API in Sentinel to consume Unified Audit Log. 6. In Azure Sentinel, create Data Connectors for Office 365 and Log Analytics. 7. Create KQL detection rules: query for bulk exports (>1000 rows), access to SSN columns, after-hours queries. 8. Configure Incidents from detections with SOAR playbook triggers. 9. Create Power BI dashboard for audit KPIs: daily active users, top data consumers, failed access attempts.", "governanceConsiderations": "Establish Security Operations Center (SOC) workflow for Sentinel incidents related to HR data. Define escalation procedures for confirmed security incidents (e.g., data exfiltration). Implement log retention policies: 6 months hot storage, 2 years cold archive. Conduct quarterly reviews of detection rules to reduce false positives. Require formal change management approval for queries accessing sensitive audit data. Ensure audit log access is restricted to security team and select compliance officers.", "peopleAnalyticsUseCases": ["Real-time detection of bulk export of executive compensation reports (>100 rows) to external email domains, triggering SOC investigation into potential salary data exfiltration.", "Anomaly detection identifying that a non-HR user (e.g., IT service account) is accessing employee SSN and salary columns during off-hours, indicating potential credential compromise.", "Forensic investigation of a departing manager's last-day activities: queried employee performance data, shared compensation history report with personal email, then deleted workspace link."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["row-level-security", "workspace-permission-governance", "dlp-policy-enforcement", "privileged-access-management"], "incompatibleWith": [], "prerequisites": [], "tags": ["audit", "siem", "sentinel", "monitoring", "security"], "referenceLinks": [{"label": "Fabric Activity Logging and Monitoring", "url": "https://learn.microsoft.com/en-us/fabric/admin/monitoring-workspace"}, {"label": "Azure Sentinel Overview", "url": "https://learn.microsoft.com/en-us/azure/sentinel/overview"}, {"label": "Power BI Audit Logging", "url": "https://learn.microsoft.com/en-us/power-bi/admin/service-admin-auditing"}], "estimatedImplementationEffort": "3-4 weeks", "costImplications": "Log Analytics Workspace: $0.70/GB ingested. Azure Sentinel: $100+ per GB for first 100GB/day. Detection rule tuning requires SOC analyst time."}, {"id": "dsr-fulfillment", "name": "Data Subject Request Fulfillment Pipeline", "domain": "Data Governance and Security", "domainId": 3, "category": "Privacy Compliance", "summary": "Automates GDPR/PIPEDA data subject rights (access and erasure) with workflows using Purview data lineage, soft-delete mechanisms, and verification processes.", "description": "Data Subject Requests (DSR) are formal legal requests from individuals to access (GDPR Article 15) or erase (GDPR Article 17, right-to-be-forgotten) their personal data. Manual DSR handling is error-prone, slow, and costly. Automated DSR fulfillment pipelines use Microsoft Purview Data Map to identify all systems containing an individual's data, orchestrate retrieval or deletion via Data Factory, perform soft-delete in Fabric Lakehouse with audit trail, notify requestor, and verify completion. For HR analytics, DSR requests require identifying employee records across Lakehouse, data warehouse, Power BI datasets, and archived systems. Soft-delete (marking records inactive) enables 30-day appeal period before hard-delete. Lineage mapping shows which analytics and reports are impacted by erasure. Workflow includes validation steps preventing accidental bulk deletion.", "fabricComponents": ["Microsoft Purview Data Map", "Data Factory Pipeline", "OneLake", "Fabric Notebooks", "Lakehouse"], "pros": ["Automates time-consuming manual search and deletion, reducing DSR fulfillment time from weeks to days and reducing legal/compliance costs.", "Uses Purview lineage to ensure complete erasure across all systems; manual processes frequently miss copies or derived data.", "Provides audit trail proving compliance with GDPR/PIPEDA deadlines (30 days for access, 30 days for erasure), supporting regulatory defense."], "cons": ["Incomplete or inaccurate Purview Data Map can result in missed data systems and non-compliance; requires continuous curation.", "Soft-delete relies on application logic filtering deleted records; risk of query bugs exposing deleted data if filters are incorrect.", "Determining which analytic derivatives (e.g., aggregates, models, exports) must be updated/deleted is complex and data-dependent."], "usageInstructions": "1. Create DSR request intake form (SharePoint form or Power Apps) capturing: requestor name, email, request type (access/erasure), date. 2. Create Data Factory pipeline: Step 1 - Query Purview Data Map API for assets containing individual's data (e.g., Employee.EmployeeId = 12345). Step 2 - Generate list of impacted systems/tables. Step 3 - For access requests, export data to encrypted file in secure SharePoint. For erasure requests, mark records with is_dsr_deleted=1, log deletion timestamp/reason. Step 4 - Wait 30 days for appeals. Step 5 - Hard-delete (VACUUM, UPDATE DELETE). Step 6 - Notify requestor and compliance team. 7. Implement controls: require approvals for bulk erasure, audit all DSR operations, verify deletion completeness.", "governanceConsiderations": "Establish DSR response team with legal, HR, privacy, and data teams. Document DSR procedures and compliance timelines per jurisdiction. Implement technical access controls preventing unauthorized DSR request manipulation. Maintain DSR request audit log with decision and verification steps. Conduct quarterly audits to verify soft-deleted records are truly inaccessible to analytics. Maintain appeal process for 30-day grace period post-deletion request.", "peopleAnalyticsUseCases": ["Automated fulfillment of GDPR access request from Canadian employee: Purview identifies data in Lakehouse employee table, warehouse HRIS system, Power BI reports. Pipeline exports to encrypted PDF in 5 days, within GDPR 30-day SLA.", "Right-to-erasure request from terminated EU employee: soft-delete removes from all active tables, historical audit trails remain but employee salary/SSN are masked from all analytics, 30-day appeal period prevents accidental permanent deletion.", "Cross-border DSR for US employee working in EU office: pipeline identifies data in US Lake House capacity (compliant with GDPR data residency because US subject to standard contractual clauses), executes soft-delete with audit trail."], "complexity": "High", "maturity": "Emerging", "compatibleWith": ["purview-data-map", "medallion-architecture", "audit-siem-integration"], "incompatibleWith": [], "prerequisites": ["purview-data-map", "medallion-architecture"], "tags": ["gdpr", "pipeda", "dsr", "privacy", "right-to-erasure"], "referenceLinks": [{"label": "GDPR Data Subject Rights", "url": "https://learn.microsoft.com/en-us/compliance/regulatory/gdpr-data-subject-rights"}, {"label": "Microsoft Purview Data Map", "url": "https://learn.microsoft.com/en-us/purview/purview-data-catalog"}, {"label": "Data Subject Request Management", "url": "https://learn.microsoft.com/en-us/compliance/regulatory/manage-gdpr-data-subject-requests-summary"}], "estimatedImplementationEffort": "6-8 weeks", "costImplications": "Purview Data Map: included in premium licensing. Data Factory DSR pipelines: $0.50 per execution. Estimated 100-200 DSR requests/year at $50 cost to process each."}, {"id": "disaster-recovery-geo", "name": "Disaster Recovery and Geo-Redundancy", "domain": "Data Sharing and Distribution", "domainId": 8, "category": "Business Continuity", "summary": "Configures OneLake with zone-redundant storage (ZRS) and geo-replication to paired Azure region, with manual failover procedures and multi-geo capacity for data residency.", "description": "Disaster recovery ensures business continuity when regional outages occur. OneLake Zone-Redundant Storage (ZRS) replicates data across three availability zones within a region, protecting against zone-level failures. Geo-replication asynchronously copies data to a paired region (e.g., US East 2 to US Central 1), protecting against regional disasters. Fabric Capacity can be provisioned in multiple regions to enable manual failover. For HR analytics, disaster recovery ensures employee data is always available and compliant with data residency laws (e.g., Canadian data stays in Canada region). Multi-geo capacity configuration enables capacity to be deployed in specific regions. Backup procedures include daily snapshots exported to secure storage. Manual failover involves updating connection strings and repointing workspaces to failover capacity in alternate region. Recovery Time Objective (RTO) for manual failover is 4-8 hours; automated failover via service mesh future capability targets RTO <15 minutes.", "fabricComponents": ["OneLake", "Fabric Capacity", "Azure Region Pairs", "Zone-Redundant Storage"], "pros": ["ZRS provides automatic replication without application changes, protecting against zone and data center failures with no additional cost.", "Geo-replication enables regional compliance (e.g., Canadian data in Canada) and disaster recovery to another region, meeting business continuity SLAs.", "Manual failover procedures are tested and documented, enabling rapid recovery with minimal training."], "cons": ["Geo-replication introduces asynchronous delay (minutes to hours); recent data changes may not be replicated if failover occurs immediately after write.", "Manual failover requires human action and coordination, typically taking 4-8 hours; automated failover is not yet available.", "Multi-region Fabric Capacity costs increase significantly; maintaining hot-hot failover doubles capacity costs vs. cold standby."], "usageInstructions": "1. Enable ZRS on OneLake workspace: Workspace Settings > Storage Redundancy > Zone-Redundant Storage. 2. Configure geo-replication: Workspace Settings > Disaster Recovery > Enable Geo-Replication to paired region (e.g., East 2 to Central 1). 3. Create Data Factory pipeline for daily snapshot export: copy all Lakehouse tables to Azure Storage Account in alternate region. 4. Document failover runbook: 1) Declare disaster, 2) Notify stakeholders, 3) Verify failover region readiness, 4) Update Fabric workspace connection to failover capacity, 5) Repoint Power BI datasets to failover semantic models, 6) Validate data integrity in failover region. 5. Conduct quarterly disaster recovery drill: simulate regional failure, execute failover, validate analytics in alternate region, measure RTO. 6. Maintain failover capacity in standby mode or scale down when not in use.", "governanceConsiderations": "Establish disaster recovery committee with IT, business continuity, and HR stakeholders. Define RTO (Recovery Time Objective) and RPO (Recovery Point Objective) per service level agreement. Document failover procedures and roles. Test failover procedures quarterly with full team participation. Monitor geo-replication lag and alert if lag exceeds SLA. Maintain separate credentials for failover region. Ensure external audit of disaster recovery controls annually.", "peopleAnalyticsUseCases": ["Regional outage in US East region: automatic ZRS failover within same region (1-2 minutes). If both East zones unavailable, manual failover to US Central capacity (4 hours RTO) ensures payroll analytics continue.", "Compliance requirement for Canadian employee data to remain in Canada: Canadian Fabric Capacity with geo-replication to alternate Canadian region, ensuring data never leaves Canada even during failover.", "Financial services regulatory requirement for near-zero RPO: async geo-replication has 30-minute lag, so daily snapshot export to alternate region ensures RPO <24 hours."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["medallion-architecture", "encryption-at-rest-cmk", "audit-siem-integration"], "incompatibleWith": [], "prerequisites": [], "tags": ["disaster-recovery", "business-continuity", "geo-redundancy", "zrs", "failover"], "referenceLinks": [{"label": "OneLake Disaster Recovery and Geo-Redundancy", "url": "https://learn.microsoft.com/en-us/fabric/onelake/redundancy"}, {"label": "Azure Region Pairs for Business Continuity", "url": "https://learn.microsoft.com/en-us/azure/reliability/cross-region-replication-azure"}, {"label": "Fabric Capacity Provisioning and Failover", "url": "https://learn.microsoft.com/en-us/fabric/admin/capacity-admin"}], "estimatedImplementationEffort": "4-6 weeks", "costImplications": "ZRS: no additional cost. Geo-replication: $0.02/GB/month for replication. Failover capacity: $16/hour standby, $96/hour active (Premium P1)."}, {"id": "network-isolation-private-links", "name": "Network Isolation with Private Endpoints", "domain": "Data Governance and Security", "domainId": 3, "category": "Network Security", "summary": "Implements Azure Private Endpoints for Fabric Capacity and managed private endpoints for Spark/compute, ensuring traffic flows only over private VNet and preventing internet exposure.", "description": "Private Endpoints eliminate internet routes for Fabric services, instead routing all traffic through Azure VNet private network. Fabric Capacity (compute and storage) uses Private Endpoints to ensure no data traverses public internet. Managed Private Endpoints for Spark Compute enable secure communication with private data sources (SQL Database in VNet, storage accounts with firewall enabled). For HR analytics in regulated environments (financial services, healthcare), network isolation ensures employee data never transits public internet. Network security groups (NSGs) can further restrict traffic by source/destination IP. VNet integration enables hybrid connectivity: on-premises HR systems can securely connect to Fabric via ExpressRoute. Outbound traffic can be forced through proxy/firewall for inspection. Azure Private Link services provide 99.99% availability.", "fabricComponents": ["Azure Private Link", "Azure VNet", "Managed Private Endpoints", "Fabric Capacity"], "pros": ["Eliminates internet exposure of Fabric endpoints, significantly reducing attack surface and complying with network isolation requirements.", "Enables hybrid connectivity to on-premises systems without VPN, improving performance and security.", "Provides network-layer segmentation complementing identity and data-layer security controls, following defense-in-depth principle."], "cons": ["Private Endpoints add operational complexity: VNet design, routing, DNS configuration, and troubleshooting require network expertise.", "Cost increases: Fabric Capacity pricing unchanged but Private Endpoints charge $0.50/hour per endpoint, adding $360/month per capacity.", "Hybrid connectivity requires ExpressRoute or Site-to-Site VPN; on-premises connectivity adds complexity and latency."], "usageInstructions": "1. Create Azure VNet in the same region as Fabric Capacity. 2. In Fabric Workspace Settings, enable Private Endpoints: select Capacity > Network > Enable Private Endpoint. 3. Azure creates Private Link Service and provides Private Endpoint Connection. 4. In your VNet, create Private Endpoint resource: +Create > Private Endpoint, select Fabric service, select subnet, configure DNS integration. 5. Update DNS resolver to map fabric.microsoft.com to private IP (e.g., 10.0.0.5). 6. Test connectivity: connect VM in VNet, query nslookup fabric.microsoft.com (should resolve to private IP). 7. Configure Managed Private Endpoints for Spark: Workspace > Managed Private Endpoints > +New, select target resource (SQL Database, storage), approve in target resource. 8. Configure Network Security Group (NSG): allow only VNet subnets to reach Fabric Private Endpoint, deny internet-routed traffic.", "governanceConsiderations": "Establish network security architecture review with IT security and compliance teams. Document VNet design and Private Endpoint locations. Implement bastion hosts for secure VM access instead of public IPs. Enforce NSG rules preventing traffic to public internet. Monitor Private Endpoint connections and deny suspicious outbound attempts. Require change management approval for VNet/NSG changes. Conduct quarterly network segmentation audits.", "peopleAnalyticsUseCases": ["Financial services firm isolates HR analytics Fabric Capacity on private VNet, restricts outbound traffic to approved SIEM sink via proxy, ensuring no employee data transits internet.", "Healthcare provider connects on-premises HRIS system via ExpressRoute to Fabric over private endpoint, enabling analytics without routing PII through public internet or VPN.", "Multi-tenant SaaS provider isolates each customer's Fabric Capacity on separate VNet, using managed private endpoints to connect to customer-specific SQL databases on-premises."], "complexity": "High", "maturity": "GA", "compatibleWith": ["encryption-at-rest-cmk", "workspace-permission-governance", "audit-siem-integration"], "incompatibleWith": [], "prerequisites": [], "tags": ["network-security", "private-endpoints", "vnet", "isolation", "compliance"], "referenceLinks": [{"label": "Azure Private Endpoints for Fabric", "url": "https://learn.microsoft.com/en-us/fabric/security/security-private-endpoints-overview"}, {"label": "Azure VNet and NSG Configuration", "url": "https://learn.microsoft.com/en-us/azure/virtual-network/manage-network-security-group"}, {"label": "Managed Private Endpoints for Spark", "url": "https://learn.microsoft.com/en-us/fabric/security/security-managed-private-endpoints-overview"}], "estimatedImplementationEffort": "4-6 weeks", "costImplications": "Private Endpoint: $0.50/hour ($360/month per endpoint). VNet/NSG: no cost. ExpressRoute (if used): $0.30-0.50/hour depending on bandwidth."}, {"id": "privileged-access-management", "name": "Just-in-Time Privileged Access Management", "domain": "Data Governance and Security", "domainId": 3, "category": "Identity Security", "summary": "Implements Azure Entra ID Privileged Identity Management (PIM) for just-in-time elevation of workspace admin roles, with approval workflows and MFA enforcement.", "description": "Privileged Identity Management (PIM) reduces standing permissions, requiring users to request temporary elevation to admin roles. For HR analytics, workspace admins manage data governance policies, sharing, and sensitive configurations. Rather than making users permanent admins (standing privilege), PIM makes them 'Eligible Admins' who must request activation. Requests require approval from compliance officer and MFA authentication. Activation is time-limited (1-8 hours) and audited. Break-glass emergency accounts provide fallback access if primary admins are unavailable. Multi-factor authentication (MFA) is required for all privileged actions. Audit logs in Entra ID capture all activation requests, approvals, and actions performed during elevated access. For compliance, PIM demonstrates least-privilege principle and immediate detection of unauthorized privilege escalation attempts.", "fabricComponents": ["Azure Entra ID PIM", "Fabric Workspace Roles", "Azure Key Vault", "Azure Sentinel"], "pros": ["Reduces standing privilege, dramatically lowering risk of compromise: admin account hack affects only active activation window (1-8 hours) not 365 days.", "Requires approval for every elevation, creating human checkpoint preventing unauthorized access.", "Complete audit trail of privilege escalation enables detection of unusual elevation patterns and supports compliance audits."], "cons": ["PIM adds friction to emergency access scenarios, requiring approval workflow adds 15-30 minutes to incident response.", "Admin users must have Entra ID Premium P2 license, adding cost ~$30-50/user/month.", "Misconfigured break-glass accounts (credentials leaked, not rotated) undermine PIM's security benefits."], "usageInstructions": "1. Ensure Entra ID Premium P2 licensed. 2. In Entra ID > Privileged Identity Management > Fabric Workspace Roles, select 'Workspace Admin' role. 3. Set eligible users: add workspace managers as Eligible (not Permanent) members. 4. Configure activation: require approval, require MFA, set max activation duration to 4 hours. 5. Select approval delegator: compliance officer or security team. 6. Configure notifications: alert on elevations, email approvers. 7. Create break-glass account: dedicate account for emergency access, store credentials in physical safe (not digital). 8. Test activation: Eligible Admin requests activation, approver receives email, activates with MFA. 9. Monitor in Azure Sentinel: create alert for approval denials, unusual activation times, after-hours elevations. 10. Quarterly review: audit PIM logs, revoke unused eligible access.", "governanceConsiderations": "Establish PIM governance committee with security, HR, and compliance stakeholders. Document approval process and SLA for activation requests (should be <15 minutes). Implement technical controls enforcing MFA (hardware keys preferred). Rotate break-glass credentials quarterly with multiple stakeholders witnessing. Test break-glass emergency access annually. Review PIM logs monthly for suspicious patterns. Enforce organization-wide policy: no permanent privileged roles, all admins must use PIM.", "peopleAnalyticsUseCases": ["HR analytics workspace admin (normally ineligible) requests elevation to investigate unauthorized data export, approver grants 4-hour activation via MFA approval, admin investigates access logs with limited time window.", "Data governance officer regularly activates to review sensitivity label assignments and RLS policies, must approve each activation with business justification.", "Emergency incident: production Fabric capacity is down, break-glass account holder activates with MFA, gains temporary admin access to restart services, audited and revoked within 1 hour."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["workspace-permission-governance", "audit-siem-integration", "encryption-at-rest-cmk"], "incompatibleWith": [], "prerequisites": [], "tags": ["pim", "privileged-access", "identity", "mfa", "compliance"], "referenceLinks": [{"label": "Azure Entra ID Privileged Identity Management", "url": "https://learn.microsoft.com/en-us/entra/id-governance/privileged-identity-management/pim-configure"}, {"label": "PIM for Fabric Workspace Roles", "url": "https://learn.microsoft.com/en-us/fabric/admin/pim-setup"}, {"label": "Multi-Factor Authentication Best Practices", "url": "https://learn.microsoft.com/en-us/entra/identity/authentication/concept-mfa-licensing"}], "estimatedImplementationEffort": "2-3 weeks", "costImplications": "Entra ID Premium P2: $30-50/user/month (per admin). PIM: included in Premium P2. Hardware MFA keys: $20-50 per unit."}, {"id": "change-management-four-eyes", "name": "Dual-Approval Change Management Pipeline", "domain": "Data Transformation and Processing", "domainId": 2, "category": "Change Control", "summary": "Enforces dual-approval (business and governance) before production pipeline deployment, implementing four-eyes principle via Azure DevOps gates.", "description": "Four-eyes principle requires two independent approvals before changes to production systems. For HR analytics, changes to data pipelines (transformations, data quality rules, refresh schedules) can affect payroll, benefits, or compliance analytics. Requiring dual approval ensures both business correctness (HR manager) and governance compliance (data governance officer) are verified. Azure DevOps Deployment Pipelines implement gates between environments: development -> staging -> production. Staging gate requires business approval from HR analytics owner confirming transformations are correct. Production gate requires data governance approval confirming RLS policies, data lineage, and compliance are maintained. Pull request reviews enforce code quality and documentation before merge to main. Git branching strategy separates features, requiring peer review. Change log automatically documents approvers, timestamp, and change summary. Rejection of changes includes audit trail for compliance.", "fabricComponents": ["Azure DevOps", "Data Factory Pipeline", "Fabric Deployment Pipelines", "Git Integration"], "pros": ["Enforces dual approval, preventing single-person errors and unauthorized changes to critical analytics.", "Creates audit trail proving compliance with change control requirements, supporting SOX, HIPAA audits.", "Improves quality by requiring peer review before production deployment."], "cons": ["Adds cycle time: waiting for two approvers can delay urgent fixes (typical cycle time 24-48 hours).", "Requires both approvers to be available; absence of approver blocks deployment.", "False sense of security if approvers don't actually review changes carefully."], "usageInstructions": "1. Set up Azure DevOps project with Git repo for Fabric pipeline definitions. 2. Configure branch policy on main: require pull request reviews, minimum 2 approvers (business + governance), status checks passing. 3. Create staging deployment pipeline: trigger on PR approval, deploy to staging environment. 4. Add pre-deployment gate before staging: auto-approve (runs tests, validates syntax). 5. Add pre-deployment gate before production: manual approval, allow only specific users (data governance team). Require justification/description. 6. Create business approval gate: HR manager reviews transformations, confirms correctness. 7. Track approvals: Azure DevOps automatically logs timestamp, approver identity, comments. 8. Reject approvals include mandatory reason. 9. Create dashboard: count deployments, approval time metrics, rejection rates. 10. Quarterly review: analyze approval bottlenecks, optimize process.", "governanceConsiderations": "Define who can request, approve, and reject changes: business sponsor (business approval), data governance (compliance approval). Document approval criteria: business approval verifies transformations match requirements, governance approval verifies RLS, lineage, data quality, compliance. Implement escalation path for urgent changes (e.g., 24-hour SLA for critical bug fixes). Audit approval logs monthly. Require documented change rationale in pull request. Disallow approval from same person who submitted change (dual-approval enforced technically).", "peopleAnalyticsUseCases": ["Data engineer submits PR changing employee salary aggregation logic (e.g., fixing bonus calculation). Business sponsor (payroll manager) approves confirming calculation is correct. Data governance approves confirming aggregation preserves privacy (k>=5). Production deployment occurs only after both approvals.", "New HR dataset onboarded to Lakehouse: ingestion pipeline PR submitted. Business sponsor approves confirming data matches HRIS system. Data governance approves confirming sensitivity labels assigned, lineage documented. Both approvals required before prod activation.", "Urgent hotfix: salary dashboard showing incorrect totals. Change submitted with 'urgent' flag. Both approvers pinged, typically respond within 2-4 hours. Production deployment after both approvals."], "complexity": "Medium", "maturity": "Emerging", "compatibleWith": ["deployment-pipelines", "audit-siem-integration"], "incompatibleWith": [], "prerequisites": [], "tags": ["change-management", "devops", "approval", "governance", "four-eyes"], "referenceLinks": [{"label": "Azure DevOps Release Gates and Approvals", "url": "https://learn.microsoft.com/en-us/azure/devops/pipelines/release/approvals/approvals"}, {"label": "Fabric Deployment Pipelines", "url": "https://learn.microsoft.com/en-us/fabric/cicd/deployment-pipelines/intro"}, {"label": "Git Branching Strategy and Pull Request Reviews", "url": "https://learn.microsoft.com/en-us/azure/devops/repos/git/branch-policies"}], "estimatedImplementationEffort": "3-4 weeks", "costImplications": "Azure DevOps: free for up to 5 users/project. Git: no cost. Slack/Teams notifications: included. Process overhead: ~1-2 hours per change for approvals."}, {"id": "cross-border-data-residency", "name": "Cross-Border Data Residency Isolation", "domain": "Data Sharing and Distribution", "domainId": 8, "category": "Data Sovereignty", "summary": "Isolates employee data by geography using multi-geo Fabric capacities, ensuring Canadian employee data remains in Canada and US data in US, with aggregate-only cross-border reporting.", "description": "Cross-border data residency ensures that employee personal data is stored and processed only within specified geographies per regulatory requirements. For multinational HR analytics, Canadian employees' data (salary, SSN, benefits) must remain in Canada due to PIPEDA. US data must remain in US due to state regulations and employer obligations. Fabric multi-geo capacity configuration assigns workspaces to specific regions; OneLake data is geo-pinned to that region. Cross-border reporting uses aggregate-only views (k-anonymity approach) ensuring individual records cannot be queried across borders. Data sharing uses Shortcuts with filtering, preventing cross-border record-level export. Periodic reviews ensure no data has migrated across borders. Audit logs track cross-border access attempts.", "fabricComponents": ["Fabric Capacity", "OneLake", "Workspace Assignment", "Multi-Geo Configuration"], "pros": ["Meets regulatory data residency requirements (PIPEDA, GDPR, local laws) preventing costly compliance violations.", "Enables multi-country operations with confidence that data stays in authorized regions.", "Provides operational resilience: country-level outage affects only that region's operations, not global."], "cons": ["Multi-region capacities increase costs ~2-3x vs. single-region (separate capacity per region).", "Analytics across regions requires federated queries or cross-border aggregates; complex joins are impossible.", "Data migration for employee moves (e.g., employee relocates from Canada to US) requires careful handling: old records deletion or transfer."], "usageInstructions": "1. Create separate Fabric Capacity per geography: Canadian Capacity (Canada Central region), US Capacity (US East 2 region). 2. Create separate workspaces per geography: 'HR-Analytics-CA' in Canadian capacity, 'HR-Analytics-US' in US capacity. 3. Ingest employee data to geo-pinned Lakehouse: Canadian employee table in Canada workspace, US employee table in US workspace. 4. For cross-border reporting, create aggregate-only views: SELECT department, YEAR(dob) as year_of_birth, COUNT(*) as employee_count FROM employees WHERE country='CA' GROUP BY department, YEAR(dob) HAVING COUNT(*) >= 5. 5. Create federated semantic models: Power BI connects to Canadian and US aggregate views, combines aggregates (no record-level data). 6. Prevent cross-border shortcuts: Workspace Sharing > restrict shortcuts to same-region workspaces. 7. Audit cross-border access: Log Analytics tracks queries across regions, alert on suspicious activity. 8. Data migration procedure: terminating employee, update country field, archive to historical table in original region, do not migrate raw records.", "governanceConsiderations": "Establish data residency governance committee with legal, compliance, and HR stakeholders per country. Document residency requirements by jurisdiction (PIPEDA, GDPR, state laws). Implement technical enforcement: prevent shortcuts crossing regions, audit logs for attempted cross-border access. Quarterly audit: verify no employee data exists in wrong region. Data transfer agreements: document how employee relocations are handled. Secure deletion: ensure migrated data is permanently deleted from source region.", "peopleAnalyticsUseCases": ["Canadian bank with Canadian HQ + US subsidiary: employees are separated by capacity/workspace. Canadian headquarters views all Canadian employee analytics in Canadian workspace (compliant with PIPEDA). US subsidiary views US employee analytics in US workspace. Joint reporting uses aggregates: total Canadian headcount + total US headcount, no cross-border record-level joins.", "Multinational tech company with employees in Canada, US, and EU: separate capacities for each region. Annual global report aggregates at country-group level: 'Canada 5000 employees, US 10000 employees, EU 3000 employees' without exposing individual records.", "Employee relocation: Canadian employee transfers to US. Old record in Canadian Lakehouse is soft-deleted (marked inactive), no migration to US workspace. US onboarding creates new record in US workspace for same employee."], "complexity": "High", "maturity": "GA", "compatibleWith": ["medallion-architecture", "encryption-at-rest-cmk", "disaster-recovery-geo"], "incompatibleWith": [], "prerequisites": [], "tags": ["data-residency", "compliance", "gdpr", "pipeda", "sovereignty"], "referenceLinks": [{"label": "Microsoft Fabric Multi-Geo Capabilities", "url": "https://learn.microsoft.com/en-us/fabric/admin/capacity-multi-geo"}, {"label": "PIPEDA Data Residency Requirements", "url": "https://learn.microsoft.com/en-us/compliance/regulatory/offering-pipeda"}, {"label": "GDPR Data Residency and Transfers", "url": "https://learn.microsoft.com/en-us/compliance/regulatory/gdpr-data-location"}], "estimatedImplementationEffort": "6-8 weeks", "costImplications": "Multi-region Fabric Capacity: 2-3x capacity cost (separate capacity per region). Premium P1 capacity: $16/hour per region. Cross-border OneLake replication: $0.02/GB/month."}, {"id": "encryption-at-rest-cmk", "name": "Encryption at Rest with Customer-Managed Keys", "domain": "Data Governance and Security", "domainId": 3, "category": "Encryption", "summary": "Implements encryption at rest using customer-managed keys (CMK) stored in Azure Key Vault for OneLake workspace data. Ensures TLS 1.2+ for all data in transit and maintains FIPS 140-2 compliance for sensitive HR data.", "description": "Customer-Managed Key (CMK) encryption provides organizational control over encryption key lifecycle and rotation. In financial services HR analytics, CMK encryption via Azure Key Vault ensures that OneLake workspace data is encrypted with keys managed by your organization, not Microsoft. All data in transit uses TLS 1.2 or higher. This approach meets regulatory requirements for key custody, enables key rotation policies, supports audit logging for key access, and provides compliance with FIPS 140-2 standards for handling highly sensitive employee financial and personal data. Integration with Entra ID service principals allows role-based access control over decryption operations.", "fabricComponents": ["Azure Key Vault", "OneLake", "Fabric Workspace Settings", "Entra ID Service Principal"], "pros": ["Provides organizational control over encryption keys with full audit trail of key access and rotations.", "Enables compliance with regulations requiring customer-managed encryption (SOX, GDPR, PIPEDA, HIPAA).", "Supports break-glass emergency key access protocols and disaster recovery scenarios."], "cons": ["Increases operational complexity requiring key rotation and lifecycle management procedures.", "Key Vault service calls add latency to Fabric operations, typically <10ms but noticeable at scale.", "Requires careful IAM design to prevent accidental key lockout that could make data unrecoverable."], "usageInstructions": "1. Create an Azure Key Vault resource in the same region as Fabric capacity. 2. Generate RSA 3072-bit or 4096-bit CMK in Key Vault. 3. Grant Fabric capacity system-assigned managed identity Key Wrap/Unwrap permissions on the key. 4. In Fabric Workspace Settings, select 'Customer Managed Key' and specify the Key Vault URI. 5. Configure key rotation policy (annual minimum). 6. Enable Key Vault audit logging and monitor access. 7. Test disaster recovery failover procedures quarterly.", "governanceConsiderations": "Establish a key management committee with business and security stakeholders. Document key rotation procedures and emergency access protocols. Implement access reviews quarterly for Key Vault permissions. Ensure backup keys are stored in a geographically separate region. Monitor failed decryption attempts and investigate anomalies. Integrate key rotation events into change management processes.", "peopleAnalyticsUseCases": ["Salary survey analysis where payroll data is encrypted with organization-controlled CMK, enabling internal audits while maintaining key custody.", "Executive compensation dashboards requiring SOX compliance where CMK encryption demonstrates regulatory control over sensitive executive personal data.", "International employee equity tracking where Canadian employee data uses Canadian Key Vault instance and US data uses US Key Vault for data residency compliance."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["sensitivity-labels", "purview-data-map", "dynamic-data-masking", "row-level-security"], "incompatibleWith": [], "prerequisites": [], "tags": ["encryption", "security", "compliance", "key-management", "financial-services"], "referenceLinks": [{"label": "Encryption at Rest in Microsoft Fabric", "url": "https://learn.microsoft.com/en-us/fabric/enterprise/encryption-at-rest"}, {"label": "Azure Key Vault Overview", "url": "https://learn.microsoft.com/en-us/azure/key-vault/general/overview"}, {"label": "FIPS 140-2 Compliance in Azure", "url": "https://learn.microsoft.com/en-us/compliance/regulatory/offering-fips-140-2"}], "estimatedImplementationEffort": "2-3 weeks", "costImplications": "Azure Key Vault: $0.34/10k operations; CMK key storage $1/month. Key Vault calls add minimal overhead."}, {"id": "dlp-policy-enforcement", "name": "Data Loss Prevention Policy Enforcement", "domain": "Data Governance and Security", "domainId": 3, "category": "Data Protection", "summary": "Configures Microsoft Purview Data Loss Prevention (DLP) policies to detect and block unauthorized export or sharing of sensitive information types including SSN, salary data, and bank account numbers.", "description": "Data Loss Prevention (DLP) policies in Microsoft Purview automatically detect sensitive information types within Fabric and Power BI, then enforce preventive or detective controls. For HR analytics, DLP policies identify patterns matching US Social Security Numbers, salary ranges, employee IDs, and bank account numbers. When detected, policies can block exports to CSV/Excel, prevent uploads to personal OneDrive, restrict sharing to external domains, require approval for sensitive queries, or log incidents to Azure Sentinel for SOC review. DLP works alongside sensitivity labels to provide multi-layered protection. Policies can be scoped to specific workspaces, datasets, or semantic models. Integration with Sensitivity Labels and Power BI Workspace settings ensures consistent enforcement across analytics surfaces.", "fabricComponents": ["Microsoft Purview DLP", "Sensitivity Labels", "Power BI", "OneLake"], "pros": ["Detects sensitive data automatically using built-in and custom regex patterns without manual classification.", "Blocks unauthorized export at the point of action (export, copy, print) reducing insider threat risk.", "Provides audit trail of blocked/allowed actions and integrates with SIEM for incident response workflows."], "cons": ["Can trigger false positives on benign data patterns, requiring regular tuning and user exceptions.", "DLP policies are primarily detective in Power BI; preventive enforcement requires specific workspace integration.", "Regex patterns for custom sensitive data types require security team expertise and ongoing maintenance."], "usageInstructions": "1. Open Microsoft Purview Compliance Center and navigate to Data Loss Prevention > Policies. 2. Create new policy with template 'Financial Info' or 'PII' as baseline. 3. Add custom sensitive info types: SSN regex (^\\\\d{3}-\\\\d{2}-\\\\d{4}$), salary range (\\\\$[0-9]{3,4}[KM]). 4. Configure rule actions: 'Restrict access' for Power BI, 'Require justification' for exports, 'Send alert to admin'. 5. Set policy scope to HR workspace and datasets. 6. Enable incident reporting to Azure Sentinel. 7. Test policy with sample data before production rollout. 8. Review blocked incidents monthly.", "governanceConsiderations": "Establish a DLP governance committee with HR, security, and legal teams. Document all custom sensitive data type patterns and business justification. Implement exception request workflow with business and compliance approval. Monitor false positive rate and adjust thresholds quarterly. Ensure DLP incidents are correlated with user access logs and audit trails. Train HR analytics team on compliant export procedures.", "peopleAnalyticsUseCases": ["Automated blocking of salary equity analysis exports containing aggregated ranges to unauthorized recipients, while allowing HR team to export to approved HR department network shares.", "Detection and alert when employee benefit election data containing SSN is copied to clipboard or exported to personal email, triggering incident response investigation.", "Prevention of executive compensation dashboard drill-through exports to external consultants without pre-approval workflow, enforcing approval for sensitive roles."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["sensitivity-labels", "row-level-security", "dynamic-data-masking", "audit-siem-integration"], "incompatibleWith": [], "prerequisites": ["sensitivity-labels"], "tags": ["data-loss-prevention", "dlp", "compliance", "data-protection", "security"], "referenceLinks": [{"label": "Data Loss Prevention in Microsoft Purview", "url": "https://learn.microsoft.com/en-us/purview/dlp-learn-about-dlp"}, {"label": "DLP Policy Creation and Management", "url": "https://learn.microsoft.com/en-us/purview/create-test-tune-dlp-policy"}, {"label": "Sensitive Information Types in Purview", "url": "https://learn.microsoft.com/en-us/purview/sensitive-information-type-learn-about"}], "estimatedImplementationEffort": "3-4 weeks", "costImplications": "Included in Purview premium license (part of Microsoft 365 E5 or standalone). DLP policy evaluation adds <1% query latency."}, {"id": "pii-tokenization", "name": "PII Tokenization and Pseudonymization", "domain": "Data Transformation and Processing", "domainId": 2, "category": "Privacy Engineering", "summary": "Replaces Personally Identifiable Information (PII) with deterministic tokens using Microsoft Presidio and PySpark, enabling data sharing and analytics without exposing actual personal data.", "description": "PII tokenization uses deterministic hashing to replace sensitive personal identifiers with tokens while maintaining the ability to link records. Microsoft Presidio, an AI-powered data protection service, automatically detects PII entities (names, addresses, phone numbers, emails) within notebooks and datasets. Identified PII is replaced with tokens, with the mapping stored securely in Azure Key Vault. For HR analytics, this enables sharing anonymized talent data with third-party vendors, consultants, and development teams without exposing actual employee identity. Deterministic tokenization (same PII always maps to same token) preserves join semantics. PySpark transformations within Fabric Notebooks perform tokenization at scale. Different token formats support different use cases: hash tokens for aggregate analytics, UUID tokens for record-level joins. Reverse lookup tokens require Key Vault authorization for decryption.", "fabricComponents": ["Fabric Notebooks", "PySpark", "Microsoft Presidio", "Azure Key Vault", "Lakehouse"], "pros": ["Enables secure data sharing with business partners and vendors by removing actual identity without losing join capability.", "Detects PII automatically using AI models rather than relying on manual regex patterns, catching complex PII patterns.", "Supports different tokenization strategies per use case: hash-only tokens for analytics, reversible tokens for authorized users with Key Vault access."], "cons": ["Deterministic tokenization vulnerable to frequency analysis if PII distribution is skewed (e.g., only 3 VP names).", "Presidio requires pre-trained models and tuning to avoid false positives/negatives in organization-specific contexts.", "Large-scale tokenization can impact notebook execution time; requires parallel PySpark processing for efficiency."], "usageInstructions": "1. Install Presidio in Fabric Notebook: pip install presidio-analyzer presidio-anonymizer. 2. Load employee data in PySpark DataFrame. 3. Configure Presidio analyzer with entity patterns: PERSON, PHONE_NUMBER, EMAIL_ADDRESS. 4. Create tokenization mapping: use Presidio to detect PII, generate deterministic SHA-256 hash token or UUID. 5. Store mapping in Key Vault secret with access restricted to governance team. 6. Replace PII with tokens using Presidio anonymizer. 7. Save tokenized dataset to Silver/Gold layer. 8. Log all tokenization operations with user, timestamp, entity count. 9. Test reverse lookup process with authorized user.", "governanceConsiderations": "Establish PII tokenization review board with privacy, security, and HR stakeholders. Document which entity types are tokenized and which remain for legitimate analytics. Implement access controls requiring Key Vault authorization for reverse lookups. Monitor tokenization logs for suspicious decryption patterns. Conduct quarterly audits of Presidio configuration to ensure no drift in entity detection. Maintain separate tokenization keys per data consumer or use case.", "peopleAnalyticsUseCases": ["Sharing anonymized employee experience survey data with external consulting firm to analyze cultural trends without exposing employee names, emails, or departments.", "Enabling development team to use production-like employee hierarchy and org structure data for testing without accessing real employee identities, using UUID tokens in place of names.", "Providing recruitment analytics to third-party recruitment platform showing skills and experience patterns without revealing actual employee identity, enabling vendor to match internal talent to opportunities."], "complexity": "High", "maturity": "Emerging", "compatibleWith": ["medallion-architecture", "spark-notebook-etl", "data-quality-validation", "dlp-policy-enforcement"], "incompatibleWith": [], "prerequisites": [], "tags": ["pii", "anonymization", "privacy", "data-sharing", "presidio"], "referenceLinks": [{"label": "Microsoft Presidio Overview", "url": "https://learn.microsoft.com/en-us/presidio/"}, {"label": "Presidio with Fabric Notebooks", "url": "https://github.com/microsoft/presidio"}, {"label": "Data Anonymization Techniques", "url": "https://learn.microsoft.com/en-us/compliance/regulatory/gdpr-data-subject-rights"}], "estimatedImplementationEffort": "4-6 weeks", "costImplications": "Presidio: open-source, no licensing cost. Key Vault storage: minimal. Spark compute: standard Fabric notebook rates."}, {"id": "data-retention-lifecycle", "name": "Automated Data Retention and Purge Pipeline", "domain": "Data Transformation and Processing", "domainId": 2, "category": "Lifecycle Management", "summary": "Implements scheduled automated pipelines that evaluate data age against retention rules, execute secure deletion with audit trails, and support legal hold overrides for compliance.", "description": "Data retention and purge pipelines ensure that employee data is not retained longer than required by law or business policy. For HR analytics, employee records must be retained per employment law (typically 3-7 years depending on jurisdiction), then securely deleted. Automated pipelines check record creation/modification dates against configurable retention policies, identify data eligible for purge, execute secure deletion via soft-delete flags or physical removal, and generate audit logs. Legal holds prevent deletion during litigation or regulatory investigations. Data Factory pipelines coordinate with Fabric notebooks to identify eligible records using SQL queries (SELECT * WHERE modified_date < DATEADD(year,-3,GETDATE())), mark for deletion, execute deletion operations, and log to audit tables. Soft-delete approach (marking records inactive) preserves referential integrity and enables recovery. Hard-delete (physical removal from Delta Lake) reduces storage and improves query performance but requires careful execution.", "fabricComponents": ["Data Factory Pipeline", "Fabric Notebooks", "OneLake", "Delta Lake", "Lakehouse"], "pros": ["Automates compliance with data retention regulations (GDPR right-to-erasure, employment law, SOX records retention).", "Reduces storage costs by removing obsolete data and improves query performance on smaller active datasets.", "Maintains complete audit trail of deleted records and reasons, supporting forensic investigations and compliance audits."], "cons": ["Incorrectly configured retention rules can cause accidental data loss; requires thorough testing and change management approval.", "Soft-delete approaches maintain referential integrity but require query filters to exclude deleted records; hard-delete is faster but riskier.", "Legal holds must be tracked separately; coordination between legal, HR, and data teams is required to manage hold status."], "usageInstructions": "1. Define retention policies in configuration table: entity type, retention period (years), jurisdiction, legal hold indicator. 2. Create Data Factory pipeline with Copy Activity to identify records eligible for purge using SQL: SELECT * WHERE datediff(year, modified_date, getdate()) >= retention_years AND legal_hold = 0. 3. Use Fabric Notebook to mark records as deleted (UPDATE table SET is_deleted = 1 WHERE id IN (...)) instead of hard delete initially. 4. Execute soft-delete daily/weekly per schedule. 5. After 30-day recovery period, run hard delete: VACUUM table_name RETAIN 0 HOURS; DELETE FROM table WHERE is_deleted = 1. 6. Log all operations: user, timestamp, record count, reasons. 7. Send audit report to compliance/legal team monthly.", "governanceConsiderations": "Establish data retention committee with legal, HR, compliance, and data teams. Document retention policies for each data entity type by jurisdiction. Implement approval workflow for legal hold changes. Set up audit alerts for large purge operations. Conduct quarterly audits of purge logs to verify correctness. Test disaster recovery to ensure deleted data cannot be recovered from backups without explicit authorization. Maintain retention policy version control.", "peopleAnalyticsUseCases": ["Automatically delete contingent worker records 2 years after separation date and contract end, enabling compliance with employment laws while reducing data footprint.", "Purge applicant data after hiring decision per FCRA requirements (typically 3 years), except for records under litigation hold from employment disputes.", "Soft-delete employee benefit elections from terminated employees after 7 years but preserve audit trail for pension calculations and benefit inquiries, with legal hold preventing deletion during claims."], "complexity": "High", "maturity": "Emerging", "compatibleWith": ["medallion-architecture", "spark-notebook-etl", "delta-lake-partitioning", "audit-siem-integration"], "incompatibleWith": [], "prerequisites": ["medallion-architecture"], "tags": ["retention", "lifecycle", "compliance", "gdpr", "data-deletion"], "referenceLinks": [{"label": "Azure Data Factory Scheduling and Triggers", "url": "https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers"}, {"label": "GDPR Data Retention and Right to Erasure", "url": "https://learn.microsoft.com/en-us/compliance/regulatory/gdpr-data-subject-rights"}, {"label": "Delta Lake VACUUM and DELETE Operations", "url": "https://learn.microsoft.com/en-us/fabric/onelake/delta-lake-overview"}], "estimatedImplementationEffort": "5-7 weeks", "costImplications": "Data Factory pipeline runs: $0.50 per execution. Fabric compute for Notebook purge jobs: standard rates. Storage savings from deletion: 10-30% reduction in employee data volume."}, {"id": "anonymization-k-anonymity", "name": "Statistical Anonymization for HR Analytics", "domain": "Data Governance and Security", "domainId": 3, "category": "Privacy Engineering", "summary": "Implements k-anonymity and differential privacy techniques to ensure published HR analytics cannot identify individuals through aggregation attacks or frequency analysis.", "description": "K-anonymity ensures that any combination of quasi-identifiers (age, department, salary band) appears in at least k records, preventing record linkage attacks. Differential privacy adds carefully calibrated noise to query results, enabling accurate aggregate statistics while preventing inference of individual values. For HR analytics, aggregation-only views enforce k-anonymity by requiring HAVING COUNT(*) >= 5 clauses on all queries, preventing disclosure of rare populations (e.g., the sole executive in a location). Noise injection adds Laplace or Gaussian noise proportional to sensitivity, with epsilon (privacy budget) controlling noise magnitude. Views in Fabric Warehouse or SQL Analytics Endpoint implement these controls. PySpark notebooks implement noise injection for ad-hoc analyses. Practical applications include publishing salary bands by role (not individuals), showing headcount by department (minimum 5 per group), and publishing engagement scores by location-function groups.", "fabricComponents": ["Fabric Warehouse", "SQL Analytics Endpoint", "PySpark Notebooks", "Lakehouse"], "pros": ["Provides formal mathematical guarantees against re-identification attacks, meeting GDPR and other privacy regulations' proportionality requirements.", "Enables publishing aggregate HR analytics to broad audiences (all employees, board, public) without risk of individual inference.", "Can be implemented as database views and functions, integrating seamlessly into existing analytics without changing consuming applications."], "cons": ["K-anonymity vulnerable to semantic attacks if quasi-identifiers are not carefully chosen; requires domain expertise to identify all identifying combinations.", "Noise injection reduces statistical utility; high privacy budgets (epsilon) may allow inference while low budgets (epsilon <0.1) add significant noise.", "Maintaining k-anonymity becomes harder as dataset grows and rare populations appear; may force conservative suppression of legitimate insights."], "usageInstructions": "1. Identify quasi-identifiers in HR data (age, salary band, department, location, tenure, role). 2. Create aggregation-only views with HAVING COUNT(*) >= 5. 3. Test for k=5 anonymity: SELECT department, role, COUNT(*) FROM employees GROUP BY department, role HAVING COUNT(*) >= 5. 4. For salary analytics, implement noise injection in Python: create base aggregate (SELECT department, avg(salary) as avg_sal FROM employees GROUP BY department), calculate sensitivity (max salary difference), generate Laplace noise with epsilon=0.5, add noise to aggregates. 5. Create security principal with view-only permissions to anonymized views. 6. Publish anonymized views via Power BI semantic model with field-level security. 7. Document epsilon budgets and k values for each published analysis.", "governanceConsiderations": "Establish privacy analytics working group with data scientists, privacy officers, and HR stakeholders. Document quasi-identifier sets and privacy threat model per analysis. Set epsilon budgets based on acceptable privacy-utility tradeoff. Review all published aggregates for k-anonymity before release. Track total epsilon consumption across all published analyses to prevent privacy budget exhaustion. Implement technical controls preventing ad-hoc SQL query access to raw employee data.", "peopleAnalyticsUseCases": ["Publishing company-wide salary equity analysis to all employees showing average salary by department-level-location, with k=10 minimum group size, enabling transparency without revealing individual salaries.", "Generating anonymized performance distribution histograms for board review with noise injection ensuring no distribution shape discloses individual outliers.", "Creating recruitment funnel analytics published externally showing aggregate conversion rates by source-role with k=5 minimums, preventing inference of individual candidate outcomes."], "complexity": "High", "maturity": "Emerging", "compatibleWith": ["medallion-architecture", "row-level-security", "dynamic-data-masking", "purview-data-map"], "incompatibleWith": [], "prerequisites": ["medallion-architecture", "row-level-security"], "tags": ["anonymization", "k-anonymity", "differential-privacy", "privacy", "compliance"], "referenceLinks": [{"label": "K-Anonymity and Privacy Techniques", "url": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/privacy-techniques"}, {"label": "Differential Privacy in Azure", "url": "https://learn.microsoft.com/en-us/purview/compliance-privacy-basics"}, {"label": "GDPR Data Minimization Principles", "url": "https://learn.microsoft.com/en-us/compliance/regulatory/gdpr-data-minimization"}], "estimatedImplementationEffort": "6-8 weeks", "costImplications": "No licensing cost; implemented in Fabric Warehouse at standard query rates. Noise injection PySpark notebooks at standard compute rates."}, {"id": "audit-siem-integration", "name": "Audit Log Export and SIEM Integration", "domain": "Data Governance and Security", "domainId": 3, "category": "Monitoring", "summary": "Forwards Fabric activity logs and Power BI audit logs to Azure Sentinel for centralized SOC monitoring, anomaly detection, and forensic investigation.", "description": "Audit log integration ensures all Fabric and Power BI activities (data access, semantic model changes, sharing, export, deletion) are captured, centralized, and analyzed for security incidents. Fabric activity logs (workspace creation, item access, refresh execution, query patterns) flow to Log Analytics Workspace. Power BI activity logs (report view, dataset refresh, sharing changes, export) are forwarded to the Unified Audit Log via the Office 365 Management API. Azure Sentinel consumes these logs, applies correlation rules to detect anomalies (bulk data export, after-hours access, privilege escalation), enriches with identity and threat intelligence, and triggers incidents for SOC investigation. Custom KQL (Kusto Query Language) queries detect HR-specific threats: unusual bulk report exports, access to sensitive employee data by non-HR users, policy violation patterns. Retention ensures logs are available for 1-2 years for forensic analysis and compliance audits.", "fabricComponents": ["Azure Sentinel", "Log Analytics Workspace", "Power BI Activity Log", "Unified Audit Log", "Azure Monitor"], "pros": ["Centralizes audit logs from Fabric and Power BI in single searchable repository enabling cross-system correlation and forensic investigations.", "Detects insider threats and compliance violations in real-time through rule-based alerting and anomaly detection, enabling rapid response.", "Provides evidence trail for regulatory audits and incident response, meeting SOX, HIPAA, and other audit requirements."], "cons": ["Azure Sentinel ingestion costs scale with log volume; busy Fabric environments can generate 1000+ GB/month of logs, increasing licensing costs.", "Lag between activity and Sentinel processing (typically 1-5 minutes) means real-time detection is limited; historical detection takes longer.", "False positive rates in correlation rules can overwhelm SOC; requires significant tuning and stakeholder coordination."], "usageInstructions": "1. Create Log Analytics Workspace in Azure. 2. Enable Fabric activity logging: Workspace Settings > Audit and Compliance > Enable Activity Logging. 3. Create Data Connector in Log Analytics: Fabric Activity > Diagnostic Settings > Send to Log Analytics Workspace. 4. Enable Power BI audit logging: Power BI Admin Portal > Audit and Compliance > Turn on audit log search. 5. Configure Office 365 Management API in Sentinel to consume Unified Audit Log. 6. In Azure Sentinel, create Data Connectors for Office 365 and Log Analytics. 7. Create KQL detection rules: query for bulk exports (>1000 rows), access to SSN columns, after-hours queries. 8. Configure Incidents from detections with SOAR playbook triggers. 9. Create Power BI dashboard for audit KPIs: daily active users, top data consumers, failed access attempts.", "governanceConsiderations": "Establish Security Operations Center (SOC) workflow for Sentinel incidents related to HR data. Define escalation procedures for confirmed security incidents (e.g., data exfiltration). Implement log retention policies: 6 months hot storage, 2 years cold archive. Conduct quarterly reviews of detection rules to reduce false positives. Require formal change management approval for queries accessing sensitive audit data. Ensure audit log access is restricted to security team and select compliance officers.", "peopleAnalyticsUseCases": ["Real-time detection of bulk export of executive compensation reports (>100 rows) to external email domains, triggering SOC investigation into potential salary data exfiltration.", "Anomaly detection identifying that a non-HR user (e.g., IT service account) is accessing employee SSN and salary columns during off-hours, indicating potential credential compromise.", "Forensic investigation of a departing manager's last-day activities: queried employee performance data, shared compensation history report with personal email, then deleted workspace link."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["row-level-security", "workspace-permission-governance", "dlp-policy-enforcement", "privileged-access-management"], "incompatibleWith": [], "prerequisites": [], "tags": ["audit", "siem", "sentinel", "monitoring", "security"], "referenceLinks": [{"label": "Fabric Activity Logging and Monitoring", "url": "https://learn.microsoft.com/en-us/fabric/admin/monitoring-workspace"}, {"label": "Azure Sentinel Overview", "url": "https://learn.microsoft.com/en-us/azure/sentinel/overview"}, {"label": "Power BI Audit Logging", "url": "https://learn.microsoft.com/en-us/power-bi/admin/service-admin-auditing"}], "estimatedImplementationEffort": "3-4 weeks", "costImplications": "Log Analytics Workspace: $0.70/GB ingested. Azure Sentinel: $100+ per GB for first 100GB/day. Detection rule tuning requires SOC analyst time."}, {"id": "dsr-fulfillment", "name": "Data Subject Request Fulfillment Pipeline", "domain": "Data Governance and Security", "domainId": 3, "category": "Privacy Compliance", "summary": "Automates GDPR/PIPEDA data subject rights (access and erasure) with workflows using Purview data lineage, soft-delete mechanisms, and verification processes.", "description": "Data Subject Requests (DSR) are formal legal requests from individuals to access (GDPR Article 15) or erase (GDPR Article 17, right-to-be-forgotten) their personal data. Manual DSR handling is error-prone, slow, and costly. Automated DSR fulfillment pipelines use Microsoft Purview Data Map to identify all systems containing an individual's data, orchestrate retrieval or deletion via Data Factory, perform soft-delete in Fabric Lakehouse with audit trail, notify requestor, and verify completion. For HR analytics, DSR requests require identifying employee records across Lakehouse, data warehouse, Power BI datasets, and archived systems. Soft-delete (marking records inactive) enables 30-day appeal period before hard-delete. Lineage mapping shows which analytics and reports are impacted by erasure. Workflow includes validation steps preventing accidental bulk deletion.", "fabricComponents": ["Microsoft Purview Data Map", "Data Factory Pipeline", "OneLake", "Fabric Notebooks", "Lakehouse"], "pros": ["Automates time-consuming manual search and deletion, reducing DSR fulfillment time from weeks to days and reducing legal/compliance costs.", "Uses Purview lineage to ensure complete erasure across all systems; manual processes frequently miss copies or derived data.", "Provides audit trail proving compliance with GDPR/PIPEDA deadlines (30 days for access, 30 days for erasure), supporting regulatory defense."], "cons": ["Incomplete or inaccurate Purview Data Map can result in missed data systems and non-compliance; requires continuous curation.", "Soft-delete relies on application logic filtering deleted records; risk of query bugs exposing deleted data if filters are incorrect.", "Determining which analytic derivatives (e.g., aggregates, models, exports) must be updated/deleted is complex and data-dependent."], "usageInstructions": "1. Create DSR request intake form (SharePoint form or Power Apps) capturing: requestor name, email, request type (access/erasure), date. 2. Create Data Factory pipeline: Step 1 - Query Purview Data Map API for assets containing individual's data (e.g., Employee.EmployeeId = 12345). Step 2 - Generate list of impacted systems/tables. Step 3 - For access requests, export data to encrypted file in secure SharePoint. For erasure requests, mark records with is_dsr_deleted=1, log deletion timestamp/reason. Step 4 - Wait 30 days for appeals. Step 5 - Hard-delete (VACUUM, UPDATE DELETE). Step 6 - Notify requestor and compliance team. 7. Implement controls: require approvals for bulk erasure, audit all DSR operations, verify deletion completeness.", "governanceConsiderations": "Establish DSR response team with legal, HR, privacy, and data teams. Document DSR procedures and compliance timelines per jurisdiction. Implement technical access controls preventing unauthorized DSR request manipulation. Maintain DSR request audit log with decision and verification steps. Conduct quarterly audits to verify soft-deleted records are truly inaccessible to analytics. Maintain appeal process for 30-day grace period post-deletion request.", "peopleAnalyticsUseCases": ["Automated fulfillment of GDPR access request from Canadian employee: Purview identifies data in Lakehouse employee table, warehouse HRIS system, Power BI reports. Pipeline exports to encrypted PDF in 5 days, within GDPR 30-day SLA.", "Right-to-erasure request from terminated EU employee: soft-delete removes from all active tables, historical audit trails remain but employee salary/SSN are masked from all analytics, 30-day appeal period prevents accidental permanent deletion.", "Cross-border DSR for US employee working in EU office: pipeline identifies data in US Lake House capacity (compliant with GDPR data residency because US subject to standard contractual clauses), executes soft-delete with audit trail."], "complexity": "High", "maturity": "Emerging", "compatibleWith": ["purview-data-map", "medallion-architecture", "audit-siem-integration"], "incompatibleWith": [], "prerequisites": ["purview-data-map", "medallion-architecture"], "tags": ["gdpr", "pipeda", "dsr", "privacy", "right-to-erasure"], "referenceLinks": [{"label": "GDPR Data Subject Rights", "url": "https://learn.microsoft.com/en-us/compliance/regulatory/gdpr-data-subject-rights"}, {"label": "Microsoft Purview Data Map", "url": "https://learn.microsoft.com/en-us/purview/purview-data-catalog"}, {"label": "Data Subject Request Management", "url": "https://learn.microsoft.com/en-us/compliance/regulatory/manage-gdpr-data-subject-requests-summary"}], "estimatedImplementationEffort": "6-8 weeks", "costImplications": "Purview Data Map: included in premium licensing. Data Factory DSR pipelines: $0.50 per execution. Estimated 100-200 DSR requests/year at $50 cost to process each."}, {"id": "disaster-recovery-geo", "name": "Disaster Recovery and Geo-Redundancy", "domain": "Data Sharing and Distribution", "domainId": 8, "category": "Business Continuity", "summary": "Configures OneLake with zone-redundant storage (ZRS) and geo-replication to paired Azure region, with manual failover procedures and multi-geo capacity for data residency.", "description": "Disaster recovery ensures business continuity when regional outages occur. OneLake Zone-Redundant Storage (ZRS) replicates data across three availability zones within a region, protecting against zone-level failures. Geo-replication asynchronously copies data to a paired region (e.g., US East 2 to US Central 1), protecting against regional disasters. Fabric Capacity can be provisioned in multiple regions to enable manual failover. For HR analytics, disaster recovery ensures employee data is always available and compliant with data residency laws (e.g., Canadian data stays in Canada region). Multi-geo capacity configuration enables capacity to be deployed in specific regions. Backup procedures include daily snapshots exported to secure storage. Manual failover involves updating connection strings and repointing workspaces to failover capacity in alternate region. Recovery Time Objective (RTO) for manual failover is 4-8 hours; automated failover via service mesh future capability targets RTO <15 minutes.", "fabricComponents": ["OneLake", "Fabric Capacity", "Azure Region Pairs", "Zone-Redundant Storage"], "pros": ["ZRS provides automatic replication without application changes, protecting against zone and data center failures with no additional cost.", "Geo-replication enables regional compliance (e.g., Canadian data in Canada) and disaster recovery to another region, meeting business continuity SLAs.", "Manual failover procedures are tested and documented, enabling rapid recovery with minimal training."], "cons": ["Geo-replication introduces asynchronous delay (minutes to hours); recent data changes may not be replicated if failover occurs immediately after write.", "Manual failover requires human action and coordination, typically taking 4-8 hours; automated failover is not yet available.", "Multi-region Fabric Capacity costs increase significantly; maintaining hot-hot failover doubles capacity costs vs. cold standby."], "usageInstructions": "1. Enable ZRS on OneLake workspace: Workspace Settings > Storage Redundancy > Zone-Redundant Storage. 2. Configure geo-replication: Workspace Settings > Disaster Recovery > Enable Geo-Replication to paired region (e.g., East 2 to Central 1). 3. Create Data Factory pipeline for daily snapshot export: copy all Lakehouse tables to Azure Storage Account in alternate region. 4. Document failover runbook: 1) Declare disaster, 2) Notify stakeholders, 3) Verify failover region readiness, 4) Update Fabric workspace connection to failover capacity, 5) Repoint Power BI datasets to failover semantic models, 6) Validate data integrity in failover region. 5. Conduct quarterly disaster recovery drill: simulate regional failure, execute failover, validate analytics in alternate region, measure RTO. 6. Maintain failover capacity in standby mode or scale down when not in use.", "governanceConsiderations": "Establish disaster recovery committee with IT, business continuity, and HR stakeholders. Define RTO (Recovery Time Objective) and RPO (Recovery Point Objective) per service level agreement. Document failover procedures and roles. Test failover procedures quarterly with full team participation. Monitor geo-replication lag and alert if lag exceeds SLA. Maintain separate credentials for failover region. Ensure external audit of disaster recovery controls annually.", "peopleAnalyticsUseCases": ["Regional outage in US East region: automatic ZRS failover within same region (1-2 minutes). If both East zones unavailable, manual failover to US Central capacity (4 hours RTO) ensures payroll analytics continue.", "Compliance requirement for Canadian employee data to remain in Canada: Canadian Fabric Capacity with geo-replication to alternate Canadian region, ensuring data never leaves Canada even during failover.", "Financial services regulatory requirement for near-zero RPO: async geo-replication has 30-minute lag, so daily snapshot export to alternate region ensures RPO <24 hours."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["medallion-architecture", "encryption-at-rest-cmk", "audit-siem-integration"], "incompatibleWith": [], "prerequisites": [], "tags": ["disaster-recovery", "business-continuity", "geo-redundancy", "zrs", "failover"], "referenceLinks": [{"label": "OneLake Disaster Recovery and Geo-Redundancy", "url": "https://learn.microsoft.com/en-us/fabric/onelake/redundancy"}, {"label": "Azure Region Pairs for Business Continuity", "url": "https://learn.microsoft.com/en-us/azure/reliability/cross-region-replication-azure"}, {"label": "Fabric Capacity Provisioning and Failover", "url": "https://learn.microsoft.com/en-us/fabric/admin/capacity-admin"}], "estimatedImplementationEffort": "4-6 weeks", "costImplications": "ZRS: no additional cost. Geo-replication: $0.02/GB/month for replication. Failover capacity: $16/hour standby, $96/hour active (Premium P1)."}, {"id": "network-isolation-private-links", "name": "Network Isolation with Private Endpoints", "domain": "Data Governance and Security", "domainId": 3, "category": "Network Security", "summary": "Implements Azure Private Endpoints for Fabric Capacity and managed private endpoints for Spark/compute, ensuring traffic flows only over private VNet and preventing internet exposure.", "description": "Private Endpoints eliminate internet routes for Fabric services, instead routing all traffic through Azure VNet private network. Fabric Capacity (compute and storage) uses Private Endpoints to ensure no data traverses public internet. Managed Private Endpoints for Spark Compute enable secure communication with private data sources (SQL Database in VNet, storage accounts with firewall enabled). For HR analytics in regulated environments (financial services, healthcare), network isolation ensures employee data never transits public internet. Network security groups (NSGs) can further restrict traffic by source/destination IP. VNet integration enables hybrid connectivity: on-premises HR systems can securely connect to Fabric via ExpressRoute. Outbound traffic can be forced through proxy/firewall for inspection. Azure Private Link services provide 99.99% availability.", "fabricComponents": ["Azure Private Link", "Azure VNet", "Managed Private Endpoints", "Fabric Capacity"], "pros": ["Eliminates internet exposure of Fabric endpoints, significantly reducing attack surface and complying with network isolation requirements.", "Enables hybrid connectivity to on-premises systems without VPN, improving performance and security.", "Provides network-layer segmentation complementing identity and data-layer security controls, following defense-in-depth principle."], "cons": ["Private Endpoints add operational complexity: VNet design, routing, DNS configuration, and troubleshooting require network expertise.", "Cost increases: Fabric Capacity pricing unchanged but Private Endpoints charge $0.50/hour per endpoint, adding $360/month per capacity.", "Hybrid connectivity requires ExpressRoute or Site-to-Site VPN; on-premises connectivity adds complexity and latency."], "usageInstructions": "1. Create Azure VNet in the same region as Fabric Capacity. 2. In Fabric Workspace Settings, enable Private Endpoints: select Capacity > Network > Enable Private Endpoint. 3. Azure creates Private Link Service and provides Private Endpoint Connection. 4. In your VNet, create Private Endpoint resource: +Create > Private Endpoint, select Fabric service, select subnet, configure DNS integration. 5. Update DNS resolver to map fabric.microsoft.com to private IP (e.g., 10.0.0.5). 6. Test connectivity: connect VM in VNet, query nslookup fabric.microsoft.com (should resolve to private IP). 7. Configure Managed Private Endpoints for Spark: Workspace > Managed Private Endpoints > +New, select target resource (SQL Database, storage), approve in target resource. 8. Configure Network Security Group (NSG): allow only VNet subnets to reach Fabric Private Endpoint, deny internet-routed traffic.", "governanceConsiderations": "Establish network security architecture review with IT security and compliance teams. Document VNet design and Private Endpoint locations. Implement bastion hosts for secure VM access instead of public IPs. Enforce NSG rules preventing traffic to public internet. Monitor Private Endpoint connections and deny suspicious outbound attempts. Require change management approval for VNet/NSG changes. Conduct quarterly network segmentation audits.", "peopleAnalyticsUseCases": ["Financial services firm isolates HR analytics Fabric Capacity on private VNet, restricts outbound traffic to approved SIEM sink via proxy, ensuring no employee data transits internet.", "Healthcare provider connects on-premises HRIS system via ExpressRoute to Fabric over private endpoint, enabling analytics without routing PII through public internet or VPN.", "Multi-tenant SaaS provider isolates each customer's Fabric Capacity on separate VNet, using managed private endpoints to connect to customer-specific SQL databases on-premises."], "complexity": "High", "maturity": "GA", "compatibleWith": ["encryption-at-rest-cmk", "workspace-permission-governance", "audit-siem-integration"], "incompatibleWith": [], "prerequisites": [], "tags": ["network-security", "private-endpoints", "vnet", "isolation", "compliance"], "referenceLinks": [{"label": "Azure Private Endpoints for Fabric", "url": "https://learn.microsoft.com/en-us/fabric/security/security-private-endpoints-overview"}, {"label": "Azure VNet and NSG Configuration", "url": "https://learn.microsoft.com/en-us/azure/virtual-network/manage-network-security-group"}, {"label": "Managed Private Endpoints for Spark", "url": "https://learn.microsoft.com/en-us/fabric/security/security-managed-private-endpoints-overview"}], "estimatedImplementationEffort": "4-6 weeks", "costImplications": "Private Endpoint: $0.50/hour ($360/month per endpoint). VNet/NSG: no cost. ExpressRoute (if used): $0.30-0.50/hour depending on bandwidth."}, {"id": "privileged-access-management", "name": "Just-in-Time Privileged Access Management", "domain": "Data Governance and Security", "domainId": 3, "category": "Identity Security", "summary": "Implements Azure Entra ID Privileged Identity Management (PIM) for just-in-time elevation of workspace admin roles, with approval workflows and MFA enforcement.", "description": "Privileged Identity Management (PIM) reduces standing permissions, requiring users to request temporary elevation to admin roles. For HR analytics, workspace admins manage data governance policies, sharing, and sensitive configurations. Rather than making users permanent admins (standing privilege), PIM makes them 'Eligible Admins' who must request activation. Requests require approval from compliance officer and MFA authentication. Activation is time-limited (1-8 hours) and audited. Break-glass emergency accounts provide fallback access if primary admins are unavailable. Multi-factor authentication (MFA) is required for all privileged actions. Audit logs in Entra ID capture all activation requests, approvals, and actions performed during elevated access. For compliance, PIM demonstrates least-privilege principle and immediate detection of unauthorized privilege escalation attempts.", "fabricComponents": ["Azure Entra ID PIM", "Fabric Workspace Roles", "Azure Key Vault", "Azure Sentinel"], "pros": ["Reduces standing privilege, dramatically lowering risk of compromise: admin account hack affects only active activation window (1-8 hours) not 365 days.", "Requires approval for every elevation, creating human checkpoint preventing unauthorized access.", "Complete audit trail of privilege escalation enables detection of unusual elevation patterns and supports compliance audits."], "cons": ["PIM adds friction to emergency access scenarios, requiring approval workflow adds 15-30 minutes to incident response.", "Admin users must have Entra ID Premium P2 license, adding cost ~$30-50/user/month.", "Misconfigured break-glass accounts (credentials leaked, not rotated) undermine PIM's security benefits."], "usageInstructions": "1. Ensure Entra ID Premium P2 licensed. 2. In Entra ID > Privileged Identity Management > Fabric Workspace Roles, select 'Workspace Admin' role. 3. Set eligible users: add workspace managers as Eligible (not Permanent) members. 4. Configure activation: require approval, require MFA, set max activation duration to 4 hours. 5. Select approval delegator: compliance officer or security team. 6. Configure notifications: alert on elevations, email approvers. 7. Create break-glass account: dedicate account for emergency access, store credentials in physical safe (not digital). 8. Test activation: Eligible Admin requests activation, approver receives email, activates with MFA. 9. Monitor in Azure Sentinel: create alert for approval denials, unusual activation times, after-hours elevations. 10. Quarterly review: audit PIM logs, revoke unused eligible access.", "governanceConsiderations": "Establish PIM governance committee with security, HR, and compliance stakeholders. Document approval process and SLA for activation requests (should be <15 minutes). Implement technical controls enforcing MFA (hardware keys preferred). Rotate break-glass credentials quarterly with multiple stakeholders witnessing. Test break-glass emergency access annually. Review PIM logs monthly for suspicious patterns. Enforce organization-wide policy: no permanent privileged roles, all admins must use PIM.", "peopleAnalyticsUseCases": ["HR analytics workspace admin (normally ineligible) requests elevation to investigate unauthorized data export, approver grants 4-hour activation via MFA approval, admin investigates access logs with limited time window.", "Data governance officer regularly activates to review sensitivity label assignments and RLS policies, must approve each activation with business justification.", "Emergency incident: production Fabric capacity is down, break-glass account holder activates with MFA, gains temporary admin access to restart services, audited and revoked within 1 hour."], "complexity": "Medium", "maturity": "GA", "compatibleWith": ["workspace-permission-governance", "audit-siem-integration", "encryption-at-rest-cmk"], "incompatibleWith": [], "prerequisites": [], "tags": ["pim", "privileged-access", "identity", "mfa", "compliance"], "referenceLinks": [{"label": "Azure Entra ID Privileged Identity Management", "url": "https://learn.microsoft.com/en-us/entra/id-governance/privileged-identity-management/pim-configure"}, {"label": "PIM for Fabric Workspace Roles", "url": "https://learn.microsoft.com/en-us/fabric/admin/pim-setup"}, {"label": "Multi-Factor Authentication Best Practices", "url": "https://learn.microsoft.com/en-us/entra/identity/authentication/concept-mfa-licensing"}], "estimatedImplementationEffort": "2-3 weeks", "costImplications": "Entra ID Premium P2: $30-50/user/month (per admin). PIM: included in Premium P2. Hardware MFA keys: $20-50 per unit."}, {"id": "change-management-four-eyes", "name": "Dual-Approval Change Management Pipeline", "domain": "Data Transformation and Processing", "domainId": 2, "category": "Change Control", "summary": "Enforces dual-approval (business and governance) before production pipeline deployment, implementing four-eyes principle via Azure DevOps gates.", "description": "Four-eyes principle requires two independent approvals before changes to production systems. For HR analytics, changes to data pipelines (transformations, data quality rules, refresh schedules) can affect payroll, benefits, or compliance analytics. Requiring dual approval ensures both business correctness (HR manager) and governance compliance (data governance officer) are verified. Azure DevOps Deployment Pipelines implement gates between environments: development -> staging -> production. Staging gate requires business approval from HR analytics owner confirming transformations are correct. Production gate requires data governance approval confirming RLS policies, data lineage, and compliance are maintained. Pull request reviews enforce code quality and documentation before merge to main. Git branching strategy separates features, requiring peer review. Change log automatically documents approvers, timestamp, and change summary. Rejection of changes includes audit trail for compliance.", "fabricComponents": ["Azure DevOps", "Data Factory Pipeline", "Fabric Deployment Pipelines", "Git Integration"], "pros": ["Enforces dual approval, preventing single-person errors and unauthorized changes to critical analytics.", "Creates audit trail proving compliance with change control requirements, supporting SOX, HIPAA audits.", "Improves quality by requiring peer review before production deployment."], "cons": ["Adds cycle time: waiting for two approvers can delay urgent fixes (typical cycle time 24-48 hours).", "Requires both approvers to be available; absence of approver blocks deployment.", "False sense of security if approvers don't actually review changes carefully."], "usageInstructions": "1. Set up Azure DevOps project with Git repo for Fabric pipeline definitions. 2. Configure branch policy on main: require pull request reviews, minimum 2 approvers (business + governance), status checks passing. 3. Create staging deployment pipeline: trigger on PR approval, deploy to staging environment. 4. Add pre-deployment gate before staging: auto-approve (runs tests, validates syntax). 5. Add pre-deployment gate before production: manual approval, allow only specific users (data governance team). Require justification/description. 6. Create business approval gate: HR manager reviews transformations, confirms correctness. 7. Track approvals: Azure DevOps automatically logs timestamp, approver identity, comments. 8. Reject approvals include mandatory reason. 9. Create dashboard: count deployments, approval time metrics, rejection rates. 10. Quarterly review: analyze approval bottlenecks, optimize process.", "governanceConsiderations": "Define who can request, approve, and reject changes: business sponsor (business approval), data governance (compliance approval). Document approval criteria: business approval verifies transformations match requirements, governance approval verifies RLS, lineage, data quality, compliance. Implement escalation path for urgent changes (e.g., 24-hour SLA for critical bug fixes). Audit approval logs monthly. Require documented change rationale in pull request. Disallow approval from same person who submitted change (dual-approval enforced technically).", "peopleAnalyticsUseCases": ["Data engineer submits PR changing employee salary aggregation logic (e.g., fixing bonus calculation). Business sponsor (payroll manager) approves confirming calculation is correct. Data governance approves confirming aggregation preserves privacy (k>=5). Production deployment occurs only after both approvals.", "New HR dataset onboarded to Lakehouse: ingestion pipeline PR submitted. Business sponsor approves confirming data matches HRIS system. Data governance approves confirming sensitivity labels assigned, lineage documented. Both approvals required before prod activation.", "Urgent hotfix: salary dashboard showing incorrect totals. Change submitted with 'urgent' flag. Both approvers pinged, typically respond within 2-4 hours. Production deployment after both approvals."], "complexity": "Medium", "maturity": "Emerging", "compatibleWith": ["deployment-pipelines", "audit-siem-integration"], "incompatibleWith": [], "prerequisites": [], "tags": ["change-management", "devops", "approval", "governance", "four-eyes"], "referenceLinks": [{"label": "Azure DevOps Release Gates and Approvals", "url": "https://learn.microsoft.com/en-us/azure/devops/pipelines/release/approvals/approvals"}, {"label": "Fabric Deployment Pipelines", "url": "https://learn.microsoft.com/en-us/fabric/cicd/deployment-pipelines/intro"}, {"label": "Git Branching Strategy and Pull Request Reviews", "url": "https://learn.microsoft.com/en-us/azure/devops/repos/git/branch-policies"}], "estimatedImplementationEffort": "3-4 weeks", "costImplications": "Azure DevOps: free for up to 5 users/project. Git: no cost. Slack/Teams notifications: included. Process overhead: ~1-2 hours per change for approvals."}, {"id": "cross-border-data-residency", "name": "Cross-Border Data Residency Isolation", "domain": "Data Sharing and Distribution", "domainId": 8, "category": "Data Sovereignty", "summary": "Isolates employee data by geography using multi-geo Fabric capacities, ensuring Canadian employee data remains in Canada and US data in US, with aggregate-only cross-border reporting.", "description": "Cross-border data residency ensures that employee personal data is stored and processed only within specified geographies per regulatory requirements. For multinational HR analytics, Canadian employees' data (salary, SSN, benefits) must remain in Canada due to PIPEDA. US data must remain in US due to state regulations and employer obligations. Fabric multi-geo capacity configuration assigns workspaces to specific regions; OneLake data is geo-pinned to that region. Cross-border reporting uses aggregate-only views (k-anonymity approach) ensuring individual records cannot be queried across borders. Data sharing uses Shortcuts with filtering, preventing cross-border record-level export. Periodic reviews ensure no data has migrated across borders. Audit logs track cross-border access attempts.", "fabricComponents": ["Fabric Capacity", "OneLake", "Workspace Assignment", "Multi-Geo Configuration"], "pros": ["Meets regulatory data residency requirements (PIPEDA, GDPR, local laws) preventing costly compliance violations.", "Enables multi-country operations with confidence that data stays in authorized regions.", "Provides operational resilience: country-level outage affects only that region's operations, not global."], "cons": ["Multi-region capacities increase costs ~2-3x vs. single-region (separate capacity per region).", "Analytics across regions requires federated queries or cross-border aggregates; complex joins are impossible.", "Data migration for employee moves (e.g., employee relocates from Canada to US) requires careful handling: old records deletion or transfer."], "usageInstructions": "1. Create separate Fabric Capacity per geography: Canadian Capacity (Canada Central region), US Capacity (US East 2 region). 2. Create separate workspaces per geography: 'HR-Analytics-CA' in Canadian capacity, 'HR-Analytics-US' in US capacity. 3. Ingest employee data to geo-pinned Lakehouse: Canadian employee table in Canada workspace, US employee table in US workspace. 4. For cross-border reporting, create aggregate-only views: SELECT department, YEAR(dob) as year_of_birth, COUNT(*) as employee_count FROM employees WHERE country='CA' GROUP BY department, YEAR(dob) HAVING COUNT(*) >= 5. 5. Create federated semantic models: Power BI connects to Canadian and US aggregate views, combines aggregates (no record-level data). 6. Prevent cross-border shortcuts: Workspace Sharing > restrict shortcuts to same-region workspaces. 7. Audit cross-border access: Log Analytics tracks queries across regions, alert on suspicious activity. 8. Data migration procedure: terminating employee, update country field, archive to historical table in original region, do not migrate raw records.", "governanceConsiderations": "Establish data residency governance committee with legal, compliance, and HR stakeholders per country. Document residency requirements by jurisdiction (PIPEDA, GDPR, state laws). Implement technical enforcement: prevent shortcuts crossing regions, audit logs for attempted cross-border access. Quarterly audit: verify no employee data exists in wrong region. Data transfer agreements: document how employee relocations are handled. Secure deletion: ensure migrated data is permanently deleted from source region.", "peopleAnalyticsUseCases": ["Canadian bank with Canadian HQ + US subsidiary: employees are separated by capacity/workspace. Canadian headquarters views all Canadian employee analytics in Canadian workspace (compliant with PIPEDA). US subsidiary views US employee analytics in US workspace. Joint reporting uses aggregates: total Canadian headcount + total US headcount, no cross-border record-level joins.", "Multinational tech company with employees in Canada, US, and EU: separate capacities for each region. Annual global report aggregates at country-group level: 'Canada 5000 employees, US 10000 employees, EU 3000 employees' without exposing individual records.", "Employee relocation: Canadian employee transfers to US. Old record in Canadian Lakehouse is soft-deleted (marked inactive), no migration to US workspace. US onboarding creates new record in US workspace for same employee."], "complexity": "High", "maturity": "GA", "compatibleWith": ["medallion-architecture", "encryption-at-rest-cmk", "disaster-recovery-geo"], "incompatibleWith": [], "prerequisites": [], "tags": ["data-residency", "compliance", "gdpr", "pipeda", "sovereignty"], "referenceLinks": [{"label": "Microsoft Fabric Multi-Geo Capabilities", "url": "https://learn.microsoft.com/en-us/fabric/admin/capacity-multi-geo"}, {"label": "PIPEDA Data Residency Requirements", "url": "https://learn.microsoft.com/en-us/compliance/regulatory/offering-pipeda"}, {"label": "GDPR Data Residency and Transfers", "url": "https://learn.microsoft.com/en-us/compliance/regulatory/gdpr-data-location"}], "estimatedImplementationEffort": "6-8 weeks", "costImplications": "Multi-region Fabric Capacity: 2-3x capacity cost (separate capacity per region). Premium P1 capacity: $16/hour per region. Cross-border OneLake replication: $0.02/GB/month."}];

        const BLUEPRINTS = [{"id": "workforce-dashboard-foundation", "name": "Workforce Analytics Dashboard (Foundation)", "description": "Basic headcount, turnover, and diversity dashboard for HR leadership providing visibility into core workforce metrics with row-level security for sensitive reporting.", "audience": "HR Leadership, CHRO, People Operations", "patterns": ["medallion-architecture", "direct-lake-semantic-model", "directlake-power-bi", "row-level-security", "sensitivity-labels", "encryption-at-rest-cmk"], "patternFlow": [{"from": "medallion-architecture", "to": "direct-lake-semantic-model", "description": "Gold-layer tables feed directly into DirectLake semantic model without data duplication"}, {"from": "direct-lake-semantic-model", "to": "directlake-power-bi", "description": "DirectLake semantic model powers interactive Power BI dashboards with instant query response"}, {"from": "directlake-power-bi", "to": "row-level-security", "description": "Power BI reports enforce row-level security to restrict data by HR role and business unit"}, {"from": "row-level-security", "to": "sensitivity-labels", "description": "RLS-protected fields labeled with sensitivity tags for audit compliance"}, {"from": "sensitivity-labels", "to": "encryption-at-rest-cmk", "description": "Sensitive headcount and compensation data encrypted with customer-managed keys"}], "editableNotes": "Customize this blueprint for your organization by selecting which workforce metrics (headcount by department, turnover trends, diversity breakdowns) are most relevant to your HR leadership KPIs. Adjust RLS roles based on your org hierarchy and reporting requirements.", "estimatedTotalEffort": "3-4 weeks", "keyGovernanceRequirements": ["Define RLS roles matching HR organizational structure", "Classify headcount and diversity data with sensitivity labels", "Encrypt Lakehouse storage with customer-managed keys", "Establish refresh SLA for daily HR metrics"], "tags": ["dashboarding", "hr-leadership", "foundational", "security", "directlake"]}, {"id": "attrition-risk-scoring", "name": "Employee Attrition Risk Prediction Pipeline", "description": "ML-based flight risk scoring with fairness controls and bias evaluation to identify at-risk employees while ensuring equitable predictions across demographic groups.", "audience": "Data Scientists, HR Analytics, Talent Retention Teams", "patterns": ["medallion-architecture", "spark-notebook-etl", "feature-store-delta", "batch-inference-pipeline", "fairness-bias-evaluation", "mlflow-model-registry", "champion-challenger", "data-activator-reflex"], "patternFlow": [{"from": "medallion-architecture", "to": "spark-notebook-etl", "description": "Gold-layer employee and engagement data extracted for feature engineering"}, {"from": "spark-notebook-etl", "to": "feature-store-delta", "description": "Engineered features (tenure, promotion history, engagement scores) stored in versioned feature tables"}, {"from": "feature-store-delta", "to": "batch-inference-pipeline", "description": "Features fed into batch scoring pipeline that predicts attrition probability monthly"}, {"from": "batch-inference-pipeline", "to": "fairness-bias-evaluation", "description": "Predictions evaluated for bias across age, gender, and tenure groups"}, {"from": "fairness-bias-evaluation", "to": "mlflow-model-registry", "description": "Fair, bias-evaluated models registered with fairness metrics and thresholds"}, {"from": "mlflow-model-registry", "to": "champion-challenger", "description": "New models tested against champion production model before promotion"}, {"from": "champion-challenger", "to": "data-activator-reflex", "description": "High-risk employee scores trigger alerts to HR managers via Data Activator actions"}], "editableNotes": "Customize this blueprint by selecting features most predictive of attrition in your organization (e.g., manager stability, compensation benchmarking, internal promotion velocity). Define fairness thresholds and bias tolerance by demographic group based on HR policy.", "estimatedTotalEffort": "8-12 weeks", "keyGovernanceRequirements": ["Track fairness metrics and bias evaluation results in MLflow", "Document feature selection and model training datasets for auditability", "Establish human review process for high-risk employee scores before HR action", "Maintain model retraining schedule and performance monitoring dashboard"], "tags": ["ml-modeling", "prediction", "fairness", "risk-scoring", "mlops"]}, {"id": "compensation-analytics-secure", "name": "Secure Compensation Analytics", "description": "Salary and bonus analytics with dynamic masking, DLP policy enforcement, and executive-level data isolation to enable compensation analytics while protecting sensitive payroll information.", "audience": "Compensation & Benefits Teams, Finance, Executive Leadership", "patterns": ["medallion-architecture", "scd-type-2", "dynamic-data-masking", "row-level-security", "dlp-policy-enforcement", "anonymization-k-anonymity", "certified-semantic-model", "paginated-reports"], "patternFlow": [{"from": "medallion-architecture", "to": "scd-type-2", "description": "Gold-layer compensation data maintains history using Slowly Changing Dimension Type 2 for salary changes"}, {"from": "scd-type-2", "to": "dynamic-data-masking", "description": "Compensation amounts masked based on user role (HR sees actuals, managers see bands, employees see anonymized)"}, {"from": "dynamic-data-masking", "to": "row-level-security", "description": "RLS ensures compensation data visible only to appropriate stakeholders (compensation team, relevant managers)"}, {"from": "row-level-security", "to": "dlp-policy-enforcement", "description": "DLP policies prevent export or copying of compensation data to unsecured locations"}, {"from": "dlp-policy-enforcement", "to": "anonymization-k-anonymity", "description": "Aggregate compensation analytics anonymized with k-anonymity to prevent individual identification"}, {"from": "anonymization-k-anonymity", "to": "certified-semantic-model", "description": "Anonymized, aggregated compensation metrics certified for self-service use by HR analysts"}, {"from": "certified-semantic-model", "to": "paginated-reports", "description": "Paginated executive compensation reports with audit trails and DLP watermarks"}], "editableNotes": "Customize masking rules to match your organization's compensation governance (e.g., what executive levels see individual vs. aggregate data). Define RLS groups for compensation team, business unit managers, and executives. Adjust k-anonymity thresholds based on your workforce size.", "estimatedTotalEffort": "6-8 weeks", "keyGovernanceRequirements": ["Define dynamic masking rules for each user role (HR specialist, manager, employee, executive)", "Establish DLP policies preventing compensation data export and enforcing watermarking", "Document SCD Type 2 salary history changes with effective dates for audit compliance", "Monitor and audit compensation data access through sensitivity labels and SIEM integration"], "tags": ["compensation", "security", "masking", "dlp", "privacy-preserving"]}, {"id": "regulatory-compliance-stack", "name": "Regulatory Compliance & Audit Stack", "description": "Full governance stack for OSFI and Federal Reserve regulated environments with encryption, sensitivity classification, audit trails, and privileged access management.", "audience": "Compliance Officers, Internal Audit, Security Teams, Risk Management", "patterns": ["encryption-at-rest-cmk", "sensitivity-labels", "purview-data-map", "dlp-policy-enforcement", "audit-siem-integration", "workspace-permission-governance", "privileged-access-management", "network-isolation-private-links"], "patternFlow": [{"from": "encryption-at-rest-cmk", "to": "sensitivity-labels", "description": "All HR data encrypted with customer-managed keys, then classified by sensitivity level (regulatory, confidential, internal)"}, {"from": "sensitivity-labels", "to": "purview-data-map", "description": "Sensitivity labels propagated through Purview to create data lineage map with classification metadata"}, {"from": "purview-data-map", "to": "dlp-policy-enforcement", "description": "Purview metadata triggers DLP policies restricting access and export of highly sensitive HR data"}, {"from": "dlp-policy-enforcement", "to": "audit-siem-integration", "description": "DLP policy violations and access attempts logged to SIEM for regulatory audit and forensics"}, {"from": "audit-siem-integration", "to": "workspace-permission-governance", "description": "SIEM audit data correlates with workspace-level permission changes for access governance audit trail"}, {"from": "workspace-permission-governance", "to": "privileged-access-management", "description": "Workspace admin and sensitive data access controlled through PAM with time-bound approval workflows"}, {"from": "privileged-access-management", "to": "network-isolation-private-links", "description": "PAM-approved access routes through private endpoints to prevent internet exposure of regulatory data"}], "editableNotes": "Customize this stack for your specific regulatory environment (OSFI, Federal Reserve, GDPR, PIPEDA). Define sensitivity labels matching regulatory classifications (e.g., 'OSFI Confidential', 'Trade Secret'). Configure PAM approval workflows for compliance officer reviews.", "estimatedTotalEffort": "12-16 weeks", "keyGovernanceRequirements": ["Implement CMK encryption with regulated key escrow and audit logging", "Establish sensitivity classification scheme aligned with regulatory requirements", "Deploy SIEM integration with 90-day+ audit log retention for regulatory inspections", "Implement PAM with separation of duties: requestor, approver, and executor roles"], "tags": ["compliance", "regulatory", "security", "audit", "governance"]}, {"id": "hr-chatbot-secure", "name": "Secure HR Policy Chatbot (GenAI)", "description": "RAG-based conversational AI agent grounded in HR policies and employee data with row-level security enforcement and audit logging for secure policy Q&A.", "audience": "HR Service Centers, Employees, Managers", "patterns": ["medallion-architecture", "rag-fabric-grounded", "secure-chat-rls", "copilot-studio-grounded", "hr-ai-guardrails", "audit-siem-integration"], "patternFlow": [{"from": "medallion-architecture", "to": "rag-fabric-grounded", "description": "Gold-layer HR policies, benefits documentation, and employee data indexed as vectors for RAG retrieval"}, {"from": "rag-fabric-grounded", "to": "secure-chat-rls", "description": "RAG retriever applies row-level security to ensure employees only see personally relevant information (their benefits, vacation balance)"}, {"from": "secure-chat-rls", "to": "copilot-studio-grounded", "description": "Secure RAG context fed into Copilot Studio for natural conversational interface with guardrails"}, {"from": "copilot-studio-grounded", "to": "hr-ai-guardrails", "description": "Copilot response generation constrained with HR-specific guardrails to prevent hallucinations and inappropriate answers"}, {"from": "hr-ai-guardrails", "to": "audit-siem-integration", "description": "All chat interactions and guardrail violations logged to SIEM for compliance auditing"}], "editableNotes": "Customize the chatbot for your HR context by selecting which policies, benefits information, and employee data to ground the RAG retriever in. Define guardrails to prevent the chatbot from answering sensitive questions (e.g., compensation, performance reviews). Configure escalation flows for complex questions.", "estimatedTotalEffort": "6-8 weeks", "keyGovernanceRequirements": ["Define RLS rules for chatbot to protect employee personal data (salary, SSN, medical info)", "Document guardrails and test for prompt injection/jailbreak attempts before production", "Enable audit logging of all chatbot interactions including question, response, and RLS context applied", "Establish escalation workflow for guardrail violations and questions requiring human HR review"], "tags": ["generative-ai", "chatbot", "rag", "security", "guardrails"]}, {"id": "onboarding-offboarding-pipeline", "name": "Employee Lifecycle Data Pipeline", "description": "Full hire-to-retire data pipeline capturing employee lifecycle events with change data capture, consent management, data retention, and support for data subject requests.", "audience": "HR Operations, Data Governance, Privacy Officers", "patterns": ["medallion-architecture", "cdc-change-capture", "scd-type-2", "data-quality-validation", "data-retention-lifecycle", "dsr-fulfillment", "sensitivity-labels"], "patternFlow": [{"from": "medallion-architecture", "to": "cdc-change-capture", "description": "Bronze layer captures all employee lifecycle events (hire, promotion, termination) via change data capture"}, {"from": "cdc-change-capture", "to": "scd-type-2", "description": "CDC events processed into Slowly Changing Dimension Type 2 to maintain full employment history with effective dates"}, {"from": "scd-type-2", "to": "data-quality-validation", "description": "SCD records validated for completeness (all required lifecycle fields present, dates consistent)"}, {"from": "data-quality-validation", "to": "data-retention-lifecycle", "description": "Valid lifecycle records assigned retention policies based on employee status and contract terms"}, {"from": "data-retention-lifecycle", "to": "dsr-fulfillment", "description": "Retention lifecycle enables data subject request fulfillment by identifying and anonymizing/deleting records per policy"}, {"from": "dsr-fulfillment", "to": "sensitivity-labels", "description": "Lifecycle records labeled with consent status and retention period for auditing DSR compliance"}], "editableNotes": "Customize retention policies for your organization's legal and business requirements (e.g., PIPEDA requires 1-year post-termination retention for severance audits). Define data quality checks matching your HRIS systems (Workday, SuccessFactors). Configure DSR templates for employee data portability requests.", "estimatedTotalEffort": "8-10 weeks", "keyGovernanceRequirements": ["Document retention policy for each employee lifecycle stage and jurisdiction (Canada, US, EU)", "Implement data quality validation rules with alerting for missing hire dates, employment terminations", "Create DSR fulfillment workflow with legal review before deletion of historical employment data", "Maintain audit log of retention policy enforcement and DSR requests with approval chains"], "tags": ["lifecycle", "privacy", "dsr", "retention", "compliance"]}, {"id": "cross-border-hr-analytics", "name": "Multi-Country HR Analytics (Data Residency)", "description": "Separate Canadian and US employee data with aggregate-only cross-border reporting to comply with data residency requirements while enabling global insights.", "audience": "Global HR Leadership, Compliance, Data Governance", "patterns": ["cross-border-data-residency", "medallion-architecture", "encryption-at-rest-cmk", "row-level-security", "anonymization-k-anonymity", "hub-spoke-workspace"], "patternFlow": [{"from": "cross-border-data-residency", "to": "medallion-architecture", "description": "Data residency rules define separate Bronze/Silver/Gold medallion stacks per country (Canada East, US East)"}, {"from": "medallion-architecture", "to": "encryption-at-rest-cmk", "description": "Each country's medallion stack encrypted with region-specific customer-managed keys (Canada uses Canadian key vault)"}, {"from": "encryption-at-rest-cmk", "to": "row-level-security", "description": "RLS prevents Canadian users from accessing US employee data and vice versa at semantic model layer"}, {"from": "row-level-security", "to": "anonymization-k-anonymity", "description": "Cross-border aggregate reporting anonymizes individual employee records with k-anonymity before combining countries"}, {"from": "anonymization-k-anonymity", "to": "hub-spoke-workspace", "description": "Hub workspace in neutral region aggregates anonymized global metrics; spoke workspaces maintain country-specific detailed data"}], "editableNotes": "Customize data residency rules for your organization's footprint (Canada, US, EU, Asia). Define k-anonymity thresholds per country based on workforce size (e.g., k=5 minimum for any demographic slice). Configure hub-spoke topology matching your governance structure.", "estimatedTotalEffort": "10-14 weeks", "keyGovernanceRequirements": ["Document data flows and residency requirements for each country with legal review", "Implement separate encryption keys per country with key rotation per local compliance requirements", "Define anonymization thresholds (k-anonymity) for cross-border reporting based on country privacy laws", "Establish hub-spoke workspace access controls with country-level RLS at semantic model layer"], "tags": ["data-residency", "privacy", "cross-border", "global", "compliance"]}, {"id": "self-service-bi-governed", "name": "Governed Self-Service BI Platform", "description": "Enable HR analysts to build reports on certified data without creating shadow analytics by providing governed self-service with certified models, deployment pipelines, and metrics scorecards.", "audience": "HR Analytics Teams, Business Analysts, HR Leadership", "patterns": ["medallion-architecture", "certified-semantic-model", "hub-spoke-workspace", "deployment-pipelines", "sensitivity-labels", "workspace-permission-governance", "metrics-scorecard"], "patternFlow": [{"from": "medallion-architecture", "to": "certified-semantic-model", "description": "Gold-layer HR metrics tables certified by data governance team for safe self-service analysis"}, {"from": "certified-semantic-model", "to": "hub-spoke-workspace", "description": "Certified model deployed to hub workspace as shared analytics foundation; spokes enable team-specific reports"}, {"from": "hub-spoke-workspace", "to": "deployment-pipelines", "description": "Hub model promoted to production via automated deployment pipeline with testing, version control, and rollback"}, {"from": "deployment-pipelines", "to": "sensitivity-labels", "description": "Deployed models inherit sensitivity labels from source data to enforce security in self-service reports"}, {"from": "sensitivity-labels", "to": "workspace-permission-governance", "description": "Workspace permissions aligned with sensitivity labels: internal data visible to HR, confidential data to leadership only"}, {"from": "workspace-permission-governance", "to": "metrics-scorecard", "description": "Metrics scorecards built on certified model in governed workspace, providing single source of truth for HR KPIs"}], "editableNotes": "Customize certified metrics to match your organization's core HR KPIs (headcount, turnover, promotion velocity, diversity). Define governance approval workflow for new metrics (data owner approval, security review). Configure deployment pipeline with testing thresholds for metric validation.", "estimatedTotalEffort": "7-9 weeks", "keyGovernanceRequirements": ["Establish semantic model certification process with data owner sign-off before self-service use", "Define deployment pipeline with automated testing, peer review, and change log for all model updates", "Configure workspace permissions and sensitivity labels matching organizational security policies", "Create metrics scorecard governance to prevent metric proliferation and ensure consistency across org"], "tags": ["self-service-bi", "governance", "democratization", "metrics", "analytics-platform"]}];

        // Domain mapping
        const DOMAINS = {
            "Data Organization and Structuring": { id: 1, color: "domain-color-1" },
            "Data Transformation and Processing": { id: 2, color: "domain-color-2" },
            "Data Governance and Security": { id: 3, color: "domain-color-3" },
            "Business Intelligence and Reporting": { id: 4, color: "domain-color-4" },
            "Machine Learning and Traditional AI": { id: 5, color: "domain-color-5" },
            "Generative AI and Conversational Interfaces": { id: 6, color: "domain-color-6" }
        };

        // Application state
        let appState = {
            currentTab: 'browse',
            filters: {
                search: '',
                domains: [],
                complexity: [],
                maturity: []
            },
            builderStack: [],
            currentPattern: null,
            currentBlueprint: null
        };

        // Initialize app
        document.addEventListener('DOMContentLoaded', initApp);

        function initApp() {
            try { setupTabs(); } catch(e) { console.error('setupTabs error:', e); }
            try { setupBrowseTab(); } catch(e) { console.error('setupBrowseTab error:', e); }
            try { setupBuilderTab(); } catch(e) { console.error('setupBuilderTab error:', e); }
            try { setupBlueprintsTab(); } catch(e) { console.error('setupBlueprintsTab error:', e); document.getElementById('blueprints-grid').innerHTML = '<div style="color:red;padding:2rem;">Error loading blueprints: ' + e.message + '</div>'; }
        }

        // TAB SWITCHING
        function setupTabs() {
            document.querySelectorAll('.tab-btn').forEach(btn => {
                btn.addEventListener('click', (e) => {
                    const tabName = e.target.dataset.tab;
                    switchTab(tabName);
                });
            });
        }

        function switchTab(tabName) {
            // Hide all tabs
            document.querySelectorAll('.tab-content').forEach(tab => {
                tab.classList.remove('active');
            });
            document.querySelectorAll('.tab-btn').forEach(btn => {
                btn.classList.remove('active');
            });

            // Show selected tab
            document.getElementById(tabName + '-tab').classList.add('active');
            document.querySelector(`[data-tab="${tabName}"]`).classList.add('active');
            appState.currentTab = tabName;
        }

        // BROWSE CATALOG TAB
        function setupBrowseTab() {
            setupFilters();
            renderCatalog();
        }

        function setupFilters() {
            // Domain filter
            const domains = [...new Set(PATTERNS.map(p => p.domain))].sort();
            const domainFilter = document.getElementById('domain-filter');
            domains.forEach(domain => {
                const count = PATTERNS.filter(p => p.domain === domain).length;
                const label = document.createElement('label');
                label.className = 'domain-item';
                label.innerHTML = `<input type="checkbox" value="${domain}" data-filter="domain">
                    <span class="domain-label">${domain}</span>
                    <span class="badge">${count}</span>`;
                domainFilter.appendChild(label);
            });

            // Complexity filter
            const complexityFilter = document.getElementById('complexity-filter');
            ['Low', 'Medium', 'High'].forEach(c => {
                const label = document.createElement('label');
                label.className = 'toggle-item';
                label.innerHTML = `<input type="checkbox" value="${c}" data-filter="complexity">
                    <span>${c}</span>`;
                complexityFilter.appendChild(label);
            });

            // Maturity filter
            const maturityFilter = document.getElementById('maturity-filter');
            ['GA', 'Preview', 'Emerging'].forEach(m => {
                const label = document.createElement('label');
                label.className = 'toggle-item';
                label.innerHTML = `<input type="checkbox" value="${m}" data-filter="maturity">
                    <span>${m}</span>`;
                maturityFilter.appendChild(label);
            });

            // Search input
            document.getElementById('search-patterns').addEventListener('input', (e) => {
                appState.filters.search = e.target.value.toLowerCase();
                renderCatalog();
            });

            // Filter checkboxes
            document.querySelectorAll('[data-filter]').forEach(cb => {
                cb.addEventListener('change', (e) => {
                    const filterType = e.target.dataset.filter;
                    const value = e.target.value;
                    if (e.target.checked) {
                        appState.filters[filterType].push(value);
                    } else {
                        appState.filters[filterType] = appState.filters[filterType].filter(v => v !== value);
                    }
                    renderCatalog();
                });
            });

            // Clear all
            document.getElementById('clear-filters').addEventListener('click', () => {
                appState.filters = { search: '', domains: [], complexity: [], maturity: [] };
                document.querySelectorAll('[data-filter]').forEach(cb => cb.checked = false);
                document.getElementById('search-patterns').value = '';
                renderCatalog();
            });
        }

        function filterPatterns() {
            return PATTERNS.filter(p => {
                // Search filter
                if (appState.filters.search) {
                    const search = appState.filters.search;
                    const matchesSearch = p.name.toLowerCase().includes(search) ||
                        p.summary.toLowerCase().includes(search) ||
                        p.tags.some(t => t.toLowerCase().includes(search));
                    if (!matchesSearch) return false;
                }

                // Domain filter
                if (appState.filters.domains.length > 0) {
                    if (!appState.filters.domains.includes(p.domain)) return false;
                }

                // Complexity filter
                if (appState.filters.complexity.length > 0) {
                    if (!appState.filters.complexity.includes(p.complexity)) return false;
                }

                // Maturity filter
                if (appState.filters.maturity.length > 0) {
                    if (!appState.filters.maturity.includes(p.maturity)) return false;
                }

                return true;
            });
        }

        function renderCatalog() {
            const filtered = filterPatterns();
            const grid = document.getElementById('catalog-grid');
            grid.innerHTML = '';

            if (filtered.length === 0) {
                grid.innerHTML = '<div style="grid-column: 1/-1; text-align: center; padding: 2rem; color: #94A3B8;">No patterns match your filters</div>';
                return;
            }

            filtered.forEach(pattern => {
                const domainColor = DOMAINS[pattern.domain].color;
                const card = document.createElement('div');
                card.className = 'card';
                card.innerHTML = `
                    <div class="card-color-bar ${domainColor}"></div>
                    <div class="card-content">
                        <div class="card-title">${pattern.name}</div>
                        <div class="card-summary">${pattern.summary}</div>
                        <div class="card-badges">
                            <span class="badge complexity ${pattern.complexity.toLowerCase()}">${pattern.complexity}</span>
                            <span class="badge maturity ${pattern.maturity.toLowerCase()}">${pattern.maturity}</span>
                        </div>
                    </div>
                `;
                card.addEventListener('click', () => showPatternModal(pattern));
                grid.appendChild(card);
            });
        }

        function showPatternModal(pattern) {
            appState.currentPattern = pattern;
            const modal = document.getElementById('pattern-modal');

            document.getElementById('modal-pattern-name').textContent = pattern.name;
            document.getElementById('modal-pattern-description').textContent = pattern.description;

            // Pros & Cons
            const prosList = document.getElementById('modal-pattern-pros');
            prosList.innerHTML = '';
            pattern.pros.forEach(pro => {
                const li = document.createElement('li');
                li.className = 'list-item';
                li.textContent = pro;
                prosList.appendChild(li);
            });

            const consList = document.getElementById('modal-pattern-cons');
            consList.innerHTML = '';
            pattern.cons.forEach(con => {
                const li = document.createElement('li');
                li.className = 'list-item';
                li.textContent = con;
                consList.appendChild(li);
            });

            document.getElementById('modal-pattern-usage').textContent = pattern.usageInstructions;
            document.getElementById('modal-pattern-governance').textContent = pattern.governanceConsiderations;

            // Use cases
            const usecaseChips = document.getElementById('modal-pattern-usecases');
            usecaseChips.innerHTML = '';
            pattern.peopleAnalyticsUseCases.forEach(uc => {
                const chip = document.createElement('div');
                chip.className = 'chip';
                chip.textContent = uc;
                usecaseChips.appendChild(chip);
            });

            // Compatible patterns
            const compatChips = document.getElementById('modal-pattern-compatible');
            compatChips.innerHTML = '';
            pattern.compatibleWith.forEach(patternId => {
                const compatPattern = PATTERNS.find(p => p.id === patternId);
                if (compatPattern) {
                    const chip = document.createElement('div');
                    chip.className = 'chip clickable';
                    chip.textContent = compatPattern.name;
                    chip.addEventListener('click', () => {
                        showPatternModal(compatPattern);
                    });
                    compatChips.appendChild(chip);
                }
            });

            // Details
            document.getElementById('modal-pattern-complexity').textContent = pattern.complexity;
            document.getElementById('modal-pattern-maturity').textContent = pattern.maturity;
            document.getElementById('modal-pattern-effort').textContent = pattern.estimatedImplementationEffort;
            document.getElementById('modal-pattern-cost').textContent = pattern.costImplications;

            // References
            const refs = document.getElementById('modal-pattern-references');
            refs.innerHTML = '';
            pattern.referenceLinks.forEach(ref => {
                const p = document.createElement('p');
                p.className = 'list-item';
                p.innerHTML = `<a href="${ref.url}" target="_blank">${ref.label}</a>`;
                refs.appendChild(p);
            });

            // Export pattern button
            document.getElementById('export-pattern-btn').onclick = () => exportPatternHTML(pattern);

            // Close button
            document.querySelector('.close-btn').onclick = () => {
                modal.classList.remove('active');
            };

            modal.addEventListener('click', (e) => {
                if (e.target === modal) modal.classList.remove('active');
            });

            modal.classList.add('active');
        }

        function exportPatternHTML(pattern) {
            const html = `<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>${pattern.name}</title>
    <style>
        body { font-family: Arial, sans-serif; line-height: 1.6; max-width: 900px; margin: 0 auto; padding: 2rem; background: #f5f5f5; }
        .container { background: white; padding: 2rem; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); }
        h1 { color: #0EA5E9; border-bottom: 3px solid #0EA5E9; padding-bottom: 1rem; }
        h2 { color: #0EA5E9; margin-top: 2rem; }
        .section { margin-bottom: 2rem; }
        .pros, .cons { columns: 2; gap: 2rem; }
        .pros li { color: #10B981; font-weight: 500; }
        .cons li { color: #EF4444; font-weight: 500; }
        .governance { background: #fff3cd; border-left: 4px solid #ffc107; padding: 1rem; border-radius: 4px; }
        .details { background: #f9f9f9; padding: 1rem; border-radius: 4px; }
        .details p { margin: 0.5rem 0; }
        .chips { display: flex; flex-wrap: wrap; gap: 0.5rem; }
        .chip { background: #e0e0e0; padding: 0.5rem 1rem; border-radius: 4px; font-size: 0.9rem; }
        a { color: #0EA5E9; }
    </style>
</head>
<body>
    <div class="container">
        <h1>${pattern.name}</h1>
        <p style="color: #666;"><strong>Domain:</strong> ${pattern.domain} | <strong>Complexity:</strong> ${pattern.complexity} | <strong>Maturity:</strong> ${pattern.maturity}</p>

        <div class="section">
            <h2>Overview</h2>
            <p>${pattern.description}</p>
        </div>

        <div class="section">
            <h2>Pros & Cons</h2>
            <div style="columns: 2; gap: 2rem;">
                <div>
                    <h3 style="color: #10B981;">Pros</h3>
                    <ul class="pros">
                        ${pattern.pros.map(p => `<li>${p}</li>`).join('')}
                    </ul>
                </div>
                <div>
                    <h3 style="color: #EF4444;">Cons</h3>
                    <ul class="cons">
                        ${pattern.cons.map(c => `<li>${c}</li>`).join('')}
                    </ul>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>Usage Instructions</h2>
            <pre style="background: #f5f5f5; padding: 1rem; border-radius: 4px; overflow-x: auto;">${pattern.usageInstructions}</pre>
        </div>

        <div class="section">
            <h2>Governance</h2>
            <div class="governance">${pattern.governanceConsiderations}</div>
        </div>

        <div class="section">
            <h2>HR Analytics Use Cases</h2>
            <ul>
                ${pattern.peopleAnalyticsUseCases.map(uc => `<li>${uc}</li>`).join('')}
            </ul>
        </div>

        <div class="section">
            <h2>Details</h2>
            <div class="details">
                <p><strong>Estimated Implementation Effort:</strong> ${pattern.estimatedImplementationEffort}</p>
                <p><strong>Cost Implications:</strong> ${pattern.costImplications}</p>
                <p><strong>Fabric Components:</strong> ${pattern.fabricComponents.join(', ')}</p>
                <p><strong>Tags:</strong> ${pattern.tags.join(', ')}</p>
            </div>
        </div>

        <div class="section">
            <h2>References</h2>
            <ul>
                ${pattern.referenceLinks.map(ref => `<li><a href="${ref.url}">${ref.label}</a></li>`).join('')}
            </ul>
        </div>

        <p style="text-align: center; color: #999; margin-top: 3rem; border-top: 1px solid #ddd; padding-top: 1rem;">
            Exported from Pattern Builder | Generated on ${new Date().toLocaleDateString()}
        </p>
    </div>
</body>
</html>`;
            const blob = new Blob([html], { type: 'text/html' });
            const url = window.URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = `${pattern.id}-pattern.html`;
            a.click();
            window.URL.revokeObjectURL(url);
        }

        // PATTERN BUILDER TAB
        function setupBuilderTab() {
            renderBuilderPatternList();
            updateBuilderStack();
        }

        function renderBuilderPatternList() {
            const list = document.getElementById('builder-pattern-list');
            list.innerHTML = '<h3 style="color: #0EA5E9; margin-bottom: 1rem; font-size: 0.9rem;">Available Patterns</h3>';

            [...new Set(PATTERNS.map(p => p.domain))].forEach(domain => {
                const domainPatterns = PATTERNS.filter(p => p.domain === domain);
                const section = document.createElement('div');
                section.style.marginBottom = '1.5rem';

                let html = `<div class="section-title" style="margin-top: 0;">${domain}</div>`;
                domainPatterns.forEach(pattern => {
                    const isInStack = appState.builderStack.find(p => p.id === pattern.id);
                    const incompatible = checkIncompatible(pattern);

                    let classStr = 'pattern-item';
                    if (incompatible) classStr += ' incompatible';
                    else if (appState.builderStack.length > 0 && pattern.compatibleWith.length > 0) classStr += ' compatible';

                    html += `<div class="${classStr}">
                        <div class="pattern-item-name">${pattern.name}</div>
                        <button class="pattern-item-add" onclick="addPatternToStack('${pattern.id}')" ${isInStack ? 'disabled' : ''}>${isInStack ? ' Added' : '+ Add'}</button>
                    </div>`;
                });

                section.innerHTML = html;
                list.appendChild(section);
            });
        }

        function checkIncompatible(pattern) {
            return appState.builderStack.some(p =>
                pattern.incompatibleWith.includes(p.id) || p.incompatibleWith.includes(pattern.id)
            );
        }

        function addPatternToStack(patternId) {
            const pattern = PATTERNS.find(p => p.id === patternId);
            if (!pattern) return;

            // Check incompatibilities
            for (let stacked of appState.builderStack) {
                if (pattern.incompatibleWith.includes(stacked.id)) {
                    alert(`  Cannot add "${pattern.name}" - incompatible with "${stacked.name}"`);
                    return;
                }
                if (stacked.incompatibleWith.includes(pattern.id)) {
                    alert(`  Cannot add "${pattern.name}" - incompatible with "${stacked.name}"`);
                    return;
                }
            }

            appState.builderStack.push(pattern);
            updateBuilderStack();
            renderBuilderPatternList();
        }

        function removePatternFromStack(patternId) {
            appState.builderStack = appState.builderStack.filter(p => p.id !== patternId);
            updateBuilderStack();
            renderBuilderPatternList();
        }

        function updateBuilderStack() {
            const stack = document.getElementById('builder-stack');
            const empty = document.getElementById('empty-state');

            if (appState.builderStack.length === 0) {
                stack.innerHTML = '';
                empty.style.display = 'block';
            } else {
                empty.style.display = 'none';
                stack.innerHTML = appState.builderStack.map((p, idx) => `
                    <div class="stack-pattern" draggable="true" data-pattern-id="${p.id}">
                        <div class="stack-pattern-info">
                            <div class="stack-pattern-name">${p.name}</div>
                            <div class="stack-pattern-domain">${p.domain}</div>
                        </div>
                        <button class="stack-pattern-remove" onclick="removePatternFromStack('${p.id}')">Remove</button>
                    </div>
                `).join('');
            }

            updateSummaryTable();
            checkWarnings();
        }

        function updateSummaryTable() {
            const count = appState.builderStack.length;
            document.getElementById('metric-count').textContent = count;

            if (count === 0) {
                document.getElementById('metric-complexity').textContent = '-';
                document.getElementById('metric-effort').textContent = '-';
            } else {
                const complexityMap = { 'Low': 1, 'Medium': 2, 'High': 3 };
                const avgComplexity = (appState.builderStack.reduce((sum, p) => sum + complexityMap[p.complexity], 0) / count).toFixed(1);
                const complexityLabels = ['', 'Low', 'Medium', 'High'];
                document.getElementById('metric-complexity').textContent = complexityLabels[Math.round(avgComplexity)];

                // Effort estimation
                const effortMap = { '1-3 days': 3, '1-2 weeks': 7, '2-3 weeks': 14, '2-4 weeks': 21, '2-5 days': 5, '3-4 weeks': 21, '3-5 days': 5, '3-4 days': 4, '4-6 weeks': 30, '6-8 weeks': 42, '8-10 weeks': 60, '8-12 weeks': 80, '10-14 weeks': 84, '12-16 weeks': 112 };
                const totalDays = appState.builderStack.reduce((sum, p) => {
                    const effort = p.estimatedImplementationEffort;
                    return sum + (effortMap[effort] || 7);
                }, 0);
                const weeks = Math.ceil(totalDays / 5);
                document.getElementById('metric-effort').textContent = weeks > 4 ? Math.ceil(weeks/4) + ' months' : weeks + ' weeks';
            }
        }

        function checkWarnings() {
            const warnings = document.getElementById('warnings');
            warnings.innerHTML = '';

            const missing = appState.builderStack.filter(p => p.prerequisites.length > 0).filter(p => {
                return p.prerequisites.some(prereq => !appState.builderStack.find(s => s.id === prereq));
            });

            if (missing.length > 0) {
                warnings.innerHTML = '<strong>  Prerequisites Missing:</strong><br>';
                missing.forEach(p => {
                    const unmet = p.prerequisites.filter(prereq => !appState.builderStack.find(s => s.id === prereq));
                    unmet.forEach(prereq => {
                        const prereqPattern = PATTERNS.find(x => x.id === prereq);
                        warnings.innerHTML += ` "${p.name}" requires "${prereqPattern.name}"<br>`;
                    });
                });
            }
        }

        document.addEventListener('DOMContentLoaded', () => {
            document.getElementById('export-json-btn').onclick = exportStackJSON;
            document.getElementById('generate-brief-btn').onclick = generateBrief;
            document.getElementById('print-btn').onclick = printStack;
        });

        function exportStackJSON() {
            const json = {
                stackName: 'Custom Pattern Stack',
                patterns: appState.builderStack.map(p => ({ id: p.id, name: p.name, domain: p.domain })),
                createdAt: new Date().toISOString(),
                totalPatterns: appState.builderStack.length
            };
            const blob = new Blob([JSON.stringify(json, null, 2)], { type: 'application/json' });
            const url = window.URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = 'pattern-stack.json';
            a.click();
            window.URL.revokeObjectURL(url);
        }

        function generateBrief() {
            let brief = 'PATTERN STACK BRIEF\n';
            brief += '====================\n\n';
            brief += `Total Patterns: ${appState.builderStack.length}\n`;
            brief += `Estimated Timeline: See summary table\n\n`;
            brief += 'PATTERNS:\n';
            brief += appState.builderStack.map((p, i) => `${i+1}. ${p.name} (${p.complexity}, ${p.estimatedImplementationEffort})`).join('\n');
            brief += '\n\nFABRIC COMPONENTS:\n';
            const components = new Set();
            appState.builderStack.forEach(p => p.fabricComponents.forEach(c => components.add(c)));
            Array.from(components).forEach(c => brief += ` ${c}\n`);

            alert(brief);
        }

        function printStack() {
            const printWindow = window.open('', '', 'height=600,width=800');
            let html = `<html><head><title>Pattern Stack</title><style>
                body { font-family: Arial; margin: 2rem; line-height: 1.6; }
                h1 { color: #0EA5E9; }
                h2 { color: #333; margin-top: 1.5rem; }
                .pattern { background: #f9f9f9; padding: 1rem; margin: 0.5rem 0; border-left: 4px solid #0EA5E9; }
                .pattern-name { font-weight: bold; }
                .pattern-meta { font-size: 0.9rem; color: #666; }
                .summary { background: #fff3cd; padding: 1rem; margin: 1rem 0; border-radius: 4px; }
            </style></head><body>`;

            html += '<h1>Pattern Stack Report</h1>';
            html += `<div class="summary">
                <p><strong>Total Patterns:</strong> ${appState.builderStack.length}</p>
                <p><strong>Generated:</strong> ${new Date().toLocaleDateString()}</p>
            </div>`;

            html += '<h2>Patterns</h2>';
            appState.builderStack.forEach(p => {
                html += `<div class="pattern">
                    <div class="pattern-name">${p.name}</div>
                    <div class="pattern-meta">Domain: ${p.domain} | Complexity: ${p.complexity} | Effort: ${p.estimatedImplementationEffort}</div>
                    <div style="margin-top: 0.5rem; color: #555;">${p.summary}</div>
                </div>`;
            });

            html += '</body></html>';
            printWindow.document.write(html);
            printWindow.document.close();
            setTimeout(() => printWindow.print(), 250);
        }

        // BLUEPRINTS TAB
        function setupBlueprintsTab() {
            renderBlueprintsGrid();
        }

        function renderBlueprintsGrid() {
            const grid = document.getElementById('blueprints-grid');
            grid.innerHTML = BLUEPRINTS.map(bp => `
                <div class="blueprint-card" onclick="showBlueprintDetail('${bp.id}')">
                    <div class="blueprint-title">${bp.name}</div>
                    <div class="blueprint-desc">${bp.description}</div>
                    <div class="blueprint-meta">
                        <div class="meta-item">
                            <span class="meta-label">Audience:</span>
                            <span class="meta-value">${bp.audience.split(',')[0].trim()}</span>
                        </div>
                        <div class="meta-item">
                            <span class="meta-label">Patterns:</span>
                            <span class="meta-value">${bp.patterns.length}</span>
                        </div>
                        <div class="meta-item">
                            <span class="meta-label">Effort:</span>
                            <span class="meta-value">${bp.estimatedTotalEffort}</span>
                        </div>
                    </div>
                </div>
            `).join('');
        }

        function showBlueprintDetail(blueprintId) {
            const blueprint = BLUEPRINTS.find(b => b.id === blueprintId);
            if (!blueprint) return;

            appState.currentBlueprint = blueprint;

            const grid = document.getElementById('blueprints-grid');
            const view = document.getElementById('blueprint-view');
            const viewContent = document.getElementById('blueprint-view-content');

            let html = `
                <button class="blueprint-back" onclick="closeBlueprintDetail()"> Back to Blueprints</button>
                <h2 style="color: #0EA5E9; margin-bottom: 1rem;">${blueprint.name}</h2>
                <p style="color: #E2E8F0; margin-bottom: 2rem; font-size: 0.95rem;">${blueprint.description}</p>

                <div class="modal-section">
                    <div class="modal-section-title">Pattern Flow</div>
                    <p style="color: #94A3B8; margin-bottom: 1rem; font-size: 0.9rem;">Recommended pattern sequence for this blueprint:</p>
            `;

            blueprint.patternFlow.forEach((flow, idx) => {
                html += `
                    <div class="flow-step" onclick="openPatternFromFlow('${flow.from}')">
                        <div class="flow-step-pattern">${flow.from}</div>
                        <div class="flow-step-desc"> ${flow.description}</div>
                    </div>
                `;
            });

            html += `
                </div>

                <div class="modal-section">
                    <div class="modal-section-title">Notes & Customization</div>
                    <div contenteditable="true" class="editable-notes" id="blueprint-notes">${blueprint.editableNotes}</div>
                </div>

                <div class="modal-section">
                    <div class="modal-section-title">Key Governance Requirements</div>
                    <ul style="color: #E2E8F0;">
                        ${blueprint.keyGovernanceRequirements.map(req => `<li style="margin-bottom: 0.75rem; line-height: 1.6;">${req}</li>`).join('')}
                    </ul>
                </div>

                <div class="modal-buttons" style="margin-top: 2rem;">
                    <button class="btn primary" onclick="loadBlueprintToBuilder('${blueprint.id}')">Load into Pattern Builder</button>
                    <button class="btn" onclick="exportBlueprintHTML('${blueprint.id}')">Export Blueprint</button>
                </div>
            `;

            viewContent.innerHTML = html;
            grid.style.display = 'none';
            view.classList.add('active');
        }

        function closeBlueprintDetail() {
            const grid = document.getElementById('blueprints-grid');
            const view = document.getElementById('blueprint-view');
            grid.style.display = 'grid';
            view.classList.remove('active');
            appState.currentBlueprint = null;
        }

        function openPatternFromFlow(patternId) {
            const pattern = PATTERNS.find(p => p.id === patternId);
            if (pattern) showPatternModal(pattern);
        }

        function loadBlueprintToBuilder(blueprintId) {
            const blueprint = BLUEPRINTS.find(b => b.id === blueprintId);
            if (!blueprint) return;

            appState.builderStack = [];
            blueprint.patterns.forEach(patternId => {
                const pattern = PATTERNS.find(p => p.id === patternId);
                if (pattern) appState.builderStack.push(pattern);
            });

            switchTab('builder');
            setupBuilderTab();
        }

        function exportBlueprintHTML(blueprintId) {
            const blueprint = BLUEPRINTS.find(b => b.id === blueprintId);
            const notesText = document.getElementById('blueprint-notes')?.textContent || blueprint.editableNotes;

            const html = `<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>${blueprint.name}</title>
    <style>
        body { font-family: Arial, sans-serif; line-height: 1.6; max-width: 900px; margin: 0 auto; padding: 2rem; background: #f5f5f5; }
        .container { background: white; padding: 2rem; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); }
        h1 { color: #0EA5E9; border-bottom: 3px solid #0EA5E9; padding-bottom: 1rem; }
        h2 { color: #0EA5E9; margin-top: 2rem; }
        .section { margin-bottom: 2rem; }
        .flow { background: #f9f9f9; padding: 1rem; border-left: 4px solid #0EA5E9; margin: 0.75rem 0; border-radius: 4px; }
        .governance { background: #fff3cd; border-left: 4px solid #ffc107; padding: 1rem; border-radius: 4px; }
        .notes { background: #e3f2fd; border-left: 4px solid #2196f3; padding: 1rem; border-radius: 4px; white-space: pre-wrap; }
        a { color: #0EA5E9; }
    </style>
</head>
<body>
    <div class="container">
        <h1>${blueprint.name}</h1>
        <p><strong>Audience:</strong> ${blueprint.audience}</p>
        <p><strong>Total Patterns:</strong> ${blueprint.patterns.length} | <strong>Estimated Effort:</strong> ${blueprint.estimatedTotalEffort}</p>

        <div class="section">
            <h2>Overview</h2>
            <p>${blueprint.description}</p>
        </div>

        <div class="section">
            <h2>Pattern Flow</h2>
            <p>Recommended sequence:</p>
            ${blueprint.patternFlow.map(flow => `<div class="flow"><strong>${flow.from}</strong><br/>${flow.description}</div>`).join('')}
        </div>

        <div class="section">
            <h2>Key Governance Requirements</h2>
            <ul>
                ${blueprint.keyGovernanceRequirements.map(req => `<li>${req}</li>`).join('')}
            </ul>
        </div>

        <div class="section">
            <h2>Notes & Customization</h2>
            <div class="notes">${notesText}</div>
        </div>

        <p style="text-align: center; color: #999; margin-top: 3rem; border-top: 1px solid #ddd; padding-top: 1rem;">
            Exported from Pattern Builder | Generated on ${new Date().toLocaleDateString()}
        </p>
    </div>
</body>
</html>`;

            const blob = new Blob([html], { type: 'text/html' });
            const url = window.URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = `${blueprint.id}-blueprint.html`;
            a.click();
            window.URL.revokeObjectURL(url);
        }
    </script>
</body>
</html>
