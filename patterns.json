{
  "patterns": [
    {
      "id": "medallion-architecture",
      "name": "Medallion Architecture (Bronze-Silver-Gold)",
      "domain": "Data Organization and Structuring",
      "domainId": 1,
      "category": "Lakehouse Architecture",
      "summary": "Implements a three-layer medallion architecture (Bronze, Silver, Gold) for progressive data refinement in OneLake.",
      "description": "The medallion architecture separates raw data ingestion, cleansed data, and business-ready analytics data into distinct layers. For HR analytics, employee data moves through Bronze (raw), Silver (standardized), and Gold (business-ready) layers.",
      "fabricComponents": [
        "Lakehouse",
        "OneLake",
        "Spark Notebooks",
        "Delta Lake"
      ],
      "pros": [
        "Provides clear separation of concerns with distinct data quality boundaries.",
        "Enables independent scaling and optimization of each layer.",
        "Facilitates governance by creating controlled access points."
      ],
      "cons": [
        "Introduces operational complexity with three layers.",
        "Can increase storage costs if not properly optimized.",
        "Requires upfront investment in data modeling."
      ],
      "usageInstructions": "1. Create three folders: Bronze, Silver, Gold. 2. Land raw data into Bronze. 3. Build transformations in Silver. 4. Create final tables in Gold. 5. Apply labels progressively. 6. Establish refresh schedules.",
      "governanceConsiderations": "Implement RLS at Gold layer and restrict Bronze/Silver access to engineers. Apply sensitivity labels to personal data. Maintain transformation logs for audits.",
      "peopleAnalyticsUseCases": [
        "Employee master repository moving through all layers.",
        "Payroll analytics data mart with restricted raw salary details.",
        "Organizational analytics foundation with normalized hierarchies."
      ],
      "complexity": "High",
      "maturity": "GA",
      "compatibleWith": [
        "delta-lake-partitioning",
        "onelake-shortcuts",
        "spark-notebook-etl",
        "dataflow-gen2"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "architecture",
        "data-organization",
        "lakehouse"
      ],
      "referenceLinks": [
        {
          "label": "Medallion Architecture",
          "url": "https://docs.microsoft.com/en-us/fabric/onelake/medallion-lakehouse-architecture"
        }
      ],
      "estimatedImplementationEffort": "3-4 weeks",
      "costImplications": "Storage scales with volume; optimize with VACUUM"
    },
    {
      "id": "delta-lake-partitioning",
      "name": "Delta Lake Partitioning Strategy",
      "domain": "Data Organization and Structuring",
      "domainId": 1,
      "category": "Optimization",
      "summary": "Partitions Delta tables by business dimensions to optimize query performance and reduce scan costs.",
      "description": "Partitioning divides large tables into segments based on column values like date or department, enabling Spark to skip irrelevant partitions during queries.",
      "fabricComponents": [
        "Lakehouse",
        "Delta Lake",
        "Spark Notebooks"
      ],
      "pros": [
        "Reduces query execution time through predicate pushdown.",
        "Reduces compute and storage costs by avoiding full scans.",
        "Simplifies data lifecycle management for old partitions."
      ],
      "cons": [
        "Poorly chosen columns degrade performance.",
        "Over-partitioning requires frequent compaction.",
        "Adds complexity to pipeline logic."
      ],
      "usageInstructions": "1. Analyze query patterns. 2. Select 1-3 partition columns. 3. Create table with PARTITIONED BY. 4. Ingest data with partition columns. 5. Run ANALYZE TABLE COMPUTE STATISTICS. 6. Monitor and optimize quarterly.",
      "governanceConsiderations": "Align partition columns with RLS policies. Document strategy in data catalog. Monitor partition drift. Ensure archived partitions follow retention policies.",
      "peopleAnalyticsUseCases": [
        "Payroll trend analysis across decade with fast access to recent periods.",
        "Attendance pattern retrieval without scanning all records.",
        "Time-travel org hierarchy analysis."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "medallion-architecture",
        "lakehouse-warehouse-selection",
        "spark-notebook-etl"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "medallion-architecture"
      ],
      "tags": [
        "optimization",
        "performance",
        "delta-lake"
      ],
      "referenceLinks": [
        {
          "label": "Delta Lake Partitioning",
          "url": "https://docs.microsoft.com/en-us/fabric/data-engineering/lakehouse-shortcuts"
        }
      ],
      "estimatedImplementationEffort": "1-2 weeks",
      "costImplications": "Reduces costs 50-80%; storage overhead minimal"
    },
    {
      "id": "lakehouse-warehouse-selection",
      "name": "Lakehouse vs Warehouse Selection",
      "domain": "Data Organization and Structuring",
      "domainId": 1,
      "category": "Decision Framework",
      "summary": "Decision framework for choosing between Lakehouse and Warehouse based on workload characteristics.",
      "description": "Lakehouse works well for unstructured data and ML; Warehouse for structured relational analytics. Many use both in parallel.",
      "fabricComponents": [
        "Lakehouse",
        "Warehouse",
        "OneLake",
        "Spark Notebooks"
      ],
      "pros": [
        "Optimal tool selection for different workloads.",
        "Lakehouse provides flexibility; Warehouse ensures consistency.",
        "Supports incremental adoption and migration."
      ],
      "cons": [
        "Operating both increases operational overhead.",
        "Performance strategies differ significantly.",
        "Teams must understand distinct capabilities."
      ],
      "usageInstructions": "1. Assess workload type. 2. Structured HR reporting \u2192 Warehouse. 3. Exploratory analysis \u2192 Lakehouse. 4. Semi-structured \u2192 Lakehouse. 5. If both needed: Lakehouse Bronze/Silver, Warehouse for reporting. 6. Use shortcuts.",
      "governanceConsiderations": "Warehouse provides stricter governance through schema enforcement. Restrict Warehouse to certified analytics. Use Lakehouse for non-sensitive exploration.",
      "peopleAnalyticsUseCases": [
        "Operational HR dashboards in Warehouse.",
        "Survey analysis and churn modeling in Lakehouse.",
        "ML pipelines in Lakehouse, results in Warehouse."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "medallion-architecture",
        "direct-lake-semantic-model"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "architecture",
        "storage-selection"
      ],
      "referenceLinks": [
        {
          "label": "Lakehouse Overview",
          "url": "https://docs.microsoft.com/en-us/fabric/data-engineering/lakehouse-overview"
        }
      ],
      "estimatedImplementationEffort": "1 week assessment",
      "costImplications": "Lakehouse compute scales with transforms; Warehouse has fixed DWU pricing"
    },
    {
      "id": "onelake-shortcuts",
      "name": "OneLake Shortcuts for Data Sharing",
      "domain": "Data Organization and Structuring",
      "domainId": 1,
      "category": "Data Integration",
      "summary": "Uses OneLake shortcuts as virtual references to data elsewhere without copying.",
      "description": "Shortcuts point to data in other Lakehouses, Warehouses, or external storage. Finance maintains employee master; HR and Recruiting create shortcuts to it.",
      "fabricComponents": [
        "OneLake",
        "Lakehouse",
        "Warehouse",
        "Shortcuts"
      ],
      "pros": [
        "Eliminates duplication and maintains single source of truth.",
        "Zero-copy reduces costs; updates visible immediately.",
        "Simplifies cross-team collaboration."
      ],
      "cons": [
        "Cross-workspace latency can degrade performance.",
        "Shortcuts obscure ownership and governance.",
        "Lineage becomes harder to debug."
      ],
      "usageInstructions": "1. Identify source of truth tables. 2. Create Lakehouse in consumer team. 3. Right-click folder > New shortcut. 4. Select source table. 5. Query like normal tables. 6. Monitor performance.",
      "governanceConsiderations": "Shortcuts must point to governed tables. Establish data contracts for schema stability. Document in data catalog. Apply workspace permissions. Use for read-only reference data.",
      "peopleAnalyticsUseCases": [
        "Finance employee master accessed via shortcuts by HR and Recruiting.",
        "Shared org hierarchy referenced across teams.",
        "Reducing storage by shortcutting payroll data."
      ],
      "complexity": "Low",
      "maturity": "GA",
      "compatibleWith": [
        "medallion-architecture",
        "lakehouse-warehouse-selection"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "data-sharing",
        "shortcuts",
        "zero-copy"
      ],
      "referenceLinks": [
        {
          "label": "OneLake Shortcuts",
          "url": "https://docs.microsoft.com/en-us/fabric/onelake/onelake-shortcuts"
        }
      ],
      "estimatedImplementationEffort": "2-3 days",
      "costImplications": "Zero-copy saves significantly vs replication"
    },
    {
      "id": "direct-lake-semantic-model",
      "name": "Direct Lake Semantic Model",
      "domain": "Data Organization and Structuring",
      "domainId": 1,
      "category": "Semantic Layer Design",
      "summary": "Creates semantic models that directly reference OneLake Delta tables in Direct Lake mode for real-time analytics.",
      "description": "Direct Lake bypasses VertiPaq import, providing freshness of DirectQuery with import speed. Gold-layer tables feed Power BI without duplication.",
      "fabricComponents": [
        "Semantic Model",
        "Power BI",
        "OneLake",
        "Delta Lake",
        "Lakehouse"
      ],
      "pros": [
        "Real-time data access without import overhead.",
        "Eliminates storage duplication.",
        "Combines import performance with DirectQuery freshness."
      ],
      "cons": [
        "Requires well-optimized Delta tables.",
        "Not all Power BI transformations supported.",
        "Optimization less transparent than import mode."
      ],
      "usageInstructions": "1. Ensure Gold tables are optimized. 2. Create semantic model. 3. Select Direct Lake mode. 4. Browse and select Gold tables. 5. Create relationships. 6. Build reports. 7. Monitor performance.",
      "governanceConsiderations": "Direct Lake exposes lakehouse structure; govern before creating models. Control who modifies underlying tables. Apply sensitivity labels. Document contracts. Prevent accidental deletions.",
      "peopleAnalyticsUseCases": [
        "Real-time HR dashboards from Gold tables.",
        "Live payroll cost dashboards.",
        "Quick BI iteration using Direct Lake."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "medallion-architecture",
        "lakehouse-warehouse-selection",
        "delta-lake-partitioning"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "medallion-architecture"
      ],
      "tags": [
        "semantic-model",
        "direct-lake",
        "power-bi"
      ],
      "referenceLinks": [
        {
          "label": "Direct Lake",
          "url": "https://docs.microsoft.com/en-us/power-bi/enterprise/directlake-overview"
        }
      ],
      "estimatedImplementationEffort": "1-2 weeks",
      "costImplications": "Reduces import compute; scales with query volume"
    },
    {
      "id": "hub-spoke-workspace",
      "name": "Hub-and-Spoke Workspace Design",
      "domain": "Data Organization and Structuring",
      "domainId": 1,
      "category": "Workspace Architecture",
      "summary": "Hub workspace contains shared reference data; spoke workspaces (HR, Recruiting, Finance) build domain-specific analytics.",
      "description": "Central Hub maintains employee master, org structure, cost centers. Spokes use shortcuts to reference Hub and build domain analytics.",
      "fabricComponents": [
        "Workspaces",
        "OneLake",
        "Lakehouse",
        "Shortcuts"
      ],
      "pros": [
        "Centralizes reference data governance.",
        "Enables autonomous spoke teams.",
        "Simplifies permission management."
      ],
      "cons": [
        "Cross-workspace dependencies add complexity.",
        "Hub requires dedicated team.",
        "Network latency for shortcuts."
      ],
      "usageInstructions": "1. Create Hub workspace. 2. Populate with reference tables. 3. Create Spoke workspaces. 4. Spoke teams create shortcuts to Hub. 5. Spoke builds own layers. 6. Establish data review board. 7. Document dependencies.",
      "governanceConsiderations": "Hub requires clear ownership and change management. Strict Hub permissions: stewards only. Enforce RLS at Spoke level. Document contracts. Monitor dependencies.",
      "peopleAnalyticsUseCases": [
        "Central HR Hub with Recruiting, Compensation spokes.",
        "Finance Hub with HR spoke for cost allocation.",
        "Executive Hub with HR spoke for talent alignment."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "medallion-architecture",
        "onelake-shortcuts"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "medallion-architecture"
      ],
      "tags": [
        "workspace-design",
        "hub-spoke"
      ],
      "referenceLinks": [
        {
          "label": "Workspace Management",
          "url": "https://docs.microsoft.com/en-us/fabric/admin/workspaces"
        }
      ],
      "estimatedImplementationEffort": "3-4 weeks",
      "costImplications": "Hub shared across spokes; separate Spoke costs"
    },
    {
      "id": "spark-notebook-etl",
      "name": "Spark Notebook ETL Pipelines",
      "domain": "Data Transformation and Processing",
      "domainId": 2,
      "category": "ETL Development",
      "summary": "PySpark notebooks for complex Bronze-to-Silver and Silver-to-Gold transformations with full programming power.",
      "description": "Notebooks provide flexibility for complex logic, iterative development, and large-scale transformations. Payroll notebooks standardize fields, fill missing dates, calculate tenure.",
      "fabricComponents": [
        "Notebook",
        "Spark",
        "PySpark",
        "Lakehouse",
        "Delta Lake"
      ],
      "pros": [
        "Ultimate flexibility for complex business logic.",
        "Cell-level execution enables step-by-step debugging.",
        "Automatic scaling to large datasets."
      ],
      "cons": [
        "Requires Python/Scala expertise.",
        "No visual lineage or profiling.",
        "Harder to govern."
      ],
      "usageInstructions": "1. Create notebook. 2. Read Bronze: df = spark.read.table(). 3. Transform (standardize, validate, enrich). 4. Write to Silver. 5. Test on sample data. 6. Schedule as job. 7. Add error handling.",
      "governanceConsiderations": "Implement code review for notebooks. Version control via Git. Restrict modify permissions. Log all transformations. Document assumptions.",
      "peopleAnalyticsUseCases": [
        "Complex payroll ETL with reconciliation.",
        "Employee movement tracking via snapshots.",
        "Unified talent dataset from multiple sources."
      ],
      "complexity": "High",
      "maturity": "GA",
      "compatibleWith": [
        "medallion-architecture",
        "delta-lake-partitioning",
        "incremental-watermark"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "medallion-architecture"
      ],
      "tags": [
        "etl",
        "spark",
        "pyspark",
        "transformation"
      ],
      "referenceLinks": [
        {
          "label": "Spark Notebooks",
          "url": "https://docs.microsoft.com/en-us/fabric/data-engineering/notebook-overview"
        }
      ],
      "estimatedImplementationEffort": "2-5 days per pipeline",
      "costImplications": "Scales with complexity and volume"
    },
    {
      "id": "dataflow-gen2",
      "name": "Dataflow Gen2 Low-Code Transformations",
      "domain": "Data Transformation and Processing",
      "domainId": 2,
      "category": "ETL Development",
      "summary": "Power Query Online visual ETL for simple-to-moderate transformations without coding.",
      "description": "Graphical UI for filter, merge, group, enrich. Employee master ingest, remove duplicates, rename columns, output to Lakehouse.",
      "fabricComponents": [
        "Dataflow Gen2",
        "Power Query Online",
        "Lakehouse",
        "Power BI"
      ],
      "pros": [
        "Reduces time-to-delivery for standard transforms.",
        "Built-in data profiling and quality checks.",
        "Native Power BI integration."
      ],
      "cons": [
        "Limited to moderately complex logic.",
        "Performance degrades on large datasets.",
        "Harder to version control and CI/CD."
      ],
      "usageInstructions": "1. Create Dataflow Gen2. 2. Connect to source. 3. Apply transforms: Remove Duplicates, Filter, Rename. 4. Group/summarize if needed. 5. Merge with references. 6. Preview and validate. 7. Configure destination. 8. Schedule refresh.",
      "governanceConsiderations": "Document formulas clearly. Establish change approval. Monitor refresh times. Use as preferred entry for business users. Implement labels on outputs.",
      "peopleAnalyticsUseCases": [
        "Weekly employee snapshot: ingest, dedup, filter, output.",
        "Department cost center mapping and aggregation.",
        "Applicant data prep for recruiting analytics."
      ],
      "complexity": "Low",
      "maturity": "GA",
      "compatibleWith": [
        "medallion-architecture",
        "data-quality-validation"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "etl",
        "low-code",
        "power-query"
      ],
      "referenceLinks": [
        {
          "label": "Dataflow Gen2",
          "url": "https://docs.microsoft.com/en-us/fabric/data-factory/create-first-dataflow-gen2"
        }
      ],
      "estimatedImplementationEffort": "1-3 days",
      "costImplications": "Lower compute for simple transforms; scales with frequency"
    },
    {
      "id": "incremental-watermark",
      "name": "Incremental Loading with Watermarks",
      "domain": "Data Transformation and Processing",
      "domainId": 2,
      "category": "ETL Efficiency",
      "summary": "Captures only new/modified records since last run using watermark columns to reduce refresh time.",
      "description": "Store max last_modified_date in control table; query only records > previous watermark. Nightly refresh hours reduces to minutes.",
      "fabricComponents": [
        "Spark Notebook",
        "Lakehouse",
        "Delta Lake",
        "Control Tables"
      ],
      "pros": [
        "Reduces refresh from hours to minutes.",
        "Scales elegantly with constant change volume.",
        "Enables real-time/near-real-time analytics."
      ],
      "cons": [
        "Requires reliable source change tracking.",
        "Complex to handle late-arriving data.",
        "Difficult recovery from failures."
      ],
      "usageInstructions": "1. Create control table for watermarks. 2. Read previous watermark. 3. Query source with filter. 4. Merge into Silver. 5. Update watermark. 6. Monitor for late data.",
      "governanceConsiderations": "Govern source change-tracking columns. Document watermark logic. Implement monitoring. Archive watermarks. Establish reset procedures.",
      "peopleAnalyticsUseCases": [
        "Employee transactions with daily incremental loads.",
        "Real-time headcount dashboard.",
        "Employee history capturing changes."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "medallion-architecture",
        "spark-notebook-etl",
        "scd-type-2"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "spark-notebook-etl",
        "medallion-architecture"
      ],
      "tags": [
        "incremental",
        "watermark",
        "etl"
      ],
      "referenceLinks": [
        {
          "label": "Delta Merge",
          "url": "https://docs.microsoft.com/en-us/fabric/data-engineering/delta-optimization"
        }
      ],
      "estimatedImplementationEffort": "3-5 days",
      "costImplications": "Reduces 80-90% vs full reload"
    },
    {
      "id": "scd-type-2",
      "name": "Slowly Changing Dimensions Type 2",
      "domain": "Data Transformation and Processing",
      "domainId": 2,
      "category": "Dimension Management",
      "summary": "Maintains historical versions with validity dates enabling time-travel analysis of attribute changes.",
      "description": "New rows created for changes with effective_date, end_date. Promotion creates new employee row; old row marked ended. Enables salary progression analysis.",
      "fabricComponents": [
        "Spark Notebook",
        "Delta Lake",
        "Lakehouse"
      ],
      "pros": [
        "Enables temporal analysis and past-state reconstruction.",
        "Maintains historical context for metrics.",
        "Supports full audit trail."
      ],
      "cons": [
        "Storage overhead from historical versions.",
        "Complex merge logic required.",
        "Analytics queries become more complex."
      ],
      "usageInstructions": "1. Design dimension with surrogate key, effective_date, end_date, current_flag. 2. Initial load. 3. On update: END previous, INSERT new. 4. Merge into dimension. 5. Validate no overlaps.",
      "governanceConsiderations": "Document merge logic thoroughly. Implement validation checks. Archive old dimensions. Use selectively. Establish retention policies.",
      "peopleAnalyticsUseCases": [
        "Career progression analysis with salary growth.",
        "Org change analysis with historical reporting lines.",
        "Compensation cohort analysis."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "medallion-architecture",
        "spark-notebook-etl",
        "incremental-watermark"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "spark-notebook-etl",
        "medallion-architecture"
      ],
      "tags": [
        "scd",
        "slowly-changing",
        "temporal"
      ],
      "referenceLinks": [
        {
          "label": "SCD Patterns",
          "url": "https://en.wikipedia.org/wiki/Slowly_changing_dimension"
        }
      ],
      "estimatedImplementationEffort": "3-4 days",
      "costImplications": "Storage 20-50% higher for history"
    },
    {
      "id": "cdc-change-capture",
      "name": "Change Data Capture (CDC) for Auditing",
      "domain": "Data Transformation and Processing",
      "domainId": 2,
      "category": "Data Auditing",
      "summary": "Records before/after values of all modifications with user and timestamp for compliance audit trails.",
      "description": "Salary update triggers record logging old/new values. Audit table captures INSERT/UPDATE/DELETE with metadata. Supports investigations.",
      "fabricComponents": [
        "Warehouse",
        "Spark Notebook",
        "Audit Tables"
      ],
      "pros": [
        "Complete audit trail for compliance.",
        "Enables real-time alerting on sensitive changes.",
        "Supports efficient incremental processing."
      ],
      "cons": [
        "Increases write overhead and latency.",
        "Audit tables grow very large.",
        "Edge cases require careful handling."
      ],
      "usageInstructions": "1. Create audit table. 2. Trigger on updates records changes. 3. Archive records >7 years. 4. Expose to compliance via Power BI. 5. Set up alerts.",
      "governanceConsiderations": "Restrict to compliance/audit only. Establish retention policies. Document audited fields. Archive to cold storage. Use for investigation support.",
      "peopleAnalyticsUseCases": [
        "Compliance auditing of salary changes.",
        "Detecting unauthorized modifications.",
        "Payroll reconciliation tracking."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "sensitivity-labels",
        "scd-type-2",
        "data-quality-validation"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "cdc",
        "audit",
        "compliance"
      ],
      "referenceLinks": [
        {
          "label": "CDC Patterns",
          "url": "https://docs.microsoft.com/en-us/fabric/data-warehouse/change-data-capture"
        }
      ],
      "estimatedImplementationEffort": "2-3 days",
      "costImplications": "Storage 10-30% overhead; consider tiered storage"
    },
    {
      "id": "dbt-integration",
      "name": "dbt Integration for Data Transformation",
      "domain": "Data Transformation and Processing",
      "domainId": 2,
      "category": "ETL Development",
      "summary": "SQL-based dbt models with version control, testing, documentation enabling software engineering discipline.",
      "description": "dbt projects organize SQL transformations with tests, lineage, documentation. Junior analysts contribute familiar SQL; dbt handles plumbing.",
      "fabricComponents": [
        "Warehouse",
        "Lakehouse",
        "dbt",
        "Git",
        "Spark SQL"
      ],
      "pros": [
        "Brings software engineering to analytics.",
        "Modular SQL enables junior analyst contribution.",
        "Auto-generates lineage documentation."
      ],
      "cons": [
        "Requires dbt and YAML knowledge.",
        "Performance tuning less transparent.",
        "Limited to SQL transformations."
      ],
      "usageInstructions": "1. Create dbt project. 2. Configure Warehouse target. 3. Write SQL models. 4. Define tests. 5. Run dbt run. 6. Commit to git. 7. Configure CI/CD.",
      "governanceConsiderations": "Structure by medallion layers. Implement code review. Use dbt tests for quality. Document business context. Manage breaking changes carefully.",
      "peopleAnalyticsUseCases": [
        "HR transformation logic with employee, role, compensation models.",
        "Collaborative analysis pipeline with code review.",
        "Rapid metrics iteration."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "medallion-architecture",
        "data-quality-validation",
        "spark-notebook-etl"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "medallion-architecture"
      ],
      "tags": [
        "dbt",
        "sql",
        "transformation",
        "version-control"
      ],
      "referenceLinks": [
        {
          "label": "dbt Docs",
          "url": "https://docs.getdbt.com/"
        }
      ],
      "estimatedImplementationEffort": "1-2 weeks",
      "costImplications": "Same as SQL compute; reduced overhead from reuse"
    },
    {
      "id": "data-quality-validation",
      "name": "Data Quality Validation Framework",
      "domain": "Data Transformation and Processing",
      "domainId": 2,
      "category": "Quality Assurance",
      "summary": "Automated checks after ETL for anomalies, nulls, schema violations, business rule violations preventing bad data propagation.",
      "description": "Validation tests NULL values in required fields, validates salary ranges, confirms dept codes match reference, detects duplicates. Failures pause processes.",
      "fabricComponents": [
        "Spark Notebook",
        "dbt",
        "Great Expectations",
        "Lakehouse"
      ],
      "pros": [
        "Catches issues at source before propagation.",
        "Builds analyst trust in data.",
        "Enables quick root-cause analysis."
      ],
      "cons": [
        "Requires upfront effort to define rules.",
        "Can over-reject valid data.",
        "Adds latency to pipelines."
      ],
      "usageInstructions": "1. Define rules: required_fields, ranges, valid values. 2. After load, run validation. 3. Check violations vs threshold. 4. Alert if exceeded. 5. Implement with dbt tests or Great Expectations.",
      "governanceConsiderations": "Document rules with justification. Version in code. Archive results for audits. Use as data contract documentation. Establish escalation procedures.",
      "peopleAnalyticsUseCases": [
        "Payroll validation before Finance handoff.",
        "Org hierarchy consistency checking.",
        "HRIS reconciliation."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "medallion-architecture",
        "spark-notebook-etl",
        "dbt-integration"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "medallion-architecture"
      ],
      "tags": [
        "data-quality",
        "validation",
        "testing"
      ],
      "referenceLinks": [
        {
          "label": "dbt Tests",
          "url": "https://docs.getdbt.com/docs/building-a-dbt-project/tests"
        }
      ],
      "estimatedImplementationEffort": "2-3 weeks",
      "costImplications": "Validation is 5-10% of ETL compute"
    },
    {
      "id": "purview-data-map",
      "name": "Microsoft Purview Data Map",
      "domain": "Data Governance and Security",
      "domainId": 3,
      "category": "Data Catalog",
      "summary": "Creates comprehensive data catalog in Purview mapping all assets, lineage, classifications, and sensitive data locations.",
      "description": "Purview scans Fabric workspace discovering tables, columns, lineage. Classifications mark PII (SSN, salary). Stewards govern assets and ownership.",
      "fabricComponents": [
        "Microsoft Purview",
        "Fabric",
        "Lakehouse",
        "Warehouse"
      ],
      "pros": [
        "Provides complete asset inventory and lineage.",
        "Automates sensitive data discovery.",
        "Enables steward governance at scale."
      ],
      "cons": [
        "Requires significant setup and configuration.",
        "Scans can be resource-intensive.",
        "Learning curve for Purview concepts."
      ],
      "usageInstructions": "1. Connect Fabric to Purview. 2. Configure scans. 3. Run scans. 4. Review classifications. 5. Assign stewards. 6. Create business glossary. 7. Document assets.",
      "governanceConsiderations": "Use Purview as source of truth for data governance. Integrate classifications with sensitivity labels. Assign stewards for critical assets. Track data quality metrics. Update regularly.",
      "peopleAnalyticsUseCases": [
        "Discover all HR data assets across organization.",
        "Track payroll data lineage from source to reports.",
        "Identify PII exposure and mitigation paths."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "sensitivity-labels",
        "row-level-security",
        "cdc-change-capture"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "data-catalog",
        "governance",
        "purview",
        "lineage"
      ],
      "referenceLinks": [
        {
          "label": "Microsoft Purview",
          "url": "https://docs.microsoft.com/en-us/purview/"
        }
      ],
      "estimatedImplementationEffort": "3-4 weeks",
      "costImplications": "Purview licensing plus scan compute"
    },
    {
      "id": "sensitivity-labels",
      "name": "Sensitivity Labels for Data Classification",
      "domain": "Data Governance and Security",
      "domainId": 3,
      "category": "Data Protection",
      "summary": "Applies sensitivity labels (Highly Confidential, Confidential, Internal) to tables and columns triggering data masking and access controls.",
      "description": "Label SSN and salary columns as Highly Confidential; Purview enforces masking and restricts query results. Labels cascade to reports and exports.",
      "fabricComponents": [
        "Sensitivity Labels",
        "Purview",
        "Lakehouse",
        "Warehouse",
        "Power BI"
      ],
      "pros": [
        "Automated protection based on content classification.",
        "Enforcement applies across Fabric and Power BI.",
        "Cascades to exports and reports."
      ],
      "cons": [
        "Initial labeling requires effort.",
        "Label enforcement can break some use cases.",
        "Requires governance process."
      ],
      "usageInstructions": "1. Define label taxonomy. 2. Create labels in Security & Compliance. 3. Apply to tables/columns. 4. Configure label policies. 5. Test masking. 6. Monitor usage.",
      "governanceConsiderations": "Establish consistent label taxonomy. Assign classification responsibility. Monitor label compliance. Update as data changes. Train users.",
      "peopleAnalyticsUseCases": [
        "Mark SSN, salary, health data as Highly Confidential.",
        "Restrict export of labeled data.",
        "Auto-mask salary in development environment."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "purview-data-map",
        "row-level-security",
        "dynamic-data-masking"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "sensitivity-labels",
        "classification",
        "pii",
        "protection"
      ],
      "referenceLinks": [
        {
          "label": "Sensitivity Labels",
          "url": "https://docs.microsoft.com/en-us/purview/sensitivity-labels"
        }
      ],
      "estimatedImplementationEffort": "2-3 weeks",
      "costImplications": "Minimal overhead; licensing included in Purview"
    },
    {
      "id": "row-level-security",
      "name": "Row-Level Security (RLS) at Gold Layer",
      "domain": "Data Governance and Security",
      "domainId": 3,
      "category": "Access Control",
      "summary": "Restricts query results based on user identity/role so managers see only their team's data and employees see personal data.",
      "description": "RLS rules evaluated at query time. HR admin sees all employees; manager sees only direct reports. Prevents accidental overexposure.",
      "fabricComponents": [
        "Warehouse",
        "Semantic Model",
        "Power BI",
        "RLS Roles"
      ],
      "pros": [
        "Query-time enforcement is performant.",
        "Prevents overexposure through accidental queries.",
        "Works across Warehouse and Power BI."
      ],
      "cons": [
        "RLS logic can become complex.",
        "Debugging RLS issues is difficult.",
        "Semantic model must support RLS columns."
      ],
      "usageInstructions": "1. Identify RLS dimension (e.g., manager_id, department). 2. Create Warehouse views with RLS. 3. In semantic model, define RLS roles. 4. Map users to roles. 5. Test query results per role. 6. Assign roles to users.",
      "governanceConsiderations": "RLS is critical for HR data. Test thoroughly before production. Document RLS logic. Monitor for overexposure. Update when org changes.",
      "peopleAnalyticsUseCases": [
        "Managers see direct reports salary; executives see all.",
        "Employees see personal data only.",
        "HR sees department; Finance sees cost center salary."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "medallion-architecture",
        "sensitivity-labels",
        "certified-semantic-model"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "rls",
        "row-level-security",
        "access-control"
      ],
      "referenceLinks": [
        {
          "label": "Row-Level Security",
          "url": "https://docs.microsoft.com/en-us/power-bi/enterprise/service-admin-rls"
        }
      ],
      "estimatedImplementationEffort": "2-3 weeks",
      "costImplications": "Minimal compute overhead"
    },
    {
      "id": "dynamic-data-masking",
      "name": "Dynamic Data Masking for Development/Test",
      "domain": "Data Governance and Security",
      "domainId": 3,
      "category": "Data Protection",
      "summary": "Masks sensitive data values in non-production environments so developers see realistic data without exposure.",
      "description": "Replace salary values with 0, truncate SSN to last 4 digits, replace names with 'Employee-123'. Development team tests with masked data.",
      "fabricComponents": [
        "Warehouse",
        "Spark Notebook",
        "Data Masking Policies"
      ],
      "pros": [
        "Enables realistic testing without sensitive data exposure.",
        "Reduces security incidents from dev environment breaches.",
        "Supports faster dev cycles without data sanitization."
      ],
      "cons": [
        "Masking logic can affect performance.",
        "Developers frustrated by realistic data lack.",
        "Complex to mask consistently."
      ],
      "usageInstructions": "1. Create dev/test environments. 2. Define masking rules for sensitive columns. 3. Apply masks on data refresh. 4. Validate masking prevents identification. 5. Monitor compliance.",
      "governanceConsiderations": "Mask all sensitive data in non-prod. Maintain consistent masking across all clones. Document masking rules. Monitor mask effectiveness.",
      "peopleAnalyticsUseCases": [
        "Dev database with masked SSN, salary for developers.",
        "Test environment with realistic structure but masked values.",
        "Compliance environment with redacted data."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "sensitivity-labels",
        "row-level-security"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "data-masking",
        "pii-protection",
        "development"
      ],
      "referenceLinks": [
        {
          "label": "Data Masking",
          "url": "https://docs.microsoft.com/en-us/sql/relational-databases/security/dynamic-data-masking"
        }
      ],
      "estimatedImplementationEffort": "2-3 weeks",
      "costImplications": "Minimal overhead for masking operations"
    },
    {
      "id": "abac-access-control",
      "name": "Attribute-Based Access Control (ABAC)",
      "domain": "Data Governance and Security",
      "domainId": 3,
      "category": "Access Control",
      "summary": "Access decisions based on user attributes (department, role, location) plus resource attributes enabling fine-grained, scalable permissions.",
      "description": "Role/department attributes determine access. HR team member + HR resource = access. Scales better than managing individual user permissions.",
      "fabricComponents": [
        "Workspace Permissions",
        "Semantic Model Roles",
        "Azure AD Groups"
      ],
      "pros": [
        "Scales to large organizations.",
        "Changes managed through attributes, not user lists.",
        "Reduces permission management overhead."
      ],
      "cons": [
        "Requires attribute governance.",
        "Complex logic can be hard to audit.",
        "Debugging attribute-based denials difficult."
      ],
      "usageInstructions": "1. Define access attributes. 2. Populate attributes in Azure AD. 3. Create Azure AD groups by attributes. 4. Assign groups to Workspace/Model roles. 5. Test access per attribute combination. 6. Monitor attribute changes.",
      "governanceConsiderations": "Attribute definitions require business input. Master attributes in Azure AD. Audit attribute changes. Regular access reviews. Update as org changes.",
      "peopleAnalyticsUseCases": [
        "Department attribute controls workspace access.",
        "Role attribute determines semantic model permissions.",
        "Location attribute gates data access.",
        "Manager attribute enables RLS."
      ],
      "complexity": "High",
      "maturity": "GA",
      "compatibleWith": [
        "row-level-security",
        "workspace-permission-governance"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "abac",
        "attribute-based",
        "access-control"
      ],
      "referenceLinks": [
        {
          "label": "ABAC Concepts",
          "url": "https://docs.microsoft.com/en-us/azure/active-directory/conditional-access/overview"
        }
      ],
      "estimatedImplementationEffort": "4-6 weeks",
      "costImplications": "Azure AD licensing; minimal compute"
    },
    {
      "id": "workspace-permission-governance",
      "name": "Workspace Permission Governance",
      "domain": "Data Governance and Security",
      "domainId": 3,
      "category": "Access Control",
      "summary": "Manages workspace role assignments (Admin, Member, Contributor, Viewer) through approval workflows preventing unauthorized access creep.",
      "description": "Access requests go through approval. Quarterly access reviews. Admins audit who has what role. Revoke unused access promptly.",
      "fabricComponents": [
        "Workspaces",
        "Roles",
        "Azure AD",
        "Access Reviews"
      ],
      "pros": [
        "Prevents unauthorized access accumulation.",
        "Audit trail of who approves access.",
        "Regular reviews catch stale access."
      ],
      "cons": [
        "Adds overhead to access provisioning.",
        "Review fatigue with many users.",
        "Requires disciplined process."
      ],
      "usageInstructions": "1. Define role matrix. 2. Establish request process. 3. Configure approval workflow. 4. Quarterly access review. 5. Deprovision unused access. 6. Audit log.",
      "governanceConsiderations": "Clear role definitions. Documented request process. Approval authority defined. Review frequency set. Audit logged.",
      "peopleAnalyticsUseCases": [
        "Request approval for workspace access.",
        "Quarterly review of HR Analytics workspace roles.",
        "Audit trail for compliance."
      ],
      "complexity": "Low",
      "maturity": "GA",
      "compatibleWith": [
        "abac-access-control",
        "purview-data-map"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "workspace-permissions",
        "governance",
        "access-control"
      ],
      "referenceLinks": [
        {
          "label": "Workspace Roles",
          "url": "https://docs.microsoft.com/en-us/fabric/admin/roles"
        }
      ],
      "estimatedImplementationEffort": "2-3 weeks",
      "costImplications": "Minimal overhead"
    },
    {
      "id": "directlake-power-bi",
      "name": "Direct Lake Power BI Semantic Models",
      "domain": "Business Intelligence and Reporting",
      "domainId": 4,
      "category": "BI Modeling",
      "summary": "Power BI semantic models using Direct Lake connectivity to Fabric Gold tables for real-time BI without data import.",
      "description": "Gold tables feed Power BI directly via Direct Lake. No nightly imports. BI analysts refresh tables instantly. Dashboards always show latest data.",
      "fabricComponents": [
        "Power BI",
        "Semantic Model",
        "Direct Lake",
        "Lakehouse"
      ],
      "pros": [
        "Real-time data for dashboards.",
        "No import bottleneck.",
        "Simplified data pipeline."
      ],
      "cons": [
        "Requires optimized Gold tables.",
        "Less transformation flexibility.",
        "Network latency possible."
      ],
      "usageInstructions": "1. Create semantic model in Workspace. 2. Connect to Gold tables via Direct Lake. 3. Define relationships. 4. Create measures. 5. Build reports. 6. Publish. 7. Monitor performance.",
      "governanceConsiderations": "Gold tables must be governed. Control who modifies lakehouse. Apply RLS at semantic model level. Document data contracts.",
      "peopleAnalyticsUseCases": [
        "Real-time HR dashboard from Gold tables.",
        "Executive payroll dashboard.",
        "Live recruiting pipeline dashboard."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "medallion-architecture",
        "direct-lake-semantic-model",
        "delta-lake-partitioning"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "medallion-architecture"
      ],
      "tags": [
        "power-bi",
        "direct-lake",
        "semantic-model"
      ],
      "referenceLinks": [
        {
          "label": "Direct Lake BI",
          "url": "https://docs.microsoft.com/en-us/power-bi/enterprise/directlake-best-practices"
        }
      ],
      "estimatedImplementationEffort": "1-2 weeks",
      "costImplications": "Reduces Premium capacity cost vs import"
    },
    {
      "id": "storage-mode-selection",
      "name": "Storage Mode Selection (Import/DirectQuery/Dual)",
      "domain": "Business Intelligence and Reporting",
      "domainId": 4,
      "category": "BI Modeling",
      "summary": "Chooses optimal storage mode per table based on size, update frequency, and performance requirements.",
      "description": "Import small lookup tables for speed. DirectQuery large slow-changing fact tables. Dual mode combines both for flexibility.",
      "fabricComponents": [
        "Power BI",
        "Semantic Model",
        "Warehouse",
        "Lakehouse"
      ],
      "pros": [
        "Optimizes performance and refresh time.",
        "Reduces Premium capacity utilization.",
        "Flexibility for heterogeneous requirements."
      ],
      "cons": [
        "Increases modeling complexity.",
        "DirectQuery can be slow without optimization.",
        "Users must understand modes."
      ],
      "usageInstructions": "1. Analyze table size and query frequency. 2. Small/frequently accessed = Import. 3. Large/slow-changing = DirectQuery. 4. Mixed = Dual. 5. Monitor refresh times. 6. Adjust modes based on perf.",
      "governanceConsiderations": "Document storage mode decisions. Monitor refresh failures. Update source query optimization. Test DirectQuery performance.",
      "peopleAnalyticsUseCases": [
        "Import employee dimension; DirectQuery large payroll facts.",
        "Dual mode org hierarchy with frequent ref lookup.",
        "Import cost center lookup; DirectQuery salary facts."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "directlake-power-bi",
        "certified-semantic-model"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "storage-mode",
        "power-bi",
        "performance"
      ],
      "referenceLinks": [
        {
          "label": "Storage Modes",
          "url": "https://docs.microsoft.com/en-us/power-bi/transform-model/desktop-storage-mode"
        }
      ],
      "estimatedImplementationEffort": "1-2 weeks",
      "costImplications": "Optimizes capacity utilization"
    },
    {
      "id": "composite-model",
      "name": "Composite Models (Multi-Source Mashing)",
      "domain": "Business Intelligence and Reporting",
      "domainId": 4,
      "category": "BI Modeling",
      "summary": "Combines tables from multiple sources (Lakehouse, Warehouse, SQL, Excel) in single semantic model for unified analytics.",
      "description": "Employee master from Lakehouse, payroll from Warehouse, survey data from Excel. Composite model joins across sources.",
      "fabricComponents": [
        "Power BI",
        "Semantic Model",
        "Composite Model",
        "Multiple Sources"
      ],
      "pros": [
        "Unified analytics across sources.",
        "Reduces data movement.",
        "Flexible source management."
      ],
      "cons": [
        "Query complexity increases.",
        "Cross-source joins can be slow.",
        "Debugging difficult."
      ],
      "usageInstructions": "1. Create semantic model. 2. Add tables from multiple sources. 3. Define relationships across sources. 4. Create measures. 5. Test query performance. 6. Monitor.",
      "governanceConsiderations": "Document source integration logic. Monitor cross-source join performance. Establish data ownership across sources.",
      "peopleAnalyticsUseCases": [
        "Employee Lakehouse table joined with payroll Warehouse table.",
        "Org Lakehouse joined with survey results from Excel.",
        "Performance Lakehouse joined with compensation Warehouse."
      ],
      "complexity": "High",
      "maturity": "GA",
      "compatibleWith": [
        "storage-mode-selection",
        "certified-semantic-model"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "composite-model",
        "multi-source",
        "power-bi"
      ],
      "referenceLinks": [
        {
          "label": "Composite Models",
          "url": "https://docs.microsoft.com/en-us/power-bi/transform-model/desktop-composite-models"
        }
      ],
      "estimatedImplementationEffort": "2-3 weeks",
      "costImplications": "Higher compute for cross-source joins"
    },
    {
      "id": "certified-semantic-model",
      "name": "Certified Semantic Models",
      "domain": "Business Intelligence and Reporting",
      "domainId": 4,
      "category": "BI Governance",
      "summary": "BI team certifies semantic models ensuring consistent definitions, quality metrics, and single source of truth for organization.",
      "description": "Gold-layer semantic models certified by BI team mark metrics as authoritative. Analysts use certified models for consistency.",
      "fabricComponents": [
        "Power BI",
        "Semantic Model",
        "Workspace"
      ],
      "pros": [
        "Ensures metric consistency across dashboards.",
        "Reduces metric duplication.",
        "Facilitates self-service BI."
      ],
      "cons": [
        "Requires strong BI governance.",
        "Slows new model deployment.",
        "Can bottleneck innovation."
      ],
      "usageInstructions": "1. Build semantic model. 2. Document metrics and definitions. 3. Have BI team certify. 4. Mark as Certified. 5. Analysts build reports. 6. Update as needed.",
      "governanceConsiderations": "Establish certification criteria. Document metric definitions. Review before certification. Update certification on changes.",
      "peopleAnalyticsUseCases": [
        "Certified headcount metric used across dashboards.",
        "Certified FTE calculation model.",
        "Certified cost per hire metric."
      ],
      "complexity": "Low",
      "maturity": "GA",
      "compatibleWith": [
        "directlake-power-bi",
        "storage-mode-selection",
        "composite-model"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "certified-model",
        "governance",
        "metrics"
      ],
      "referenceLinks": [
        {
          "label": "Certified Models",
          "url": "https://docs.microsoft.com/en-us/power-bi/collaborate-share/service-certify-datasets"
        }
      ],
      "estimatedImplementationEffort": "2-3 weeks",
      "costImplications": "Minimal overhead"
    },
    {
      "id": "paginated-reports",
      "name": "Paginated Reports for Formal Documents",
      "domain": "Business Intelligence and Reporting",
      "domainId": 4,
      "category": "Report Development",
      "summary": "Power BI paginated reports for pixel-perfect formal documents like payroll statements, regulatory reports, and audit certifications.",
      "description": "Paginated reports in Power BI for formatted output. Employee tax statements, OFCCP compliance reports, board summaries.",
      "fabricComponents": [
        "Power BI",
        "Paginated Reports",
        "Warehouse",
        "Semantic Model"
      ],
      "pros": [
        "Pixel-perfect formatting for formal documents.",
        "Supports complex layouts and headers.",
        "Suitable for printing and distribution."
      ],
      "cons": [
        "Slower to develop than dashboards.",
        "Limited interactivity.",
        "Requires RDL knowledge."
      ],
      "usageInstructions": "1. Create paginated report in Power BI. 2. Define parameters. 3. Design layout. 4. Connect to data source. 5. Format for printing. 6. Test output. 7. Schedule.",
      "governanceConsiderations": "Formal reports require approval. Document generation logic. Maintain version history. Ensure data accuracy.",
      "peopleAnalyticsUseCases": [
        "Employee tax statements generated monthly.",
        "OFCCP compliance report certification.",
        "Board summary with specific formatting.",
        "Payroll audit report."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "storage-mode-selection",
        "certified-semantic-model"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "paginated-reports",
        "formal-documents"
      ],
      "referenceLinks": [
        {
          "label": "Paginated Reports",
          "url": "https://docs.microsoft.com/en-us/power-bi/paginated-reports/paginated-reports-report-builder-power-bi"
        }
      ],
      "estimatedImplementationEffort": "2-4 weeks",
      "costImplications": "Minimal overhead; scales with report volume"
    },
    {
      "id": "metrics-scorecard",
      "name": "Power BI Metrics Scorecards",
      "domain": "Business Intelligence and Reporting",
      "domainId": 4,
      "category": "Report Development",
      "summary": "Visual display of key metrics with goals, trends, and out-of-range alerts enabling executive dashboards.",
      "description": "Headcount goal vs actual, cost per hire vs target, time-to-fill trend. Scorecard shows metric, trend, variance from goal.",
      "fabricComponents": [
        "Power BI",
        "Metrics",
        "Semantic Model"
      ],
      "pros": [
        "Executive-friendly visualization.",
        "Quick identification of variances.",
        "Supports scorecards at any granularity."
      ],
      "cons": [
        "Requires careful metric selection.",
        "Not suited for deep analysis.",
        "Goal management overhead."
      ],
      "usageInstructions": "1. Select metrics. 2. Define goals. 3. Create scorecard visual. 4. Connect to semantic model. 5. Format for execs. 6. Publish.",
      "governanceConsiderations": "Goals reviewed and approved. Metric definitions consistent. Regular updates. Executive alignment on priorities.",
      "peopleAnalyticsUseCases": [
        "Executive dashboard with headcount, cost, turnover metrics.",
        "Department scorecard with regional goals.",
        "Recruiting metrics scorecard with time-to-fill, cost-per-hire."
      ],
      "complexity": "Low",
      "maturity": "GA",
      "compatibleWith": [
        "certified-semantic-model",
        "directlake-power-bi"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "metrics-scorecard",
        "executive-dashboard"
      ],
      "referenceLinks": [
        {
          "label": "Metrics Scorecard",
          "url": "https://docs.microsoft.com/en-us/power-bi/create-reports/service-metrics-cards"
        }
      ],
      "estimatedImplementationEffort": "1-2 weeks",
      "costImplications": "Minimal overhead"
    },
    {
      "id": "deployment-pipelines",
      "name": "Power BI Deployment Pipelines",
      "domain": "Business Intelligence and Reporting",
      "domainId": 4,
      "category": "DevOps",
      "summary": "Automated promotion of BI artifacts from development through staging to production enabling controlled releases.",
      "description": "Dev \u2192 Stage \u2192 Prod. Semantic models and reports tested in stage before prod deployment. Reduces errors in production.",
      "fabricComponents": [
        "Power BI",
        "Deployment Pipelines",
        "Semantic Model",
        "Reports"
      ],
      "pros": [
        "Reduces deployment errors.",
        "Enables testing before production.",
        "Audit trail of changes."
      ],
      "cons": [
        "Setup complexity.",
        "Requires discipline in dev/stage separation.",
        "Can slow development cycles."
      ],
      "usageInstructions": "1. Create three workspaces: Dev, Stage, Prod. 2. Set up pipeline. 3. Develop in Dev. 4. Deploy to Stage. 5. Test. 6. Deploy to Prod. 7. Monitor.",
      "governanceConsiderations": "Change approval for Prod. Test in Stage. Monitor Prod performance. Rollback procedures. Post-mortem on failures.",
      "peopleAnalyticsUseCases": [
        "Develop dashboard in Dev; promote through Stage to Prod.",
        "Semantic model change testing before production.",
        "New report rollout with staging validation."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "certified-semantic-model",
        "directlake-power-bi"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "deployment-pipelines",
        "devops",
        "power-bi"
      ],
      "referenceLinks": [
        {
          "label": "Deployment Pipelines",
          "url": "https://docs.microsoft.com/en-us/power-bi/create-reports/deployment-pipelines-overview"
        }
      ],
      "estimatedImplementationEffort": "1-2 weeks",
      "costImplications": "Minimal overhead"
    },
    {
      "id": "batch-inference-pipeline",
      "name": "Batch Inference Pipelines",
      "domain": "Machine Learning and Traditional AI",
      "domainId": 5,
      "category": "ML Operationalization",
      "summary": "Regular batch scoring of employee records against trained ML models producing predictions (attrition risk, salary range) at scale.",
      "description": "Weekly job scores all active employees with churn model. Output predictions to Gold layer. HR uses for retention focus.",
      "fabricComponents": [
        "Spark Notebook",
        "MLflow",
        "Feature Store",
        "Lakehouse"
      ],
      "pros": [
        "Scalable scoring for thousands of employees.",
        "Scheduled inference keeps predictions current.",
        "Batch approach efficient for throughput."
      ],
      "cons": [
        "Latency from batch schedule.",
        "Storage for predictions grows.",
        "Model monitoring required."
      ],
      "usageInstructions": "1. Load feature store. 2. Load trained model from MLflow. 3. Score features. 4. Format output. 5. Write to Gold. 6. Schedule daily/weekly. 7. Monitor scores.",
      "governanceConsiderations": "Document model assumptions. Monitor prediction distributions. Audit high-risk predictions. Update regularly.",
      "peopleAnalyticsUseCases": [
        "Weekly churn risk scoring of all employees.",
        "Salary range prediction for compensation analysis.",
        "Performance rating prediction."
      ],
      "complexity": "High",
      "maturity": "GA",
      "compatibleWith": [
        "feature-store-delta",
        "mlflow-model-registry",
        "medallion-architecture"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "feature-store-delta"
      ],
      "tags": [
        "ml",
        "batch-inference",
        "predictions"
      ],
      "referenceLinks": [
        {
          "label": "Batch Scoring",
          "url": "https://docs.microsoft.com/en-us/fabric/data-science/model-serving"
        }
      ],
      "estimatedImplementationEffort": "3-4 weeks",
      "costImplications": "Spark compute scales with employee count"
    },
    {
      "id": "feature-store-delta",
      "name": "Feature Store Implementation",
      "domain": "Machine Learning and Traditional AI",
      "domainId": 5,
      "category": "ML Infrastructure",
      "summary": "Centralized repository of engineered features (tenure_years, salary_percentile) for reuse across ML models reducing redundancy.",
      "description": "Feature computation once, reuse everywhere. Tenure_years calculated once from hire_date; all models use same value.",
      "fabricComponents": [
        "Delta Lake",
        "Lakehouse",
        "Feature Store",
        "MLflow"
      ],
      "pros": [
        "Features computed once, reused everywhere.",
        "Ensures consistency across models.",
        "Facilitates collaboration."
      ],
      "cons": [
        "Setup overhead.",
        "Feature staleness if not refreshed.",
        "Storage growth."
      ],
      "usageInstructions": "1. Design features. 2. Compute features in Spark. 3. Store in Delta tables. 4. Register in feature store. 5. Models reference feature store. 6. Refresh regularly.",
      "governanceConsiderations": "Document feature logic. Version feature definitions. Monitor freshness. Retire unused features.",
      "peopleAnalyticsUseCases": [
        "Tenure, salary percentile, performance rating features.",
        "Department, manager, location features.",
        "Historical aggregations: avg salary by dept."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "medallion-architecture",
        "batch-inference-pipeline",
        "mlflow-model-registry"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "medallion-architecture"
      ],
      "tags": [
        "feature-store",
        "ml-infrastructure"
      ],
      "referenceLinks": [
        {
          "label": "Feature Stores",
          "url": "https://docs.microsoft.com/en-us/fabric/data-science/data-science-overview"
        }
      ],
      "estimatedImplementationEffort": "2-3 weeks",
      "costImplications": "Storage for feature tables; compute for refresh"
    },
    {
      "id": "mlflow-model-registry",
      "name": "MLflow Model Registry",
      "domain": "Machine Learning and Traditional AI",
      "domainId": 5,
      "category": "ML Governance",
      "summary": "Centralized repository for ML models with versioning, staging (Dev/Prod), and metadata enabling model lifecycle management.",
      "description": "Models registered in MLflow. Dev version tested; Prod version deployed. Version history for rollback. Metadata documents model purpose.",
      "fabricComponents": [
        "MLflow",
        "Spark Notebook",
        "Model Registry"
      ],
      "pros": [
        "Version control for models.",
        "Staging enables testing.",
        "Metadata enables governance.",
        "Easy rollback."
      ],
      "cons": [
        "Requires MLflow setup.",
        "Deployment automation needed.",
        "Model monitoring overhead."
      ],
      "usageInstructions": "1. Train model. 2. Register in MLflow. 3. Set Staging=Dev. 4. Test in Stage. 5. Transition to Prod. 6. Deploy. 7. Monitor.",
      "governanceConsiderations": "Document model purpose, assumptions, metrics. Code review before Prod. Monitor performance. Track versions.",
      "peopleAnalyticsUseCases": [
        "Churn model versions tracked and tested.",
        "Salary prediction model with Prod version.",
        "Performance rating model with rollback capability."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "batch-inference-pipeline",
        "feature-store-delta",
        "model-drift-detection"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "feature-store-delta"
      ],
      "tags": [
        "mlflow",
        "model-registry",
        "ml-governance"
      ],
      "referenceLinks": [
        {
          "label": "MLflow Registry",
          "url": "https://docs.microsoft.com/en-us/fabric/data-science/model-serving"
        }
      ],
      "estimatedImplementationEffort": "1-2 weeks",
      "costImplications": "Minimal overhead for registry"
    },
    {
      "id": "model-drift-detection",
      "name": "Model Drift Detection and Monitoring",
      "domain": "Machine Learning and Traditional AI",
      "domainId": 5,
      "category": "ML Monitoring",
      "summary": "Automated monitoring of model performance metrics detecting drift when predictions no longer match reality triggering retraining.",
      "description": "Monitor churn model prediction accuracy. If accuracy drops below 70%, alert to retrain. Detect input data distribution changes.",
      "fabricComponents": [
        "Spark Notebook",
        "MLflow",
        "Monitoring",
        "Delta Lake"
      ],
      "pros": [
        "Early detection of model decay.",
        "Automated alerts trigger action.",
        "Historical performance tracking."
      ],
      "cons": [
        "Requires ground truth labels.",
        "Monitoring setup overhead.",
        "Retraining may be expensive."
      ],
      "usageInstructions": "1. Define performance metrics. 2. Set baseline and alert thresholds. 3. Score model on new data. 4. Compute metrics. 5. Alert if drift detected. 6. Retrain if needed.",
      "governanceConsiderations": "Document drift thresholds. Establish retraining SLAs. Track model lifecycle. Post-mortem on failures.",
      "peopleAnalyticsUseCases": [
        "Monitor churn model accuracy; retrain monthly.",
        "Detect salary prediction model degradation.",
        "Alert on input data distribution shifts."
      ],
      "complexity": "High",
      "maturity": "Preview",
      "compatibleWith": [
        "batch-inference-pipeline",
        "mlflow-model-registry"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "mlflow-model-registry"
      ],
      "tags": [
        "model-drift",
        "monitoring",
        "ml-ops"
      ],
      "referenceLinks": [
        {
          "label": "Model Monitoring",
          "url": "https://docs.microsoft.com/en-us/fabric/data-science/model-serving"
        }
      ],
      "estimatedImplementationEffort": "2-3 weeks",
      "costImplications": "Compute for drift detection and retraining"
    },
    {
      "id": "fairness-bias-evaluation",
      "name": "Fairness and Bias Evaluation",
      "domain": "Machine Learning and Traditional AI",
      "domainId": 5,
      "category": "ML Ethics",
      "summary": "Evaluates ML models for bias in predictions across demographic groups (gender, race) ensuring fair and compliant hiring/promotion decisions.",
      "description": "Churn model predictions should not systematically disfavor any demographic. Test for disparate impact. Document findings.",
      "fabricComponents": [
        "Spark Notebook",
        "Fairness Toolkit",
        "Delta Lake"
      ],
      "pros": [
        "Ensures compliance with bias regulations.",
        "Detects systematic unfairness.",
        "Supports ethical decision-making."
      ],
      "cons": [
        "Requires labeled demographic data.",
        "No perfect fairness definition.",
        "Trade-offs between fairness metrics."
      ],
      "usageInstructions": "1. Get ground truth + demographics. 2. Run fairness analysis. 3. Compare metrics by group. 4. Document findings. 5. Adjust model if needed. 6. Retest.",
      "governanceConsiderations": "Privacy-conscious demographic collection. Document assumptions. Legal review. Regular reassessment as data changes.",
      "peopleAnalyticsUseCases": [
        "Evaluate churn model for gender bias.",
        "Assess promotion recommendation fairness.",
        "Audit salary prediction by race/ethnicity."
      ],
      "complexity": "High",
      "maturity": "Emerging",
      "compatibleWith": [
        "batch-inference-pipeline",
        "mlflow-model-registry"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "mlflow-model-registry"
      ],
      "tags": [
        "fairness",
        "bias",
        "ethics",
        "ml"
      ],
      "referenceLinks": [
        {
          "label": "Fairness Analysis",
          "url": "https://docs.microsoft.com/en-us/azure/machine-learning/concept-fairness"
        }
      ],
      "estimatedImplementationEffort": "3-4 weeks",
      "costImplications": "Analysis compute; typically small"
    },
    {
      "id": "champion-challenger",
      "name": "Champion-Challenger Model Testing",
      "domain": "Machine Learning and Traditional AI",
      "domainId": 5,
      "category": "ML Experimentation",
      "summary": "A/B testing of new model versions (Challenger) against current production model (Champion) in controlled experiments.",
      "description": "Run new churn model on 10% of employees; compare predictions to current model. If Challenger performs better, promote.",
      "fabricComponents": [
        "Spark Notebook",
        "MLflow",
        "Delta Lake",
        "Experimentation"
      ],
      "pros": [
        "Safe testing of new models.",
        "Data-driven promotion decisions.",
        "Controlled rollout reduces risk."
      ],
      "cons": [
        "Requires holdout population.",
        "Delayed rollout of improvements.",
        "Complex experiment management."
      ],
      "usageInstructions": "1. Designate Champion. 2. Train Challenger. 3. Split users: 90% Champion, 10% Challenger. 4. Run experiment. 5. Compare metrics. 6. Promote if better. 7. Full rollout.",
      "governanceConsiderations": "Ethical experiment design. User consent where needed. Results documentation. Promotion approval process.",
      "peopleAnalyticsUseCases": [
        "Test new churn model on subset before full deployment.",
        "A/B test salary prediction improvements.",
        "Validate performance rating model changes."
      ],
      "complexity": "High",
      "maturity": "GA",
      "compatibleWith": [
        "mlflow-model-registry",
        "batch-inference-pipeline"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "mlflow-model-registry"
      ],
      "tags": [
        "champion-challenger",
        "ab-testing",
        "experimentation"
      ],
      "referenceLinks": [
        {
          "label": "A/B Testing",
          "url": "https://docs.microsoft.com/en-us/azure/machine-learning/concept-responsible-ml"
        }
      ],
      "estimatedImplementationEffort": "2-3 weeks",
      "costImplications": "Compute for dual model scoring"
    },
    {
      "id": "rag-fabric-grounded",
      "name": "RAG (Retrieval-Augmented Generation) Fabric-Grounded",
      "domain": "Generative AI and Conversational Interfaces",
      "domainId": 6,
      "category": "Generative AI",
      "summary": "LLM-powered chatbot that retrieves employee data, org structure, policies from Fabric and generates answers grounded in actual data.",
      "description": "'What are John's direct reports?' retrieves from org table, passes to LLM which answers. 'Top 5 highest salaries?' retrieves from payroll, generates list.",
      "fabricComponents": [
        "Azure OpenAI",
        "Semantic Model",
        "Lakehouse",
        "RAG"
      ],
      "pros": [
        "Answers grounded in actual data, not hallucinated.",
        "Natural language interface to data.",
        "Reduces manual report requests."
      ],
      "cons": [
        "LLM cost with heavy usage.",
        "Latency from retrieval + generation.",
        "Requires prompt engineering."
      ],
      "usageInstructions": "1. Set up vector index on Gold tables. 2. Configure retrieval logic. 3. Connect to LLM API. 4. Test prompts. 5. Integrate with chat interface. 6. Monitor usage.",
      "governanceConsiderations": "Retrieved data must respect RLS. Sensitive salary data must be masked. Log all queries for audit. Limit user query volume.",
      "peopleAnalyticsUseCases": [
        "HR chatbot answering org structure questions.",
        "Employee compensation bot with salary band info.",
        "Policy chatbot with benefits/leave policies."
      ],
      "complexity": "High",
      "maturity": "Preview",
      "compatibleWith": [
        "medallion-architecture",
        "row-level-security",
        "semantic-model-certification-pipeline"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "medallion-architecture",
        "row-level-security"
      ],
      "tags": [
        "rag",
        "generative-ai",
        "chatbot"
      ],
      "referenceLinks": [
        {
          "label": "RAG Pattern",
          "url": "https://docs.microsoft.com/en-us/azure/ai-services/openai/concepts/retrieval-augmented-generation"
        }
      ],
      "estimatedImplementationEffort": "4-6 weeks",
      "costImplications": "OpenAI API costs plus storage for vectors"
    },
    {
      "id": "secure-chat-rls",
      "name": "Secure Conversational Interface with RLS",
      "domain": "Generative AI and Conversational Interfaces",
      "domainId": 6,
      "category": "Generative AI",
      "summary": "Conversational AI interface enforcing row-level security so employees see only authorized data and managers see team data.",
      "description": "Employee chatbot enforces that employees see personal data only; managers see team data. RLS applied at retrieval.",
      "fabricComponents": [
        "Chatbot Framework",
        "RLS",
        "Semantic Model",
        "Azure AD"
      ],
      "pros": [
        "Natural interface with built-in security.",
        "Reduces query errors from RLS confusion.",
        "Improves user experience."
      ],
      "cons": [
        "RLS enforcement in retrieval complex.",
        "Debugging RLS issues difficult.",
        "Performance overhead."
      ],
      "usageInstructions": "1. Implement chatbot. 2. Identify user identity. 3. Apply RLS filter to retrieval. 4. Answer questions respecting RLS. 5. Log queries. 6. Monitor.",
      "governanceConsiderations": "RLS must be correctly enforced. Audit RLS-filtered queries. Prevent RLS bypass through prompt injection.",
      "peopleAnalyticsUseCases": [
        "Employee chatbot showing personal salary only.",
        "Manager chatbot with team-scoped salary data.",
        "Executive chatbot with company-wide access."
      ],
      "complexity": "High",
      "maturity": "Preview",
      "compatibleWith": [
        "row-level-security",
        "rag-fabric-grounded"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "row-level-security"
      ],
      "tags": [
        "secure-chat",
        "rls",
        "conversational"
      ],
      "referenceLinks": [
        {
          "label": "Secure Chatbots",
          "url": "https://docs.microsoft.com/en-us/power-virtual-agents/"
        }
      ],
      "estimatedImplementationEffort": "4-6 weeks",
      "costImplications": "Chatbot platform + API costs"
    },
    {
      "id": "azure-ai-foundry-integration",
      "name": "Azure AI Foundry Integration",
      "domain": "Generative AI and Conversational Interfaces",
      "domainId": 6,
      "category": "Generative AI",
      "summary": "Integrates Azure AI Foundry (formerly Cognitive Services) for NLP, document understanding, and entity extraction on HR documents.",
      "description": "Extract key info from resumes, offer letters, termination docs. Classify employee feedback as positive/negative. Structure unstructured data.",
      "fabricComponents": [
        "Azure AI Foundry",
        "Form Recognizer",
        "Text Analytics",
        "Lakehouse"
      ],
      "pros": [
        "Pre-trained models for common NLP tasks.",
        "Document understanding with form parsing.",
        "Reduces custom ML effort."
      ],
      "cons": [
        "API costs scale with usage.",
        "Limited customization compared to custom ML.",
        "Latency for real-time extraction."
      ],
      "usageInstructions": "1. Connect Azure AI Foundry. 2. Select service (Form Recognizer, Text Analytics). 3. Call API on documents. 4. Store results in Lakehouse. 5. Reference in analytics.",
      "governanceConsiderations": "PII in documents must be protected. API calls logged. Retention policy for extracted data.",
      "peopleAnalyticsUseCases": [
        "Resume parsing to extract skills, experience.",
        "Employee feedback analysis: sentiment classification.",
        "Offer letter extraction: salary, start date.",
        "Performance review entity extraction."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "medallion-architecture",
        "spark-notebook-etl"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "azure-ai",
        "nlp",
        "document-understanding"
      ],
      "referenceLinks": [
        {
          "label": "Azure AI Foundry",
          "url": "https://docs.microsoft.com/en-us/azure/ai-services/"
        }
      ],
      "estimatedImplementationEffort": "2-3 weeks",
      "costImplications": "Azure AI Foundry API costs per request"
    },
    {
      "id": "copilot-studio-grounded",
      "name": "Copilot Studio with Fabric Data Grounding",
      "domain": "Generative AI and Conversational Interfaces",
      "domainId": 6,
      "category": "Generative AI",
      "summary": "Power Platform Copilot Studio integration with Fabric data enabling custom copilots grounded in HR analytics without custom coding.",
      "description": "Drag-drop copilot builder connecting to Fabric semantic models. Answers grounded in org data. No coding required.",
      "fabricComponents": [
        "Copilot Studio",
        "Power Platform",
        "Semantic Model",
        "Fabric"
      ],
      "pros": [
        "Low-code copilot creation.",
        "Native Fabric integration.",
        "Reduced development time.",
        "Power Platform ecosystem."
      ],
      "cons": [
        "Limited to Power Platform capabilities.",
        "Customization limited compared to custom code.",
        "Cost per interaction."
      ],
      "usageInstructions": "1. Create copilot in Studio. 2. Connect to Fabric semantic model. 3. Define intents. 4. Map to Fabric queries. 5. Test. 6. Deploy.",
      "governanceConsiderations": "Semantic model permissions enforced. Audit conversations. Sensitive data restrictions.",
      "peopleAnalyticsUseCases": [
        "HR chatbot answering org questions.",
        "Recruiter copilot with candidate data.",
        "Employee self-service copilot."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "certified-semantic-model",
        "direct-lake-semantic-model"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "copilot-studio",
        "low-code",
        "generative-ai"
      ],
      "referenceLinks": [
        {
          "label": "Copilot Studio",
          "url": "https://docs.microsoft.com/en-us/power-virtual-agents/"
        }
      ],
      "estimatedImplementationEffort": "2-3 weeks",
      "costImplications": "Copilot Studio licensing"
    },
    {
      "id": "semantic-search-vectors",
      "name": "Semantic Search with Vector Embeddings",
      "domain": "Generative AI and Conversational Interfaces",
      "domainId": 6,
      "category": "Generative AI",
      "summary": "Vector embeddings of HR data (job descriptions, policies, performance reviews) enabling semantic similarity search.",
      "description": "Embed job descriptions, search 'find roles similar to engineer.' Embed policies, search 'parental leave policy.'",
      "fabricComponents": [
        "Vector Store",
        "Embeddings API",
        "Lakehouse",
        "Semantic Search"
      ],
      "pros": [
        "Semantic similarity beyond keyword match.",
        "Supports RAG and discovery use cases.",
        "Natural language search."
      ],
      "cons": [
        "Storage overhead for vectors.",
        "Vector quality depends on embedding model.",
        "Refresh complexity."
      ],
      "usageInstructions": "1. Embed HR content. 2. Store vectors in vector DB. 3. On query, embed user query. 4. Search vector space. 5. Return top matches. 6. Feed to LLM.",
      "governanceConsiderations": "Vectors must preserve privacy of embedded content. Refresh vectors when source updates.",
      "peopleAnalyticsUseCases": [
        "Job description semantic search: find similar roles.",
        "Policy search: natural language policy questions.",
        "Performance review search: find similar feedback."
      ],
      "complexity": "High",
      "maturity": "Preview",
      "compatibleWith": [
        "rag-fabric-grounded",
        "medallion-architecture"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "semantic-search",
        "vectors",
        "embeddings"
      ],
      "referenceLinks": [
        {
          "label": "Vector Search",
          "url": "https://docs.microsoft.com/en-us/azure/search/vector-search-overview"
        }
      ],
      "estimatedImplementationEffort": "3-4 weeks",
      "costImplications": "Vector DB storage plus embedding API costs"
    },
    {
      "id": "llm-auto-narrative",
      "name": "LLM Auto-Generated Narratives",
      "domain": "Generative AI and Conversational Interfaces",
      "domainId": 6,
      "category": "Generative AI",
      "summary": "LLM automatically generates narrative descriptions of dashboards, trends, and insights reducing reporting burden.",
      "description": "Dashboard shows headcount down 5% YoY. LLM generates: 'Headcount declined 5% year-over-year to 1,200 from 1,263.'",
      "fabricComponents": [
        "Azure OpenAI",
        "Power BI",
        "Lakehouse"
      ],
      "pros": [
        "Reduces manual report writing.",
        "Generates consistent narratives.",
        "Scales insights to many users."
      ],
      "cons": [
        "LLM cost with scale.",
        "Narrative quality depends on prompts.",
        "May need human review for critical reports."
      ],
      "usageInstructions": "1. Extract dashboard metrics. 2. Format for LLM. 3. Call LLM with prompt template. 4. Generate narrative. 5. Insert in report. 6. Review.",
      "governanceConsiderations": "LLM outputs must be reviewed before publication. Sensitive data in narratives must be protected.",
      "peopleAnalyticsUseCases": [
        "Dashboard narrative generation for executive summary.",
        "Trend description in reports.",
        "Anomaly narration.",
        "Insight summarization."
      ],
      "complexity": "Medium",
      "maturity": "Preview",
      "compatibleWith": [
        "medallion-architecture",
        "directlake-power-bi"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "llm",
        "auto-narrative",
        "generative-ai"
      ],
      "referenceLinks": [
        {
          "label": "LLM Integration",
          "url": "https://docs.microsoft.com/en-us/azure/ai-services/openai/"
        }
      ],
      "estimatedImplementationEffort": "2-3 weeks",
      "costImplications": "OpenAI API costs per narrative generation"
    },
    {
      "id": "hr-ai-guardrails",
      "name": "HR-Specific AI Guardrails and Safety",
      "domain": "Generative AI and Conversational Interfaces",
      "domainId": 6,
      "category": "Generative AI",
      "summary": "Guardrails preventing AI from making inappropriate recommendations on hiring, termination, or compensation decisions.",
      "description": "Prevent salary recommendations based on protected attributes. Prevent discriminatory hiring suggestions. Audit all recommendations.",
      "fabricComponents": [
        "Azure OpenAI",
        "Guardrails Framework",
        "Auditing"
      ],
      "pros": [
        "Prevents discriminatory AI outputs.",
        "Ensures compliance with employment law.",
        "Reduces liability.",
        "Builds user trust."
      ],
      "cons": [
        "Guardrails overhead.",
        "May block valid use cases.",
        "Guardrail effectiveness hard to measure."
      ],
      "usageInstructions": "1. Define guardrails (no gender/race in salary), 2. Configure content filter. 3. Monitor LLM outputs. 4. Log violations. 5. Audit. 6. Retrain if needed.",
      "governanceConsiderations": "Legal review of guardrails. Regular guardrail testing. Audit trail of all recommendations. Transparency with users.",
      "peopleAnalyticsUseCases": [
        "Salary recommendation guardrail: no gender/race consideration.",
        "Hiring recommendation guardrail: prevent age bias.",
        "Termination recommendation: require human approval."
      ],
      "complexity": "High",
      "maturity": "Emerging",
      "compatibleWith": [
        "rag-fabric-grounded",
        "fairness-bias-evaluation"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "guardrails",
        "ai-safety",
        "compliance"
      ],
      "referenceLinks": [
        {
          "label": "Responsible AI",
          "url": "https://docs.microsoft.com/en-us/azure/machine-learning/concept-responsible-ml"
        }
      ],
      "estimatedImplementationEffort": "4-6 weeks",
      "costImplications": "Guardrails framework plus monitoring overhead"
    },
    {
      "id": "data-activator-reflex",
      "name": "Data Activator (Reflex) for Auto-Actions",
      "domain": "Alerting, Automation, and Operational Intelligence",
      "domainId": 7,
      "category": "Automation",
      "summary": "Automatic workflows triggered by data anomalies: high turnover in department \u2192 auto-email recruiter; forecast miss \u2192 escalate to VP.",
      "description": "Monitor headcount by department. If any dept loses >10% of staff in a month, auto-trigger recruiting workflow. Alert manager.",
      "fabricComponents": [
        "Data Activator",
        "Reflex",
        "Semantic Model",
        "Power Automate"
      ],
      "pros": [
        "Responds to anomalies automatically.",
        "Reduces manual monitoring.",
        "Scales to many metrics."
      ],
      "cons": [
        "False positives trigger wasted workflows.",
        "Complex rule logic is hard to debug.",
        "Costs with action volume."
      ],
      "usageInstructions": "1. Create rule on metric. 2. Define trigger condition. 3. Select action (email, Power Automate). 4. Test. 5. Deploy. 6. Monitor.",
      "governanceConsiderations": "Rules must be approved. Test before prod. Monitor false positives. Log all actions. Disable poorly performing rules.",
      "peopleAnalyticsUseCases": [
        "High turnover alert \u2192 send recruiter email.",
        "Salary anomaly detection \u2192 escalate to manager.",
        "Hiring target miss \u2192 VP alert.",
        "Compliance violation \u2192 legal escalation."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "medallion-architecture",
        "certified-semantic-model"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "medallion-architecture"
      ],
      "tags": [
        "data-activator",
        "reflex",
        "automation"
      ],
      "referenceLinks": [
        {
          "label": "Data Activator",
          "url": "https://docs.microsoft.com/en-us/fabric/real-time-analytics/data-activator/data-activator-introduction"
        }
      ],
      "estimatedImplementationEffort": "1-2 weeks per reflex",
      "costImplications": "Minimal overhead; Power Automate action costs"
    },
    {
      "id": "power-automate-triggers",
      "name": "Power Automate Workflows with Data Triggers",
      "domain": "Alerting, Automation, and Operational Intelligence",
      "domainId": 7,
      "category": "Automation",
      "summary": "Power Automate workflows triggered by HR data changes automating notifications, approvals, and downstream processes.",
      "description": "When new hire is added, automatically send welcome email, provision accounts, schedule onboarding. New termination \u2192 disable access.",
      "fabricComponents": [
        "Power Automate",
        "Warehouse",
        "Lakehouse",
        "Connectors"
      ],
      "pros": [
        "Visual workflow automation.",
        "Wide connector ecosystem.",
        "Reduces manual tasks."
      ],
      "cons": [
        "Performance at scale is limited.",
        "Complex logic becomes hard to maintain.",
        "Cost per workflow run."
      ],
      "usageInstructions": "1. Create flow. 2. Set Fabric data trigger. 3. Add actions (email, API call). 4. Test. 5. Enable. 6. Monitor.",
      "governanceConsiderations": "Flows must be approved. Sensitive actions require confirmation. Audit trail of executions. Disable unused flows.",
      "peopleAnalyticsUseCases": [
        "New hire trigger \u2192 welcome email, account provisioning.",
        "Termination trigger \u2192 disable access, exit survey.",
        "Promotion trigger \u2192 new org reporting, email announcement.",
        "Review due date \u2192 email reminder to managers."
      ],
      "complexity": "Low",
      "maturity": "GA",
      "compatibleWith": [
        "medallion-architecture",
        "data-activator-reflex"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "power-automate",
        "workflows",
        "automation"
      ],
      "referenceLinks": [
        {
          "label": "Power Automate",
          "url": "https://docs.microsoft.com/en-us/power-automate/"
        }
      ],
      "estimatedImplementationEffort": "1-2 days per workflow",
      "costImplications": "Power Automate licensing; per-action costs"
    },
    {
      "id": "metric-summarization-engine",
      "name": "Metric Summarization Engine",
      "domain": "Alerting, Automation, and Operational Intelligence",
      "domainId": 7,
      "category": "Operational Intelligence",
      "summary": "Automated daily/weekly email summaries of key metrics with trends and anomalies delivered to executives without manual compilation.",
      "description": "Daily HR metrics digest: headcount, turnover, hiring pipeline, cost summary. Auto-generated, delivered to execs 7am.",
      "fabricComponents": [
        "Lakehouse",
        "Semantic Model",
        "Automation",
        "Email"
      ],
      "pros": [
        "Executives get key metrics automatically.",
        "Reduces report compilation time.",
        "Consistent delivery schedule."
      ],
      "cons": [
        "Metric selection bias (may miss important changes).",
        "Email fatigue if too frequent.",
        "Setup complexity."
      ],
      "usageInstructions": "1. Define metrics. 2. Create automated query. 3. Format results. 4. Schedule (daily/weekly). 5. Email to distribution list. 6. Monitor engagement.",
      "governanceConsiderations": "Metrics reviewed by leadership. Delivery list managed. Format consistent. Performance tracked.",
      "peopleAnalyticsUseCases": [
        "Daily HR dashboard metrics summary.",
        "Weekly turnover report with anomalies.",
        "Monthly compensation report to executives.",
        "Quarterly talent metrics digest."
      ],
      "complexity": "Low",
      "maturity": "GA",
      "compatibleWith": [
        "medallion-architecture",
        "directlake-power-bi"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "summarization",
        "automation",
        "metrics"
      ],
      "referenceLinks": [
        {
          "label": "Scheduled Queries",
          "url": "https://docs.microsoft.com/en-us/fabric/"
        }
      ],
      "estimatedImplementationEffort": "1-2 weeks",
      "costImplications": "Minimal compute; scheduling overhead"
    },
    {
      "id": "sla-freshness-monitoring",
      "name": "SLA Freshness Monitoring",
      "domain": "Alerting, Automation, and Operational Intelligence",
      "domainId": 7,
      "category": "Operational Intelligence",
      "summary": "Monitoring pipeline SLAs ensuring data is refreshed within defined window (midnight refresh by 6am) with alerts on violations.",
      "description": "Gold layer must refresh by 6am. If refresh doesn't complete, alert ops team. Track SLA compliance month-over-month.",
      "fabricComponents": [
        "Data Factory",
        "Monitoring",
        "Alerting"
      ],
      "pros": [
        "Ensures users get fresh data on time.",
        "Quick detection of pipeline failures.",
        "SLA compliance visibility."
      ],
      "cons": [
        "SLA setup requires discipline.",
        "False alerts from planned maintenance.",
        "Debugging SLA misses complex."
      ],
      "usageInstructions": "1. Define SLA (e.g., refresh by 6am). 2. Monitor pipeline completion. 3. Alert if missed. 4. Track compliance %. 5. Post-mortem on misses.",
      "governanceConsiderations": "SLAs set realistically. Escalation procedures defined. Maintenance windows coordinated. Compliance reported.",
      "peopleAnalyticsUseCases": [
        "Gold layer refresh by 6am SLA monitoring.",
        "Payroll data SLA: reconcile by 9am.",
        "Recruiting pipeline SLA: updates daily.",
        "Executive dashboard SLA: available by 7am."
      ],
      "complexity": "Low",
      "maturity": "GA",
      "compatibleWith": [
        "medallion-architecture",
        "data-activator-reflex"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "sla",
        "freshness",
        "monitoring"
      ],
      "referenceLinks": [
        {
          "label": "Pipeline Monitoring",
          "url": "https://docs.microsoft.com/en-us/fabric/"
        }
      ],
      "estimatedImplementationEffort": "1-2 weeks",
      "costImplications": "Minimal overhead for monitoring"
    },
    {
      "id": "escalation-routing",
      "name": "Automated Escalation and Routing",
      "domain": "Alerting, Automation, and Operational Intelligence",
      "domainId": 7,
      "category": "Automation",
      "summary": "Smart routing of alerts to appropriate teams based on severity and data ownership (high turnover \u2192 dept head; data quality issue \u2192 data team).",
      "description": "High turnover alert goes to HR head. Data quality issue goes to data team. Ownership-based routing ensures right person acts.",
      "fabricComponents": [
        "Alerting",
        "Routing Engine",
        "Power Automate"
      ],
      "pros": [
        "Right person gets right alert.",
        "Reduces response time.",
        "Prevents alert fatigue."
      ],
      "cons": [
        "Routing logic becomes complex.",
        "Ownership must be maintained.",
        "Depends on accurate severity classification."
      ],
      "usageInstructions": "1. Define alert types. 2. Map to owners. 3. Set routing rules. 4. Configure notifications. 5. Test. 6. Monitor effectiveness.",
      "governanceConsiderations": "Ownership assignments maintained. Routing rules reviewed. Escalation paths clear. Response SLAs defined.",
      "peopleAnalyticsUseCases": [
        "High turnover \u2192 department head.",
        "Data quality issue \u2192 data steward.",
        "Salary anomaly \u2192 compensation manager.",
        "Compliance issue \u2192 HR legal."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "data-activator-reflex",
        "power-automate-triggers"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "escalation",
        "routing",
        "alerting"
      ],
      "referenceLinks": [
        {
          "label": "Routing Logic",
          "url": "https://docs.microsoft.com/en-us/power-automate/"
        }
      ],
      "estimatedImplementationEffort": "2-3 weeks",
      "costImplications": "Minimal overhead"
    },
    {
      "id": "cross-workspace-shortcuts",
      "name": "Cross-Workspace Data Sharing via Shortcuts",
      "domain": "Data Sharing and Distribution",
      "domainId": 8,
      "category": "Data Distribution",
      "summary": "Shortcuts enable different workspaces (HR, Finance, Recruiting) to share data without copying, maintaining single source of truth.",
      "description": "Finance workspace has authoritative employee master and cost centers. HR and Recruiting create shortcuts, always get latest data.",
      "fabricComponents": [
        "OneLake",
        "Shortcuts",
        "Workspaces"
      ],
      "pros": [
        "Single source of truth.",
        "Zero-copy sharing.",
        "Automatic updates visible."
      ],
      "cons": [
        "Cross-workspace latency.",
        "Complex ownership management.",
        "Lineage harder to track."
      ],
      "usageInstructions": "1. Source workspace has data. 2. Consumer workspace creates shortcut. 3. Reference shortcut like local table. 4. Monitor performance.",
      "governanceConsiderations": "Source table ownership clear. Data contracts documented. Shortcut access controlled. Performance monitored.",
      "peopleAnalyticsUseCases": [
        "Finance employee master shared to HR via shortcuts.",
        "Recruiting access cost center via Finance shortcut.",
        "All teams access central org structure."
      ],
      "complexity": "Low",
      "maturity": "GA",
      "compatibleWith": [
        "onelake-shortcuts",
        "hub-spoke-workspace"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "shortcuts",
        "cross-workspace",
        "sharing"
      ],
      "referenceLinks": [
        {
          "label": "Cross-Workspace Shortcuts",
          "url": "https://docs.microsoft.com/en-us/fabric/onelake/onelake-shortcuts"
        }
      ],
      "estimatedImplementationEffort": "1-2 days",
      "costImplications": "Zero-copy saves significantly"
    },
    {
      "id": "cross-tenant-sharing",
      "name": "Cross-Tenant Data Sharing (B2B Scenarios)",
      "domain": "Data Sharing and Distribution",
      "domainId": 8,
      "category": "Data Distribution",
      "summary": "Sharing data across Azure AD tenants enabling partner organizations to access shared analytics while maintaining security.",
      "description": "Parent company shares org structure and benchmark data with subsidiary. Subsidiary accesses via external shortcuts securely.",
      "fabricComponents": [
        "OneLake",
        "Shortcuts",
        "Tenants",
        "Azure AD"
      ],
      "pros": [
        "Enables partner collaboration.",
        "Maintains security across tenants.",
        "Zero-copy for cross-tenant data."
      ],
      "cons": [
        "Complex permission management.",
        "Cross-tenant latency.",
        "Requires Azure AD trust."
      ],
      "usageInstructions": "1. Source tenant grants access. 2. Target tenant creates shortcut. 3. Authenticate across tenants. 4. Reference shortcut.",
      "governanceConsiderations": "Explicit cross-tenant contracts. Legal agreements. Access regularly reviewed. Sensitive data restricted.",
      "peopleAnalyticsUseCases": [
        "Parent company shares benchmarks with subsidiary.",
        "Shared services org shares data with business units.",
        "JV shares hiring data with partners."
      ],
      "complexity": "High",
      "maturity": "Preview",
      "compatibleWith": [
        "onelake-shortcuts"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "cross-tenant",
        "b2b",
        "sharing"
      ],
      "referenceLinks": [
        {
          "label": "Tenant Collaboration",
          "url": "https://docs.microsoft.com/en-us/fabric/"
        }
      ],
      "estimatedImplementationEffort": "4-6 weeks",
      "costImplications": "Minimal overhead; security complexity"
    },
    {
      "id": "semantic-model-certification-pipeline",
      "name": "Semantic Model Certification Pipeline",
      "domain": "Data Sharing and Distribution",
      "domainId": 8,
      "category": "BI Governance",
      "summary": "Automated certification workflow where data teams publish semantic models and BI team verifies quality before marking Certified.",
      "description": "Data team publishes employee dimension model. BI team runs tests, checks metadata, certifies if passed. Analysts use certified models.",
      "fabricComponents": [
        "Semantic Model",
        "Power BI",
        "CI/CD",
        "Approval Workflow"
      ],
      "pros": [
        "Ensures semantic model quality.",
        "Reduces model duplication.",
        "Governance automation."
      ],
      "cons": [
        "Slows model time-to-value.",
        "Certification bottleneck.",
        "Requires clear criteria."
      ],
      "usageInstructions": "1. Data team publishes model. 2. Auto-run quality checks. 3. BI team reviews. 4. Approve/reject. 5. Publish as Certified. 6. Analysts use.",
      "governanceConsiderations": "Certification criteria clear. Review time SLA defined. Criteria version controlled. Regular audit of certified models.",
      "peopleAnalyticsUseCases": [
        "Employee dimension certification before BI use.",
        "Payroll semantic model certification.",
        "Org structure model quality gates.",
        "Metrics model certification."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "certified-semantic-model",
        "deployment-pipelines"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "semantic-model",
        "certification",
        "governance"
      ],
      "referenceLinks": [
        {
          "label": "Model Certification",
          "url": "https://docs.microsoft.com/en-us/power-bi/collaborate-share/service-certify-datasets"
        }
      ],
      "estimatedImplementationEffort": "2-3 weeks",
      "costImplications": "Minimal overhead; certification automation"
    },
    {
      "id": "api-exposure-management",
      "name": "REST API Exposure and Management",
      "domain": "Data Sharing and Distribution",
      "domainId": 8,
      "category": "Data Distribution",
      "summary": "Expose curated analytics data via REST APIs enabling external systems (HRIS, payroll, recruiting platforms) to consume Fabric data.",
      "description": "REST API endpoints for employee master, org structure, payroll data. External HRIS calls API to fetch org updates. Managed API throttling.",
      "fabricComponents": [
        "Warehouse",
        "SQL Endpoints",
        "API Management",
        "REST API"
      ],
      "pros": [
        "Programmatic data access.",
        "Enables external integrations.",
        "API governance and throttling.",
        "Reduces data replication."
      ],
      "cons": [
        "API security overhead.",
        "Performance tuning needed.",
        "Version management complexity."
      ],
      "usageInstructions": "1. Create API Management instance. 2. Expose Warehouse via API. 3. Define endpoints. 4. Set throttling. 5. Manage keys. 6. Monitor usage.",
      "governanceConsiderations": "API authentication required. Rate limiting enforced. Data access logged. Sensitive data restricted at API level.",
      "peopleAnalyticsUseCases": [
        "API for employee master to external HRIS.",
        "Org structure API to recruiting platforms.",
        "Payroll data API to finance systems.",
        "Compensation band API to offer letter system."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "lakehouse-warehouse-selection",
        "row-level-security"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "api",
        "rest",
        "data-distribution"
      ],
      "referenceLinks": [
        {
          "label": "API Management",
          "url": "https://docs.microsoft.com/en-us/azure/api-management/"
        }
      ],
      "estimatedImplementationEffort": "2-3 weeks",
      "costImplications": "API Management licensing plus query compute"
    },
    {
      "id": "dataset-subscription-alerting",
      "name": "Dataset Subscription and Change Alerts",
      "domain": "Data Sharing and Distribution",
      "domainId": 8,
      "category": "Data Distribution",
      "summary": "Subscribers receive alerts when datasets change or refresh, enabling downstream systems to react to data updates.",
      "description": "Recruiting platform subscribes to org structure changes. When org structure table updates, recruiting system receives alert, refetches data.",
      "fabricComponents": [
        "Event Grid",
        "Webhooks",
        "Lakehouse",
        "Subscribers"
      ],
      "pros": [
        "Push-based data distribution.",
        "Subscribers notified of changes.",
        "Reduces polling.",
        "Real-time integration."
      ],
      "cons": [
        "Webhook management overhead.",
        "Failure handling complexity.",
        "Debugging integration issues."
      ],
      "usageInstructions": "1. Configure Event Grid. 2. Define dataset change events. 3. Create webhooks for subscribers. 4. Subscribers listen. 5. Notify on change. 6. Handle failures.",
      "governanceConsiderations": "Subscription management. Event audit trail. Webhook security. Retry policies defined.",
      "peopleAnalyticsUseCases": [
        "Org structure change alerts to recruiting system.",
        "Employee master update alerts to payroll.",
        "Compensation change alerts to benefits system.",
        "Headcount change alerts to planning tools."
      ],
      "complexity": "Medium",
      "maturity": "Preview",
      "compatibleWith": [
        "medallion-architecture",
        "data-activator-reflex"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "subscriptions",
        "webhooks",
        "event-driven"
      ],
      "referenceLinks": [
        {
          "label": "Event Grid",
          "url": "https://docs.microsoft.com/en-us/azure/event-grid/overview"
        }
      ],
      "estimatedImplementationEffort": "2-3 weeks",
      "costImplications": "Event Grid pricing plus webhook compute"
    },
    {
      "id": "encryption-at-rest-cmk",
      "name": "Encryption at Rest with Customer-Managed Keys",
      "domain": "Data Governance and Security",
      "domainId": 3,
      "category": "Encryption",
      "summary": "Implements encryption at rest using customer-managed keys (CMK) stored in Azure Key Vault for OneLake workspace data. Ensures TLS 1.2+ for all data in transit and maintains FIPS 140-2 compliance for sensitive HR data.",
      "description": "Customer-Managed Key (CMK) encryption provides organizational control over encryption key lifecycle and rotation. In financial services HR analytics, CMK encryption via Azure Key Vault ensures that OneLake workspace data is encrypted with keys managed by your organization, not Microsoft. All data in transit uses TLS 1.2 or higher. This approach meets regulatory requirements for key custody, enables key rotation policies, supports audit logging for key access, and provides compliance with FIPS 140-2 standards for handling highly sensitive employee financial and personal data. Integration with Entra ID service principals allows role-based access control over decryption operations.",
      "fabricComponents": [
        "Azure Key Vault",
        "OneLake",
        "Fabric Workspace Settings",
        "Entra ID Service Principal"
      ],
      "pros": [
        "Provides organizational control over encryption keys with full audit trail of key access and rotations.",
        "Enables compliance with regulations requiring customer-managed encryption (SOX, GDPR, PIPEDA, HIPAA).",
        "Supports break-glass emergency key access protocols and disaster recovery scenarios."
      ],
      "cons": [
        "Increases operational complexity requiring key rotation and lifecycle management procedures.",
        "Key Vault service calls add latency to Fabric operations, typically <10ms but noticeable at scale.",
        "Requires careful IAM design to prevent accidental key lockout that could make data unrecoverable."
      ],
      "usageInstructions": "1. Create an Azure Key Vault resource in the same region as Fabric capacity. 2. Generate RSA 3072-bit or 4096-bit CMK in Key Vault. 3. Grant Fabric capacity system-assigned managed identity Key Wrap/Unwrap permissions on the key. 4. In Fabric Workspace Settings, select 'Customer Managed Key' and specify the Key Vault URI. 5. Configure key rotation policy (annual minimum). 6. Enable Key Vault audit logging and monitor access. 7. Test disaster recovery failover procedures quarterly.",
      "governanceConsiderations": "Establish a key management committee with business and security stakeholders. Document key rotation procedures and emergency access protocols. Implement access reviews quarterly for Key Vault permissions. Ensure backup keys are stored in a geographically separate region. Monitor failed decryption attempts and investigate anomalies. Integrate key rotation events into change management processes.",
      "peopleAnalyticsUseCases": [
        "Salary survey analysis where payroll data is encrypted with organization-controlled CMK, enabling internal audits while maintaining key custody.",
        "Executive compensation dashboards requiring SOX compliance where CMK encryption demonstrates regulatory control over sensitive executive personal data.",
        "International employee equity tracking where Canadian employee data uses Canadian Key Vault instance and US data uses US Key Vault for data residency compliance."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "sensitivity-labels",
        "purview-data-map",
        "dynamic-data-masking",
        "row-level-security"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "encryption",
        "security",
        "compliance",
        "key-management",
        "financial-services"
      ],
      "referenceLinks": [
        {
          "label": "Encryption at Rest in Microsoft Fabric",
          "url": "https://learn.microsoft.com/en-us/fabric/enterprise/encryption-at-rest"
        },
        {
          "label": "Azure Key Vault Overview",
          "url": "https://learn.microsoft.com/en-us/azure/key-vault/general/overview"
        },
        {
          "label": "FIPS 140-2 Compliance in Azure",
          "url": "https://learn.microsoft.com/en-us/compliance/regulatory/offering-fips-140-2"
        }
      ],
      "estimatedImplementationEffort": "2-3 weeks",
      "costImplications": "Azure Key Vault: $0.34/10k operations; CMK key storage $1/month. Key Vault calls add minimal overhead."
    },
    {
      "id": "dlp-policy-enforcement",
      "name": "Data Loss Prevention Policy Enforcement",
      "domain": "Data Governance and Security",
      "domainId": 3,
      "category": "Data Protection",
      "summary": "Configures Microsoft Purview Data Loss Prevention (DLP) policies to detect and block unauthorized export or sharing of sensitive information types including SSN, salary data, and bank account numbers.",
      "description": "Data Loss Prevention (DLP) policies in Microsoft Purview automatically detect sensitive information types within Fabric and Power BI, then enforce preventive or detective controls. For HR analytics, DLP policies identify patterns matching US Social Security Numbers, salary ranges, employee IDs, and bank account numbers. When detected, policies can block exports to CSV/Excel, prevent uploads to personal OneDrive, restrict sharing to external domains, require approval for sensitive queries, or log incidents to Azure Sentinel for SOC review. DLP works alongside sensitivity labels to provide multi-layered protection. Policies can be scoped to specific workspaces, datasets, or semantic models. Integration with Sensitivity Labels and Power BI Workspace settings ensures consistent enforcement across analytics surfaces.",
      "fabricComponents": [
        "Microsoft Purview DLP",
        "Sensitivity Labels",
        "Power BI",
        "OneLake"
      ],
      "pros": [
        "Detects sensitive data automatically using built-in and custom regex patterns without manual classification.",
        "Blocks unauthorized export at the point of action (export, copy, print) reducing insider threat risk.",
        "Provides audit trail of blocked/allowed actions and integrates with SIEM for incident response workflows."
      ],
      "cons": [
        "Can trigger false positives on benign data patterns, requiring regular tuning and user exceptions.",
        "DLP policies are primarily detective in Power BI; preventive enforcement requires specific workspace integration.",
        "Regex patterns for custom sensitive data types require security team expertise and ongoing maintenance."
      ],
      "usageInstructions": "1. Open Microsoft Purview Compliance Center and navigate to Data Loss Prevention > Policies. 2. Create new policy with template 'Financial Info' or 'PII' as baseline. 3. Add custom sensitive info types: SSN regex (^\\d{3}-\\d{2}-\\d{4}$), salary range (\\$[0-9]{3,4}[KM]). 4. Configure rule actions: 'Restrict access' for Power BI, 'Require justification' for exports, 'Send alert to admin'. 5. Set policy scope to HR workspace and datasets. 6. Enable incident reporting to Azure Sentinel. 7. Test policy with sample data before production rollout. 8. Review blocked incidents monthly.",
      "governanceConsiderations": "Establish a DLP governance committee with HR, security, and legal teams. Document all custom sensitive data type patterns and business justification. Implement exception request workflow with business and compliance approval. Monitor false positive rate and adjust thresholds quarterly. Ensure DLP incidents are correlated with user access logs and audit trails. Train HR analytics team on compliant export procedures.",
      "peopleAnalyticsUseCases": [
        "Automated blocking of salary equity analysis exports containing aggregated ranges to unauthorized recipients, while allowing HR team to export to approved HR department network shares.",
        "Detection and alert when employee benefit election data containing SSN is copied to clipboard or exported to personal email, triggering incident response investigation.",
        "Prevention of executive compensation dashboard drill-through exports to external consultants without pre-approval workflow, enforcing approval for sensitive roles."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "sensitivity-labels",
        "row-level-security",
        "dynamic-data-masking",
        "audit-siem-integration"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "sensitivity-labels"
      ],
      "tags": [
        "data-loss-prevention",
        "dlp",
        "compliance",
        "data-protection",
        "security"
      ],
      "referenceLinks": [
        {
          "label": "Data Loss Prevention in Microsoft Purview",
          "url": "https://learn.microsoft.com/en-us/purview/dlp-learn-about-dlp"
        },
        {
          "label": "DLP Policy Creation and Management",
          "url": "https://learn.microsoft.com/en-us/purview/create-test-tune-dlp-policy"
        },
        {
          "label": "Sensitive Information Types in Purview",
          "url": "https://learn.microsoft.com/en-us/purview/sensitive-information-type-learn-about"
        }
      ],
      "estimatedImplementationEffort": "3-4 weeks",
      "costImplications": "Included in Purview premium license (part of Microsoft 365 E5 or standalone). DLP policy evaluation adds <1% query latency."
    },
    {
      "id": "pii-tokenization",
      "name": "PII Tokenization and Pseudonymization",
      "domain": "Data Transformation and Processing",
      "domainId": 2,
      "category": "Privacy Engineering",
      "summary": "Replaces Personally Identifiable Information (PII) with deterministic tokens using Microsoft Presidio and PySpark, enabling data sharing and analytics without exposing actual personal data.",
      "description": "PII tokenization uses deterministic hashing to replace sensitive personal identifiers with tokens while maintaining the ability to link records. Microsoft Presidio, an AI-powered data protection service, automatically detects PII entities (names, addresses, phone numbers, emails) within notebooks and datasets. Identified PII is replaced with tokens, with the mapping stored securely in Azure Key Vault. For HR analytics, this enables sharing anonymized talent data with third-party vendors, consultants, and development teams without exposing actual employee identity. Deterministic tokenization (same PII always maps to same token) preserves join semantics. PySpark transformations within Fabric Notebooks perform tokenization at scale. Different token formats support different use cases: hash tokens for aggregate analytics, UUID tokens for record-level joins. Reverse lookup tokens require Key Vault authorization for decryption.",
      "fabricComponents": [
        "Fabric Notebooks",
        "PySpark",
        "Microsoft Presidio",
        "Azure Key Vault",
        "Lakehouse"
      ],
      "pros": [
        "Enables secure data sharing with business partners and vendors by removing actual identity without losing join capability.",
        "Detects PII automatically using AI models rather than relying on manual regex patterns, catching complex PII patterns.",
        "Supports different tokenization strategies per use case: hash-only tokens for analytics, reversible tokens for authorized users with Key Vault access."
      ],
      "cons": [
        "Deterministic tokenization vulnerable to frequency analysis if PII distribution is skewed (e.g., only 3 VP names).",
        "Presidio requires pre-trained models and tuning to avoid false positives/negatives in organization-specific contexts.",
        "Large-scale tokenization can impact notebook execution time; requires parallel PySpark processing for efficiency."
      ],
      "usageInstructions": "1. Install Presidio in Fabric Notebook: pip install presidio-analyzer presidio-anonymizer. 2. Load employee data in PySpark DataFrame. 3. Configure Presidio analyzer with entity patterns: PERSON, PHONE_NUMBER, EMAIL_ADDRESS. 4. Create tokenization mapping: use Presidio to detect PII, generate deterministic SHA-256 hash token or UUID. 5. Store mapping in Key Vault secret with access restricted to governance team. 6. Replace PII with tokens using Presidio anonymizer. 7. Save tokenized dataset to Silver/Gold layer. 8. Log all tokenization operations with user, timestamp, entity count. 9. Test reverse lookup process with authorized user.",
      "governanceConsiderations": "Establish PII tokenization review board with privacy, security, and HR stakeholders. Document which entity types are tokenized and which remain for legitimate analytics. Implement access controls requiring Key Vault authorization for reverse lookups. Monitor tokenization logs for suspicious decryption patterns. Conduct quarterly audits of Presidio configuration to ensure no drift in entity detection. Maintain separate tokenization keys per data consumer or use case.",
      "peopleAnalyticsUseCases": [
        "Sharing anonymized employee experience survey data with external consulting firm to analyze cultural trends without exposing employee names, emails, or departments.",
        "Enabling development team to use production-like employee hierarchy and org structure data for testing without accessing real employee identities, using UUID tokens in place of names.",
        "Providing recruitment analytics to third-party recruitment platform showing skills and experience patterns without revealing actual employee identity, enabling vendor to match internal talent to opportunities."
      ],
      "complexity": "High",
      "maturity": "Emerging",
      "compatibleWith": [
        "medallion-architecture",
        "spark-notebook-etl",
        "data-quality-validation",
        "dlp-policy-enforcement"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "pii",
        "anonymization",
        "privacy",
        "data-sharing",
        "presidio"
      ],
      "referenceLinks": [
        {
          "label": "Microsoft Presidio Overview",
          "url": "https://learn.microsoft.com/en-us/presidio/"
        },
        {
          "label": "Presidio with Fabric Notebooks",
          "url": "https://github.com/microsoft/presidio"
        },
        {
          "label": "Data Anonymization Techniques",
          "url": "https://learn.microsoft.com/en-us/compliance/regulatory/gdpr-data-subject-rights"
        }
      ],
      "estimatedImplementationEffort": "4-6 weeks",
      "costImplications": "Presidio: open-source, no licensing cost. Key Vault storage: minimal. Spark compute: standard Fabric notebook rates."
    },
    {
      "id": "data-retention-lifecycle",
      "name": "Automated Data Retention and Purge Pipeline",
      "domain": "Data Transformation and Processing",
      "domainId": 2,
      "category": "Lifecycle Management",
      "summary": "Implements scheduled automated pipelines that evaluate data age against retention rules, execute secure deletion with audit trails, and support legal hold overrides for compliance.",
      "description": "Data retention and purge pipelines ensure that employee data is not retained longer than required by law or business policy. For HR analytics, employee records must be retained per employment law (typically 3-7 years depending on jurisdiction), then securely deleted. Automated pipelines check record creation/modification dates against configurable retention policies, identify data eligible for purge, execute secure deletion via soft-delete flags or physical removal, and generate audit logs. Legal holds prevent deletion during litigation or regulatory investigations. Data Factory pipelines coordinate with Fabric notebooks to identify eligible records using SQL queries (SELECT * WHERE modified_date < DATEADD(year,-3,GETDATE())), mark for deletion, execute deletion operations, and log to audit tables. Soft-delete approach (marking records inactive) preserves referential integrity and enables recovery. Hard-delete (physical removal from Delta Lake) reduces storage and improves query performance but requires careful execution.",
      "fabricComponents": [
        "Data Factory Pipeline",
        "Fabric Notebooks",
        "OneLake",
        "Delta Lake",
        "Lakehouse"
      ],
      "pros": [
        "Automates compliance with data retention regulations (GDPR right-to-erasure, employment law, SOX records retention).",
        "Reduces storage costs by removing obsolete data and improves query performance on smaller active datasets.",
        "Maintains complete audit trail of deleted records and reasons, supporting forensic investigations and compliance audits."
      ],
      "cons": [
        "Incorrectly configured retention rules can cause accidental data loss; requires thorough testing and change management approval.",
        "Soft-delete approaches maintain referential integrity but require query filters to exclude deleted records; hard-delete is faster but riskier.",
        "Legal holds must be tracked separately; coordination between legal, HR, and data teams is required to manage hold status."
      ],
      "usageInstructions": "1. Define retention policies in configuration table: entity type, retention period (years), jurisdiction, legal hold indicator. 2. Create Data Factory pipeline with Copy Activity to identify records eligible for purge using SQL: SELECT * WHERE datediff(year, modified_date, getdate()) >= retention_years AND legal_hold = 0. 3. Use Fabric Notebook to mark records as deleted (UPDATE table SET is_deleted = 1 WHERE id IN (...)) instead of hard delete initially. 4. Execute soft-delete daily/weekly per schedule. 5. After 30-day recovery period, run hard delete: VACUUM table_name RETAIN 0 HOURS; DELETE FROM table WHERE is_deleted = 1. 6. Log all operations: user, timestamp, record count, reasons. 7. Send audit report to compliance/legal team monthly.",
      "governanceConsiderations": "Establish data retention committee with legal, HR, compliance, and data teams. Document retention policies for each data entity type by jurisdiction. Implement approval workflow for legal hold changes. Set up audit alerts for large purge operations. Conduct quarterly audits of purge logs to verify correctness. Test disaster recovery to ensure deleted data cannot be recovered from backups without explicit authorization. Maintain retention policy version control.",
      "peopleAnalyticsUseCases": [
        "Automatically delete contingent worker records 2 years after separation date and contract end, enabling compliance with employment laws while reducing data footprint.",
        "Purge applicant data after hiring decision per FCRA requirements (typically 3 years), except for records under litigation hold from employment disputes.",
        "Soft-delete employee benefit elections from terminated employees after 7 years but preserve audit trail for pension calculations and benefit inquiries, with legal hold preventing deletion during claims."
      ],
      "complexity": "High",
      "maturity": "Emerging",
      "compatibleWith": [
        "medallion-architecture",
        "spark-notebook-etl",
        "delta-lake-partitioning",
        "audit-siem-integration"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "medallion-architecture"
      ],
      "tags": [
        "retention",
        "lifecycle",
        "compliance",
        "gdpr",
        "data-deletion"
      ],
      "referenceLinks": [
        {
          "label": "Azure Data Factory Scheduling and Triggers",
          "url": "https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers"
        },
        {
          "label": "GDPR Data Retention and Right to Erasure",
          "url": "https://learn.microsoft.com/en-us/compliance/regulatory/gdpr-data-subject-rights"
        },
        {
          "label": "Delta Lake VACUUM and DELETE Operations",
          "url": "https://learn.microsoft.com/en-us/fabric/onelake/delta-lake-overview"
        }
      ],
      "estimatedImplementationEffort": "5-7 weeks",
      "costImplications": "Data Factory pipeline runs: $0.50 per execution. Fabric compute for Notebook purge jobs: standard rates. Storage savings from deletion: 10-30% reduction in employee data volume."
    },
    {
      "id": "anonymization-k-anonymity",
      "name": "Statistical Anonymization for HR Analytics",
      "domain": "Data Governance and Security",
      "domainId": 3,
      "category": "Privacy Engineering",
      "summary": "Implements k-anonymity and differential privacy techniques to ensure published HR analytics cannot identify individuals through aggregation attacks or frequency analysis.",
      "description": "K-anonymity ensures that any combination of quasi-identifiers (age, department, salary band) appears in at least k records, preventing record linkage attacks. Differential privacy adds carefully calibrated noise to query results, enabling accurate aggregate statistics while preventing inference of individual values. For HR analytics, aggregation-only views enforce k-anonymity by requiring HAVING COUNT(*) >= 5 clauses on all queries, preventing disclosure of rare populations (e.g., the sole executive in a location). Noise injection adds Laplace or Gaussian noise proportional to sensitivity, with epsilon (privacy budget) controlling noise magnitude. Views in Fabric Warehouse or SQL Analytics Endpoint implement these controls. PySpark notebooks implement noise injection for ad-hoc analyses. Practical applications include publishing salary bands by role (not individuals), showing headcount by department (minimum 5 per group), and publishing engagement scores by location-function groups.",
      "fabricComponents": [
        "Fabric Warehouse",
        "SQL Analytics Endpoint",
        "PySpark Notebooks",
        "Lakehouse"
      ],
      "pros": [
        "Provides formal mathematical guarantees against re-identification attacks, meeting GDPR and other privacy regulations' proportionality requirements.",
        "Enables publishing aggregate HR analytics to broad audiences (all employees, board, public) without risk of individual inference.",
        "Can be implemented as database views and functions, integrating seamlessly into existing analytics without changing consuming applications."
      ],
      "cons": [
        "K-anonymity vulnerable to semantic attacks if quasi-identifiers are not carefully chosen; requires domain expertise to identify all identifying combinations.",
        "Noise injection reduces statistical utility; high privacy budgets (epsilon) may allow inference while low budgets (epsilon <0.1) add significant noise.",
        "Maintaining k-anonymity becomes harder as dataset grows and rare populations appear; may force conservative suppression of legitimate insights."
      ],
      "usageInstructions": "1. Identify quasi-identifiers in HR data (age, salary band, department, location, tenure, role). 2. Create aggregation-only views with HAVING COUNT(*) >= 5. 3. Test for k=5 anonymity: SELECT department, role, COUNT(*) FROM employees GROUP BY department, role HAVING COUNT(*) >= 5. 4. For salary analytics, implement noise injection in Python: create base aggregate (SELECT department, avg(salary) as avg_sal FROM employees GROUP BY department), calculate sensitivity (max salary difference), generate Laplace noise with epsilon=0.5, add noise to aggregates. 5. Create security principal with view-only permissions to anonymized views. 6. Publish anonymized views via Power BI semantic model with field-level security. 7. Document epsilon budgets and k values for each published analysis.",
      "governanceConsiderations": "Establish privacy analytics working group with data scientists, privacy officers, and HR stakeholders. Document quasi-identifier sets and privacy threat model per analysis. Set epsilon budgets based on acceptable privacy-utility tradeoff. Review all published aggregates for k-anonymity before release. Track total epsilon consumption across all published analyses to prevent privacy budget exhaustion. Implement technical controls preventing ad-hoc SQL query access to raw employee data.",
      "peopleAnalyticsUseCases": [
        "Publishing company-wide salary equity analysis to all employees showing average salary by department-level-location, with k=10 minimum group size, enabling transparency without revealing individual salaries.",
        "Generating anonymized performance distribution histograms for board review with noise injection ensuring no distribution shape discloses individual outliers.",
        "Creating recruitment funnel analytics published externally showing aggregate conversion rates by source-role with k=5 minimums, preventing inference of individual candidate outcomes."
      ],
      "complexity": "High",
      "maturity": "Emerging",
      "compatibleWith": [
        "medallion-architecture",
        "row-level-security",
        "dynamic-data-masking",
        "purview-data-map"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "medallion-architecture",
        "row-level-security"
      ],
      "tags": [
        "anonymization",
        "k-anonymity",
        "differential-privacy",
        "privacy",
        "compliance"
      ],
      "referenceLinks": [
        {
          "label": "K-Anonymity and Privacy Techniques",
          "url": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/privacy-techniques"
        },
        {
          "label": "Differential Privacy in Azure",
          "url": "https://learn.microsoft.com/en-us/purview/compliance-privacy-basics"
        },
        {
          "label": "GDPR Data Minimization Principles",
          "url": "https://learn.microsoft.com/en-us/compliance/regulatory/gdpr-data-minimization"
        }
      ],
      "estimatedImplementationEffort": "6-8 weeks",
      "costImplications": "No licensing cost; implemented in Fabric Warehouse at standard query rates. Noise injection PySpark notebooks at standard compute rates."
    },
    {
      "id": "audit-siem-integration",
      "name": "Audit Log Export and SIEM Integration",
      "domain": "Data Governance and Security",
      "domainId": 3,
      "category": "Monitoring",
      "summary": "Forwards Fabric activity logs and Power BI audit logs to Azure Sentinel for centralized SOC monitoring, anomaly detection, and forensic investigation.",
      "description": "Audit log integration ensures all Fabric and Power BI activities (data access, semantic model changes, sharing, export, deletion) are captured, centralized, and analyzed for security incidents. Fabric activity logs (workspace creation, item access, refresh execution, query patterns) flow to Log Analytics Workspace. Power BI activity logs (report view, dataset refresh, sharing changes, export) are forwarded to the Unified Audit Log via the Office 365 Management API. Azure Sentinel consumes these logs, applies correlation rules to detect anomalies (bulk data export, after-hours access, privilege escalation), enriches with identity and threat intelligence, and triggers incidents for SOC investigation. Custom KQL (Kusto Query Language) queries detect HR-specific threats: unusual bulk report exports, access to sensitive employee data by non-HR users, policy violation patterns. Retention ensures logs are available for 1-2 years for forensic analysis and compliance audits.",
      "fabricComponents": [
        "Azure Sentinel",
        "Log Analytics Workspace",
        "Power BI Activity Log",
        "Unified Audit Log",
        "Azure Monitor"
      ],
      "pros": [
        "Centralizes audit logs from Fabric and Power BI in single searchable repository enabling cross-system correlation and forensic investigations.",
        "Detects insider threats and compliance violations in real-time through rule-based alerting and anomaly detection, enabling rapid response.",
        "Provides evidence trail for regulatory audits and incident response, meeting SOX, HIPAA, and other audit requirements."
      ],
      "cons": [
        "Azure Sentinel ingestion costs scale with log volume; busy Fabric environments can generate 1000+ GB/month of logs, increasing licensing costs.",
        "Lag between activity and Sentinel processing (typically 1-5 minutes) means real-time detection is limited; historical detection takes longer.",
        "False positive rates in correlation rules can overwhelm SOC; requires significant tuning and stakeholder coordination."
      ],
      "usageInstructions": "1. Create Log Analytics Workspace in Azure. 2. Enable Fabric activity logging: Workspace Settings > Audit and Compliance > Enable Activity Logging. 3. Create Data Connector in Log Analytics: Fabric Activity > Diagnostic Settings > Send to Log Analytics Workspace. 4. Enable Power BI audit logging: Power BI Admin Portal > Audit and Compliance > Turn on audit log search. 5. Configure Office 365 Management API in Sentinel to consume Unified Audit Log. 6. In Azure Sentinel, create Data Connectors for Office 365 and Log Analytics. 7. Create KQL detection rules: query for bulk exports (>1000 rows), access to SSN columns, after-hours queries. 8. Configure Incidents from detections with SOAR playbook triggers. 9. Create Power BI dashboard for audit KPIs: daily active users, top data consumers, failed access attempts.",
      "governanceConsiderations": "Establish Security Operations Center (SOC) workflow for Sentinel incidents related to HR data. Define escalation procedures for confirmed security incidents (e.g., data exfiltration). Implement log retention policies: 6 months hot storage, 2 years cold archive. Conduct quarterly reviews of detection rules to reduce false positives. Require formal change management approval for queries accessing sensitive audit data. Ensure audit log access is restricted to security team and select compliance officers.",
      "peopleAnalyticsUseCases": [
        "Real-time detection of bulk export of executive compensation reports (>100 rows) to external email domains, triggering SOC investigation into potential salary data exfiltration.",
        "Anomaly detection identifying that a non-HR user (e.g., IT service account) is accessing employee SSN and salary columns during off-hours, indicating potential credential compromise.",
        "Forensic investigation of a departing manager's last-day activities: queried employee performance data, shared compensation history report with personal email, then deleted workspace link."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "row-level-security",
        "workspace-permission-governance",
        "dlp-policy-enforcement",
        "privileged-access-management"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "audit",
        "siem",
        "sentinel",
        "monitoring",
        "security"
      ],
      "referenceLinks": [
        {
          "label": "Fabric Activity Logging and Monitoring",
          "url": "https://learn.microsoft.com/en-us/fabric/admin/monitoring-workspace"
        },
        {
          "label": "Azure Sentinel Overview",
          "url": "https://learn.microsoft.com/en-us/azure/sentinel/overview"
        },
        {
          "label": "Power BI Audit Logging",
          "url": "https://learn.microsoft.com/en-us/power-bi/admin/service-admin-auditing"
        }
      ],
      "estimatedImplementationEffort": "3-4 weeks",
      "costImplications": "Log Analytics Workspace: $0.70/GB ingested. Azure Sentinel: $100+ per GB for first 100GB/day. Detection rule tuning requires SOC analyst time."
    },
    {
      "id": "dsr-fulfillment",
      "name": "Data Subject Request Fulfillment Pipeline",
      "domain": "Data Governance and Security",
      "domainId": 3,
      "category": "Privacy Compliance",
      "summary": "Automates GDPR/PIPEDA data subject rights (access and erasure) with workflows using Purview data lineage, soft-delete mechanisms, and verification processes.",
      "description": "Data Subject Requests (DSR) are formal legal requests from individuals to access (GDPR Article 15) or erase (GDPR Article 17, right-to-be-forgotten) their personal data. Manual DSR handling is error-prone, slow, and costly. Automated DSR fulfillment pipelines use Microsoft Purview Data Map to identify all systems containing an individual's data, orchestrate retrieval or deletion via Data Factory, perform soft-delete in Fabric Lakehouse with audit trail, notify requestor, and verify completion. For HR analytics, DSR requests require identifying employee records across Lakehouse, data warehouse, Power BI datasets, and archived systems. Soft-delete (marking records inactive) enables 30-day appeal period before hard-delete. Lineage mapping shows which analytics and reports are impacted by erasure. Workflow includes validation steps preventing accidental bulk deletion.",
      "fabricComponents": [
        "Microsoft Purview Data Map",
        "Data Factory Pipeline",
        "OneLake",
        "Fabric Notebooks",
        "Lakehouse"
      ],
      "pros": [
        "Automates time-consuming manual search and deletion, reducing DSR fulfillment time from weeks to days and reducing legal/compliance costs.",
        "Uses Purview lineage to ensure complete erasure across all systems; manual processes frequently miss copies or derived data.",
        "Provides audit trail proving compliance with GDPR/PIPEDA deadlines (30 days for access, 30 days for erasure), supporting regulatory defense."
      ],
      "cons": [
        "Incomplete or inaccurate Purview Data Map can result in missed data systems and non-compliance; requires continuous curation.",
        "Soft-delete relies on application logic filtering deleted records; risk of query bugs exposing deleted data if filters are incorrect.",
        "Determining which analytic derivatives (e.g., aggregates, models, exports) must be updated/deleted is complex and data-dependent."
      ],
      "usageInstructions": "1. Create DSR request intake form (SharePoint form or Power Apps) capturing: requestor name, email, request type (access/erasure), date. 2. Create Data Factory pipeline: Step 1 - Query Purview Data Map API for assets containing individual's data (e.g., Employee.EmployeeId = 12345). Step 2 - Generate list of impacted systems/tables. Step 3 - For access requests, export data to encrypted file in secure SharePoint. For erasure requests, mark records with is_dsr_deleted=1, log deletion timestamp/reason. Step 4 - Wait 30 days for appeals. Step 5 - Hard-delete (VACUUM, UPDATE DELETE). Step 6 - Notify requestor and compliance team. 7. Implement controls: require approvals for bulk erasure, audit all DSR operations, verify deletion completeness.",
      "governanceConsiderations": "Establish DSR response team with legal, HR, privacy, and data teams. Document DSR procedures and compliance timelines per jurisdiction. Implement technical access controls preventing unauthorized DSR request manipulation. Maintain DSR request audit log with decision and verification steps. Conduct quarterly audits to verify soft-deleted records are truly inaccessible to analytics. Maintain appeal process for 30-day grace period post-deletion request.",
      "peopleAnalyticsUseCases": [
        "Automated fulfillment of GDPR access request from Canadian employee: Purview identifies data in Lakehouse employee table, warehouse HRIS system, Power BI reports. Pipeline exports to encrypted PDF in 5 days, within GDPR 30-day SLA.",
        "Right-to-erasure request from terminated EU employee: soft-delete removes from all active tables, historical audit trails remain but employee salary/SSN are masked from all analytics, 30-day appeal period prevents accidental permanent deletion.",
        "Cross-border DSR for US employee working in EU office: pipeline identifies data in US Lake House capacity (compliant with GDPR data residency because US subject to standard contractual clauses), executes soft-delete with audit trail."
      ],
      "complexity": "High",
      "maturity": "Emerging",
      "compatibleWith": [
        "purview-data-map",
        "medallion-architecture",
        "audit-siem-integration"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "purview-data-map",
        "medallion-architecture"
      ],
      "tags": [
        "gdpr",
        "pipeda",
        "dsr",
        "privacy",
        "right-to-erasure"
      ],
      "referenceLinks": [
        {
          "label": "GDPR Data Subject Rights",
          "url": "https://learn.microsoft.com/en-us/compliance/regulatory/gdpr-data-subject-rights"
        },
        {
          "label": "Microsoft Purview Data Map",
          "url": "https://learn.microsoft.com/en-us/purview/purview-data-catalog"
        },
        {
          "label": "Data Subject Request Management",
          "url": "https://learn.microsoft.com/en-us/compliance/regulatory/manage-gdpr-data-subject-requests-summary"
        }
      ],
      "estimatedImplementationEffort": "6-8 weeks",
      "costImplications": "Purview Data Map: included in premium licensing. Data Factory DSR pipelines: $0.50 per execution. Estimated 100-200 DSR requests/year at $50 cost to process each."
    },
    {
      "id": "disaster-recovery-geo",
      "name": "Disaster Recovery and Geo-Redundancy",
      "domain": "Data Sharing and Distribution",
      "domainId": 8,
      "category": "Business Continuity",
      "summary": "Configures OneLake with zone-redundant storage (ZRS) and geo-replication to paired Azure region, with manual failover procedures and multi-geo capacity for data residency.",
      "description": "Disaster recovery ensures business continuity when regional outages occur. OneLake Zone-Redundant Storage (ZRS) replicates data across three availability zones within a region, protecting against zone-level failures. Geo-replication asynchronously copies data to a paired region (e.g., US East 2 to US Central 1), protecting against regional disasters. Fabric Capacity can be provisioned in multiple regions to enable manual failover. For HR analytics, disaster recovery ensures employee data is always available and compliant with data residency laws (e.g., Canadian data stays in Canada region). Multi-geo capacity configuration enables capacity to be deployed in specific regions. Backup procedures include daily snapshots exported to secure storage. Manual failover involves updating connection strings and repointing workspaces to failover capacity in alternate region. Recovery Time Objective (RTO) for manual failover is 4-8 hours; automated failover via service mesh future capability targets RTO <15 minutes.",
      "fabricComponents": [
        "OneLake",
        "Fabric Capacity",
        "Azure Region Pairs",
        "Zone-Redundant Storage"
      ],
      "pros": [
        "ZRS provides automatic replication without application changes, protecting against zone and data center failures with no additional cost.",
        "Geo-replication enables regional compliance (e.g., Canadian data in Canada) and disaster recovery to another region, meeting business continuity SLAs.",
        "Manual failover procedures are tested and documented, enabling rapid recovery with minimal training."
      ],
      "cons": [
        "Geo-replication introduces asynchronous delay (minutes to hours); recent data changes may not be replicated if failover occurs immediately after write.",
        "Manual failover requires human action and coordination, typically taking 4-8 hours; automated failover is not yet available.",
        "Multi-region Fabric Capacity costs increase significantly; maintaining hot-hot failover doubles capacity costs vs. cold standby."
      ],
      "usageInstructions": "1. Enable ZRS on OneLake workspace: Workspace Settings > Storage Redundancy > Zone-Redundant Storage. 2. Configure geo-replication: Workspace Settings > Disaster Recovery > Enable Geo-Replication to paired region (e.g., East 2 to Central 1). 3. Create Data Factory pipeline for daily snapshot export: copy all Lakehouse tables to Azure Storage Account in alternate region. 4. Document failover runbook: 1) Declare disaster, 2) Notify stakeholders, 3) Verify failover region readiness, 4) Update Fabric workspace connection to failover capacity, 5) Repoint Power BI datasets to failover semantic models, 6) Validate data integrity in failover region. 5. Conduct quarterly disaster recovery drill: simulate regional failure, execute failover, validate analytics in alternate region, measure RTO. 6. Maintain failover capacity in standby mode or scale down when not in use.",
      "governanceConsiderations": "Establish disaster recovery committee with IT, business continuity, and HR stakeholders. Define RTO (Recovery Time Objective) and RPO (Recovery Point Objective) per service level agreement. Document failover procedures and roles. Test failover procedures quarterly with full team participation. Monitor geo-replication lag and alert if lag exceeds SLA. Maintain separate credentials for failover region. Ensure external audit of disaster recovery controls annually.",
      "peopleAnalyticsUseCases": [
        "Regional outage in US East region: automatic ZRS failover within same region (1-2 minutes). If both East zones unavailable, manual failover to US Central capacity (4 hours RTO) ensures payroll analytics continue.",
        "Compliance requirement for Canadian employee data to remain in Canada: Canadian Fabric Capacity with geo-replication to alternate Canadian region, ensuring data never leaves Canada even during failover.",
        "Financial services regulatory requirement for near-zero RPO: async geo-replication has 30-minute lag, so daily snapshot export to alternate region ensures RPO <24 hours."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "medallion-architecture",
        "encryption-at-rest-cmk",
        "audit-siem-integration"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "disaster-recovery",
        "business-continuity",
        "geo-redundancy",
        "zrs",
        "failover"
      ],
      "referenceLinks": [
        {
          "label": "OneLake Disaster Recovery and Geo-Redundancy",
          "url": "https://learn.microsoft.com/en-us/fabric/onelake/redundancy"
        },
        {
          "label": "Azure Region Pairs for Business Continuity",
          "url": "https://learn.microsoft.com/en-us/azure/reliability/cross-region-replication-azure"
        },
        {
          "label": "Fabric Capacity Provisioning and Failover",
          "url": "https://learn.microsoft.com/en-us/fabric/admin/capacity-admin"
        }
      ],
      "estimatedImplementationEffort": "4-6 weeks",
      "costImplications": "ZRS: no additional cost. Geo-replication: $0.02/GB/month for replication. Failover capacity: $16/hour standby, $96/hour active (Premium P1)."
    },
    {
      "id": "network-isolation-private-links",
      "name": "Network Isolation with Private Endpoints",
      "domain": "Data Governance and Security",
      "domainId": 3,
      "category": "Network Security",
      "summary": "Implements Azure Private Endpoints for Fabric Capacity and managed private endpoints for Spark/compute, ensuring traffic flows only over private VNet and preventing internet exposure.",
      "description": "Private Endpoints eliminate internet routes for Fabric services, instead routing all traffic through Azure VNet private network. Fabric Capacity (compute and storage) uses Private Endpoints to ensure no data traverses public internet. Managed Private Endpoints for Spark Compute enable secure communication with private data sources (SQL Database in VNet, storage accounts with firewall enabled). For HR analytics in regulated environments (financial services, healthcare), network isolation ensures employee data never transits public internet. Network security groups (NSGs) can further restrict traffic by source/destination IP. VNet integration enables hybrid connectivity: on-premises HR systems can securely connect to Fabric via ExpressRoute. Outbound traffic can be forced through proxy/firewall for inspection. Azure Private Link services provide 99.99% availability.",
      "fabricComponents": [
        "Azure Private Link",
        "Azure VNet",
        "Managed Private Endpoints",
        "Fabric Capacity"
      ],
      "pros": [
        "Eliminates internet exposure of Fabric endpoints, significantly reducing attack surface and complying with network isolation requirements.",
        "Enables hybrid connectivity to on-premises systems without VPN, improving performance and security.",
        "Provides network-layer segmentation complementing identity and data-layer security controls, following defense-in-depth principle."
      ],
      "cons": [
        "Private Endpoints add operational complexity: VNet design, routing, DNS configuration, and troubleshooting require network expertise.",
        "Cost increases: Fabric Capacity pricing unchanged but Private Endpoints charge $0.50/hour per endpoint, adding $360/month per capacity.",
        "Hybrid connectivity requires ExpressRoute or Site-to-Site VPN; on-premises connectivity adds complexity and latency."
      ],
      "usageInstructions": "1. Create Azure VNet in the same region as Fabric Capacity. 2. In Fabric Workspace Settings, enable Private Endpoints: select Capacity > Network > Enable Private Endpoint. 3. Azure creates Private Link Service and provides Private Endpoint Connection. 4. In your VNet, create Private Endpoint resource: +Create > Private Endpoint, select Fabric service, select subnet, configure DNS integration. 5. Update DNS resolver to map fabric.microsoft.com to private IP (e.g., 10.0.0.5). 6. Test connectivity: connect VM in VNet, query nslookup fabric.microsoft.com (should resolve to private IP). 7. Configure Managed Private Endpoints for Spark: Workspace > Managed Private Endpoints > +New, select target resource (SQL Database, storage), approve in target resource. 8. Configure Network Security Group (NSG): allow only VNet subnets to reach Fabric Private Endpoint, deny internet-routed traffic.",
      "governanceConsiderations": "Establish network security architecture review with IT security and compliance teams. Document VNet design and Private Endpoint locations. Implement bastion hosts for secure VM access instead of public IPs. Enforce NSG rules preventing traffic to public internet. Monitor Private Endpoint connections and deny suspicious outbound attempts. Require change management approval for VNet/NSG changes. Conduct quarterly network segmentation audits.",
      "peopleAnalyticsUseCases": [
        "Financial services firm isolates HR analytics Fabric Capacity on private VNet, restricts outbound traffic to approved SIEM sink via proxy, ensuring no employee data transits internet.",
        "Healthcare provider connects on-premises HRIS system via ExpressRoute to Fabric over private endpoint, enabling analytics without routing PII through public internet or VPN.",
        "Multi-tenant SaaS provider isolates each customer's Fabric Capacity on separate VNet, using managed private endpoints to connect to customer-specific SQL databases on-premises."
      ],
      "complexity": "High",
      "maturity": "GA",
      "compatibleWith": [
        "encryption-at-rest-cmk",
        "workspace-permission-governance",
        "audit-siem-integration"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "network-security",
        "private-endpoints",
        "vnet",
        "isolation",
        "compliance"
      ],
      "referenceLinks": [
        {
          "label": "Azure Private Endpoints for Fabric",
          "url": "https://learn.microsoft.com/en-us/fabric/security/security-private-endpoints-overview"
        },
        {
          "label": "Azure VNet and NSG Configuration",
          "url": "https://learn.microsoft.com/en-us/azure/virtual-network/manage-network-security-group"
        },
        {
          "label": "Managed Private Endpoints for Spark",
          "url": "https://learn.microsoft.com/en-us/fabric/security/security-managed-private-endpoints-overview"
        }
      ],
      "estimatedImplementationEffort": "4-6 weeks",
      "costImplications": "Private Endpoint: $0.50/hour ($360/month per endpoint). VNet/NSG: no cost. ExpressRoute (if used): $0.30-0.50/hour depending on bandwidth."
    },
    {
      "id": "privileged-access-management",
      "name": "Just-in-Time Privileged Access Management",
      "domain": "Data Governance and Security",
      "domainId": 3,
      "category": "Identity Security",
      "summary": "Implements Azure Entra ID Privileged Identity Management (PIM) for just-in-time elevation of workspace admin roles, with approval workflows and MFA enforcement.",
      "description": "Privileged Identity Management (PIM) reduces standing permissions, requiring users to request temporary elevation to admin roles. For HR analytics, workspace admins manage data governance policies, sharing, and sensitive configurations. Rather than making users permanent admins (standing privilege), PIM makes them 'Eligible Admins' who must request activation. Requests require approval from compliance officer and MFA authentication. Activation is time-limited (1-8 hours) and audited. Break-glass emergency accounts provide fallback access if primary admins are unavailable. Multi-factor authentication (MFA) is required for all privileged actions. Audit logs in Entra ID capture all activation requests, approvals, and actions performed during elevated access. For compliance, PIM demonstrates least-privilege principle and immediate detection of unauthorized privilege escalation attempts.",
      "fabricComponents": [
        "Azure Entra ID PIM",
        "Fabric Workspace Roles",
        "Azure Key Vault",
        "Azure Sentinel"
      ],
      "pros": [
        "Reduces standing privilege, dramatically lowering risk of compromise: admin account hack affects only active activation window (1-8 hours) not 365 days.",
        "Requires approval for every elevation, creating human checkpoint preventing unauthorized access.",
        "Complete audit trail of privilege escalation enables detection of unusual elevation patterns and supports compliance audits."
      ],
      "cons": [
        "PIM adds friction to emergency access scenarios, requiring approval workflow adds 15-30 minutes to incident response.",
        "Admin users must have Entra ID Premium P2 license, adding cost ~$30-50/user/month.",
        "Misconfigured break-glass accounts (credentials leaked, not rotated) undermine PIM's security benefits."
      ],
      "usageInstructions": "1. Ensure Entra ID Premium P2 licensed. 2. In Entra ID > Privileged Identity Management > Fabric Workspace Roles, select 'Workspace Admin' role. 3. Set eligible users: add workspace managers as Eligible (not Permanent) members. 4. Configure activation: require approval, require MFA, set max activation duration to 4 hours. 5. Select approval delegator: compliance officer or security team. 6. Configure notifications: alert on elevations, email approvers. 7. Create break-glass account: dedicate account for emergency access, store credentials in physical safe (not digital). 8. Test activation: Eligible Admin requests activation, approver receives email, activates with MFA. 9. Monitor in Azure Sentinel: create alert for approval denials, unusual activation times, after-hours elevations. 10. Quarterly review: audit PIM logs, revoke unused eligible access.",
      "governanceConsiderations": "Establish PIM governance committee with security, HR, and compliance stakeholders. Document approval process and SLA for activation requests (should be <15 minutes). Implement technical controls enforcing MFA (hardware keys preferred). Rotate break-glass credentials quarterly with multiple stakeholders witnessing. Test break-glass emergency access annually. Review PIM logs monthly for suspicious patterns. Enforce organization-wide policy: no permanent privileged roles, all admins must use PIM.",
      "peopleAnalyticsUseCases": [
        "HR analytics workspace admin (normally ineligible) requests elevation to investigate unauthorized data export, approver grants 4-hour activation via MFA approval, admin investigates access logs with limited time window.",
        "Data governance officer regularly activates to review sensitivity label assignments and RLS policies, must approve each activation with business justification.",
        "Emergency incident: production Fabric capacity is down, break-glass account holder activates with MFA, gains temporary admin access to restart services, audited and revoked within 1 hour."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "workspace-permission-governance",
        "audit-siem-integration",
        "encryption-at-rest-cmk"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "pim",
        "privileged-access",
        "identity",
        "mfa",
        "compliance"
      ],
      "referenceLinks": [
        {
          "label": "Azure Entra ID Privileged Identity Management",
          "url": "https://learn.microsoft.com/en-us/entra/id-governance/privileged-identity-management/pim-configure"
        },
        {
          "label": "PIM for Fabric Workspace Roles",
          "url": "https://learn.microsoft.com/en-us/fabric/admin/pim-setup"
        },
        {
          "label": "Multi-Factor Authentication Best Practices",
          "url": "https://learn.microsoft.com/en-us/entra/identity/authentication/concept-mfa-licensing"
        }
      ],
      "estimatedImplementationEffort": "2-3 weeks",
      "costImplications": "Entra ID Premium P2: $30-50/user/month (per admin). PIM: included in Premium P2. Hardware MFA keys: $20-50 per unit."
    },
    {
      "id": "change-management-four-eyes",
      "name": "Dual-Approval Change Management Pipeline",
      "domain": "Data Transformation and Processing",
      "domainId": 2,
      "category": "Change Control",
      "summary": "Enforces dual-approval (business and governance) before production pipeline deployment, implementing four-eyes principle via Azure DevOps gates.",
      "description": "Four-eyes principle requires two independent approvals before changes to production systems. For HR analytics, changes to data pipelines (transformations, data quality rules, refresh schedules) can affect payroll, benefits, or compliance analytics. Requiring dual approval ensures both business correctness (HR manager) and governance compliance (data governance officer) are verified. Azure DevOps Deployment Pipelines implement gates between environments: development -> staging -> production. Staging gate requires business approval from HR analytics owner confirming transformations are correct. Production gate requires data governance approval confirming RLS policies, data lineage, and compliance are maintained. Pull request reviews enforce code quality and documentation before merge to main. Git branching strategy separates features, requiring peer review. Change log automatically documents approvers, timestamp, and change summary. Rejection of changes includes audit trail for compliance.",
      "fabricComponents": [
        "Azure DevOps",
        "Data Factory Pipeline",
        "Fabric Deployment Pipelines",
        "Git Integration"
      ],
      "pros": [
        "Enforces dual approval, preventing single-person errors and unauthorized changes to critical analytics.",
        "Creates audit trail proving compliance with change control requirements, supporting SOX, HIPAA audits.",
        "Improves quality by requiring peer review before production deployment."
      ],
      "cons": [
        "Adds cycle time: waiting for two approvers can delay urgent fixes (typical cycle time 24-48 hours).",
        "Requires both approvers to be available; absence of approver blocks deployment.",
        "False sense of security if approvers don't actually review changes carefully."
      ],
      "usageInstructions": "1. Set up Azure DevOps project with Git repo for Fabric pipeline definitions. 2. Configure branch policy on main: require pull request reviews, minimum 2 approvers (business + governance), status checks passing. 3. Create staging deployment pipeline: trigger on PR approval, deploy to staging environment. 4. Add pre-deployment gate before staging: auto-approve (runs tests, validates syntax). 5. Add pre-deployment gate before production: manual approval, allow only specific users (data governance team). Require justification/description. 6. Create business approval gate: HR manager reviews transformations, confirms correctness. 7. Track approvals: Azure DevOps automatically logs timestamp, approver identity, comments. 8. Reject approvals include mandatory reason. 9. Create dashboard: count deployments, approval time metrics, rejection rates. 10. Quarterly review: analyze approval bottlenecks, optimize process.",
      "governanceConsiderations": "Define who can request, approve, and reject changes: business sponsor (business approval), data governance (compliance approval). Document approval criteria: business approval verifies transformations match requirements, governance approval verifies RLS, lineage, data quality, compliance. Implement escalation path for urgent changes (e.g., 24-hour SLA for critical bug fixes). Audit approval logs monthly. Require documented change rationale in pull request. Disallow approval from same person who submitted change (dual-approval enforced technically).",
      "peopleAnalyticsUseCases": [
        "Data engineer submits PR changing employee salary aggregation logic (e.g., fixing bonus calculation). Business sponsor (payroll manager) approves confirming calculation is correct. Data governance approves confirming aggregation preserves privacy (k>=5). Production deployment occurs only after both approvals.",
        "New HR dataset onboarded to Lakehouse: ingestion pipeline PR submitted. Business sponsor approves confirming data matches HRIS system. Data governance approves confirming sensitivity labels assigned, lineage documented. Both approvals required before prod activation.",
        "Urgent hotfix: salary dashboard showing incorrect totals. Change submitted with 'urgent' flag. Both approvers pinged, typically respond within 2-4 hours. Production deployment after both approvals."
      ],
      "complexity": "Medium",
      "maturity": "Emerging",
      "compatibleWith": [
        "deployment-pipelines",
        "audit-siem-integration"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "change-management",
        "devops",
        "approval",
        "governance",
        "four-eyes"
      ],
      "referenceLinks": [
        {
          "label": "Azure DevOps Release Gates and Approvals",
          "url": "https://learn.microsoft.com/en-us/azure/devops/pipelines/release/approvals/approvals"
        },
        {
          "label": "Fabric Deployment Pipelines",
          "url": "https://learn.microsoft.com/en-us/fabric/cicd/deployment-pipelines/intro"
        },
        {
          "label": "Git Branching Strategy and Pull Request Reviews",
          "url": "https://learn.microsoft.com/en-us/azure/devops/repos/git/branch-policies"
        }
      ],
      "estimatedImplementationEffort": "3-4 weeks",
      "costImplications": "Azure DevOps: free for up to 5 users/project. Git: no cost. Slack/Teams notifications: included. Process overhead: ~1-2 hours per change for approvals."
    },
    {
      "id": "cross-border-data-residency",
      "name": "Cross-Border Data Residency Isolation",
      "domain": "Data Sharing and Distribution",
      "domainId": 8,
      "category": "Data Sovereignty",
      "summary": "Isolates employee data by geography using multi-geo Fabric capacities, ensuring Canadian employee data remains in Canada and US data in US, with aggregate-only cross-border reporting.",
      "description": "Cross-border data residency ensures that employee personal data is stored and processed only within specified geographies per regulatory requirements. For multinational HR analytics, Canadian employees' data (salary, SSN, benefits) must remain in Canada due to PIPEDA. US data must remain in US due to state regulations and employer obligations. Fabric multi-geo capacity configuration assigns workspaces to specific regions; OneLake data is geo-pinned to that region. Cross-border reporting uses aggregate-only views (k-anonymity approach) ensuring individual records cannot be queried across borders. Data sharing uses Shortcuts with filtering, preventing cross-border record-level export. Periodic reviews ensure no data has migrated across borders. Audit logs track cross-border access attempts.",
      "fabricComponents": [
        "Fabric Capacity",
        "OneLake",
        "Workspace Assignment",
        "Multi-Geo Configuration"
      ],
      "pros": [
        "Meets regulatory data residency requirements (PIPEDA, GDPR, local laws) preventing costly compliance violations.",
        "Enables multi-country operations with confidence that data stays in authorized regions.",
        "Provides operational resilience: country-level outage affects only that region's operations, not global."
      ],
      "cons": [
        "Multi-region capacities increase costs ~2-3x vs. single-region (separate capacity per region).",
        "Analytics across regions requires federated queries or cross-border aggregates; complex joins are impossible.",
        "Data migration for employee moves (e.g., employee relocates from Canada to US) requires careful handling: old records deletion or transfer."
      ],
      "usageInstructions": "1. Create separate Fabric Capacity per geography: Canadian Capacity (Canada Central region), US Capacity (US East 2 region). 2. Create separate workspaces per geography: 'HR-Analytics-CA' in Canadian capacity, 'HR-Analytics-US' in US capacity. 3. Ingest employee data to geo-pinned Lakehouse: Canadian employee table in Canada workspace, US employee table in US workspace. 4. For cross-border reporting, create aggregate-only views: SELECT department, YEAR(dob) as year_of_birth, COUNT(*) as employee_count FROM employees WHERE country='CA' GROUP BY department, YEAR(dob) HAVING COUNT(*) >= 5. 5. Create federated semantic models: Power BI connects to Canadian and US aggregate views, combines aggregates (no record-level data). 6. Prevent cross-border shortcuts: Workspace Sharing > restrict shortcuts to same-region workspaces. 7. Audit cross-border access: Log Analytics tracks queries across regions, alert on suspicious activity. 8. Data migration procedure: terminating employee, update country field, archive to historical table in original region, do not migrate raw records.",
      "governanceConsiderations": "Establish data residency governance committee with legal, compliance, and HR stakeholders per country. Document residency requirements by jurisdiction (PIPEDA, GDPR, state laws). Implement technical enforcement: prevent shortcuts crossing regions, audit logs for attempted cross-border access. Quarterly audit: verify no employee data exists in wrong region. Data transfer agreements: document how employee relocations are handled. Secure deletion: ensure migrated data is permanently deleted from source region.",
      "peopleAnalyticsUseCases": [
        "Canadian bank with Canadian HQ + US subsidiary: employees are separated by capacity/workspace. Canadian headquarters views all Canadian employee analytics in Canadian workspace (compliant with PIPEDA). US subsidiary views US employee analytics in US workspace. Joint reporting uses aggregates: total Canadian headcount + total US headcount, no cross-border record-level joins.",
        "Multinational tech company with employees in Canada, US, and EU: separate capacities for each region. Annual global report aggregates at country-group level: 'Canada 5000 employees, US 10000 employees, EU 3000 employees' without exposing individual records.",
        "Employee relocation: Canadian employee transfers to US. Old record in Canadian Lakehouse is soft-deleted (marked inactive), no migration to US workspace. US onboarding creates new record in US workspace for same employee."
      ],
      "complexity": "High",
      "maturity": "GA",
      "compatibleWith": [
        "medallion-architecture",
        "encryption-at-rest-cmk",
        "disaster-recovery-geo"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "data-residency",
        "compliance",
        "gdpr",
        "pipeda",
        "sovereignty"
      ],
      "referenceLinks": [
        {
          "label": "Microsoft Fabric Multi-Geo Capabilities",
          "url": "https://learn.microsoft.com/en-us/fabric/admin/capacity-multi-geo"
        },
        {
          "label": "PIPEDA Data Residency Requirements",
          "url": "https://learn.microsoft.com/en-us/compliance/regulatory/offering-pipeda"
        },
        {
          "label": "GDPR Data Residency and Transfers",
          "url": "https://learn.microsoft.com/en-us/compliance/regulatory/gdpr-data-location"
        }
      ],
      "estimatedImplementationEffort": "6-8 weeks",
      "costImplications": "Multi-region Fabric Capacity: 2-3x capacity cost (separate capacity per region). Premium P1 capacity: $16/hour per region. Cross-border OneLake replication: $0.02/GB/month."
    },
    {
      "id": "encryption-at-rest-cmk",
      "name": "Encryption at Rest with Customer-Managed Keys",
      "domain": "Data Governance and Security",
      "domainId": 3,
      "category": "Encryption",
      "summary": "Implements encryption at rest using customer-managed keys (CMK) stored in Azure Key Vault for OneLake workspace data. Ensures TLS 1.2+ for all data in transit and maintains FIPS 140-2 compliance for sensitive HR data.",
      "description": "Customer-Managed Key (CMK) encryption provides organizational control over encryption key lifecycle and rotation. In financial services HR analytics, CMK encryption via Azure Key Vault ensures that OneLake workspace data is encrypted with keys managed by your organization, not Microsoft. All data in transit uses TLS 1.2 or higher. This approach meets regulatory requirements for key custody, enables key rotation policies, supports audit logging for key access, and provides compliance with FIPS 140-2 standards for handling highly sensitive employee financial and personal data. Integration with Entra ID service principals allows role-based access control over decryption operations.",
      "fabricComponents": [
        "Azure Key Vault",
        "OneLake",
        "Fabric Workspace Settings",
        "Entra ID Service Principal"
      ],
      "pros": [
        "Provides organizational control over encryption keys with full audit trail of key access and rotations.",
        "Enables compliance with regulations requiring customer-managed encryption (SOX, GDPR, PIPEDA, HIPAA).",
        "Supports break-glass emergency key access protocols and disaster recovery scenarios."
      ],
      "cons": [
        "Increases operational complexity requiring key rotation and lifecycle management procedures.",
        "Key Vault service calls add latency to Fabric operations, typically <10ms but noticeable at scale.",
        "Requires careful IAM design to prevent accidental key lockout that could make data unrecoverable."
      ],
      "usageInstructions": "1. Create an Azure Key Vault resource in the same region as Fabric capacity. 2. Generate RSA 3072-bit or 4096-bit CMK in Key Vault. 3. Grant Fabric capacity system-assigned managed identity Key Wrap/Unwrap permissions on the key. 4. In Fabric Workspace Settings, select 'Customer Managed Key' and specify the Key Vault URI. 5. Configure key rotation policy (annual minimum). 6. Enable Key Vault audit logging and monitor access. 7. Test disaster recovery failover procedures quarterly.",
      "governanceConsiderations": "Establish a key management committee with business and security stakeholders. Document key rotation procedures and emergency access protocols. Implement access reviews quarterly for Key Vault permissions. Ensure backup keys are stored in a geographically separate region. Monitor failed decryption attempts and investigate anomalies. Integrate key rotation events into change management processes.",
      "peopleAnalyticsUseCases": [
        "Salary survey analysis where payroll data is encrypted with organization-controlled CMK, enabling internal audits while maintaining key custody.",
        "Executive compensation dashboards requiring SOX compliance where CMK encryption demonstrates regulatory control over sensitive executive personal data.",
        "International employee equity tracking where Canadian employee data uses Canadian Key Vault instance and US data uses US Key Vault for data residency compliance."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "sensitivity-labels",
        "purview-data-map",
        "dynamic-data-masking",
        "row-level-security"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "encryption",
        "security",
        "compliance",
        "key-management",
        "financial-services"
      ],
      "referenceLinks": [
        {
          "label": "Encryption at Rest in Microsoft Fabric",
          "url": "https://learn.microsoft.com/en-us/fabric/enterprise/encryption-at-rest"
        },
        {
          "label": "Azure Key Vault Overview",
          "url": "https://learn.microsoft.com/en-us/azure/key-vault/general/overview"
        },
        {
          "label": "FIPS 140-2 Compliance in Azure",
          "url": "https://learn.microsoft.com/en-us/compliance/regulatory/offering-fips-140-2"
        }
      ],
      "estimatedImplementationEffort": "2-3 weeks",
      "costImplications": "Azure Key Vault: $0.34/10k operations; CMK key storage $1/month. Key Vault calls add minimal overhead."
    },
    {
      "id": "dlp-policy-enforcement",
      "name": "Data Loss Prevention Policy Enforcement",
      "domain": "Data Governance and Security",
      "domainId": 3,
      "category": "Data Protection",
      "summary": "Configures Microsoft Purview Data Loss Prevention (DLP) policies to detect and block unauthorized export or sharing of sensitive information types including SSN, salary data, and bank account numbers.",
      "description": "Data Loss Prevention (DLP) policies in Microsoft Purview automatically detect sensitive information types within Fabric and Power BI, then enforce preventive or detective controls. For HR analytics, DLP policies identify patterns matching US Social Security Numbers, salary ranges, employee IDs, and bank account numbers. When detected, policies can block exports to CSV/Excel, prevent uploads to personal OneDrive, restrict sharing to external domains, require approval for sensitive queries, or log incidents to Azure Sentinel for SOC review. DLP works alongside sensitivity labels to provide multi-layered protection. Policies can be scoped to specific workspaces, datasets, or semantic models. Integration with Sensitivity Labels and Power BI Workspace settings ensures consistent enforcement across analytics surfaces.",
      "fabricComponents": [
        "Microsoft Purview DLP",
        "Sensitivity Labels",
        "Power BI",
        "OneLake"
      ],
      "pros": [
        "Detects sensitive data automatically using built-in and custom regex patterns without manual classification.",
        "Blocks unauthorized export at the point of action (export, copy, print) reducing insider threat risk.",
        "Provides audit trail of blocked/allowed actions and integrates with SIEM for incident response workflows."
      ],
      "cons": [
        "Can trigger false positives on benign data patterns, requiring regular tuning and user exceptions.",
        "DLP policies are primarily detective in Power BI; preventive enforcement requires specific workspace integration.",
        "Regex patterns for custom sensitive data types require security team expertise and ongoing maintenance."
      ],
      "usageInstructions": "1. Open Microsoft Purview Compliance Center and navigate to Data Loss Prevention > Policies. 2. Create new policy with template 'Financial Info' or 'PII' as baseline. 3. Add custom sensitive info types: SSN regex (^\\\\d{3}-\\\\d{2}-\\\\d{4}$), salary range (\\\\$[0-9]{3,4}[KM]). 4. Configure rule actions: 'Restrict access' for Power BI, 'Require justification' for exports, 'Send alert to admin'. 5. Set policy scope to HR workspace and datasets. 6. Enable incident reporting to Azure Sentinel. 7. Test policy with sample data before production rollout. 8. Review blocked incidents monthly.",
      "governanceConsiderations": "Establish a DLP governance committee with HR, security, and legal teams. Document all custom sensitive data type patterns and business justification. Implement exception request workflow with business and compliance approval. Monitor false positive rate and adjust thresholds quarterly. Ensure DLP incidents are correlated with user access logs and audit trails. Train HR analytics team on compliant export procedures.",
      "peopleAnalyticsUseCases": [
        "Automated blocking of salary equity analysis exports containing aggregated ranges to unauthorized recipients, while allowing HR team to export to approved HR department network shares.",
        "Detection and alert when employee benefit election data containing SSN is copied to clipboard or exported to personal email, triggering incident response investigation.",
        "Prevention of executive compensation dashboard drill-through exports to external consultants without pre-approval workflow, enforcing approval for sensitive roles."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "sensitivity-labels",
        "row-level-security",
        "dynamic-data-masking",
        "audit-siem-integration"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "sensitivity-labels"
      ],
      "tags": [
        "data-loss-prevention",
        "dlp",
        "compliance",
        "data-protection",
        "security"
      ],
      "referenceLinks": [
        {
          "label": "Data Loss Prevention in Microsoft Purview",
          "url": "https://learn.microsoft.com/en-us/purview/dlp-learn-about-dlp"
        },
        {
          "label": "DLP Policy Creation and Management",
          "url": "https://learn.microsoft.com/en-us/purview/create-test-tune-dlp-policy"
        },
        {
          "label": "Sensitive Information Types in Purview",
          "url": "https://learn.microsoft.com/en-us/purview/sensitive-information-type-learn-about"
        }
      ],
      "estimatedImplementationEffort": "3-4 weeks",
      "costImplications": "Included in Purview premium license (part of Microsoft 365 E5 or standalone). DLP policy evaluation adds <1% query latency."
    },
    {
      "id": "pii-tokenization",
      "name": "PII Tokenization and Pseudonymization",
      "domain": "Data Transformation and Processing",
      "domainId": 2,
      "category": "Privacy Engineering",
      "summary": "Replaces Personally Identifiable Information (PII) with deterministic tokens using Microsoft Presidio and PySpark, enabling data sharing and analytics without exposing actual personal data.",
      "description": "PII tokenization uses deterministic hashing to replace sensitive personal identifiers with tokens while maintaining the ability to link records. Microsoft Presidio, an AI-powered data protection service, automatically detects PII entities (names, addresses, phone numbers, emails) within notebooks and datasets. Identified PII is replaced with tokens, with the mapping stored securely in Azure Key Vault. For HR analytics, this enables sharing anonymized talent data with third-party vendors, consultants, and development teams without exposing actual employee identity. Deterministic tokenization (same PII always maps to same token) preserves join semantics. PySpark transformations within Fabric Notebooks perform tokenization at scale. Different token formats support different use cases: hash tokens for aggregate analytics, UUID tokens for record-level joins. Reverse lookup tokens require Key Vault authorization for decryption.",
      "fabricComponents": [
        "Fabric Notebooks",
        "PySpark",
        "Microsoft Presidio",
        "Azure Key Vault",
        "Lakehouse"
      ],
      "pros": [
        "Enables secure data sharing with business partners and vendors by removing actual identity without losing join capability.",
        "Detects PII automatically using AI models rather than relying on manual regex patterns, catching complex PII patterns.",
        "Supports different tokenization strategies per use case: hash-only tokens for analytics, reversible tokens for authorized users with Key Vault access."
      ],
      "cons": [
        "Deterministic tokenization vulnerable to frequency analysis if PII distribution is skewed (e.g., only 3 VP names).",
        "Presidio requires pre-trained models and tuning to avoid false positives/negatives in organization-specific contexts.",
        "Large-scale tokenization can impact notebook execution time; requires parallel PySpark processing for efficiency."
      ],
      "usageInstructions": "1. Install Presidio in Fabric Notebook: pip install presidio-analyzer presidio-anonymizer. 2. Load employee data in PySpark DataFrame. 3. Configure Presidio analyzer with entity patterns: PERSON, PHONE_NUMBER, EMAIL_ADDRESS. 4. Create tokenization mapping: use Presidio to detect PII, generate deterministic SHA-256 hash token or UUID. 5. Store mapping in Key Vault secret with access restricted to governance team. 6. Replace PII with tokens using Presidio anonymizer. 7. Save tokenized dataset to Silver/Gold layer. 8. Log all tokenization operations with user, timestamp, entity count. 9. Test reverse lookup process with authorized user.",
      "governanceConsiderations": "Establish PII tokenization review board with privacy, security, and HR stakeholders. Document which entity types are tokenized and which remain for legitimate analytics. Implement access controls requiring Key Vault authorization for reverse lookups. Monitor tokenization logs for suspicious decryption patterns. Conduct quarterly audits of Presidio configuration to ensure no drift in entity detection. Maintain separate tokenization keys per data consumer or use case.",
      "peopleAnalyticsUseCases": [
        "Sharing anonymized employee experience survey data with external consulting firm to analyze cultural trends without exposing employee names, emails, or departments.",
        "Enabling development team to use production-like employee hierarchy and org structure data for testing without accessing real employee identities, using UUID tokens in place of names.",
        "Providing recruitment analytics to third-party recruitment platform showing skills and experience patterns without revealing actual employee identity, enabling vendor to match internal talent to opportunities."
      ],
      "complexity": "High",
      "maturity": "Emerging",
      "compatibleWith": [
        "medallion-architecture",
        "spark-notebook-etl",
        "data-quality-validation",
        "dlp-policy-enforcement"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "pii",
        "anonymization",
        "privacy",
        "data-sharing",
        "presidio"
      ],
      "referenceLinks": [
        {
          "label": "Microsoft Presidio Overview",
          "url": "https://learn.microsoft.com/en-us/presidio/"
        },
        {
          "label": "Presidio with Fabric Notebooks",
          "url": "https://github.com/microsoft/presidio"
        },
        {
          "label": "Data Anonymization Techniques",
          "url": "https://learn.microsoft.com/en-us/compliance/regulatory/gdpr-data-subject-rights"
        }
      ],
      "estimatedImplementationEffort": "4-6 weeks",
      "costImplications": "Presidio: open-source, no licensing cost. Key Vault storage: minimal. Spark compute: standard Fabric notebook rates."
    },
    {
      "id": "data-retention-lifecycle",
      "name": "Automated Data Retention and Purge Pipeline",
      "domain": "Data Transformation and Processing",
      "domainId": 2,
      "category": "Lifecycle Management",
      "summary": "Implements scheduled automated pipelines that evaluate data age against retention rules, execute secure deletion with audit trails, and support legal hold overrides for compliance.",
      "description": "Data retention and purge pipelines ensure that employee data is not retained longer than required by law or business policy. For HR analytics, employee records must be retained per employment law (typically 3-7 years depending on jurisdiction), then securely deleted. Automated pipelines check record creation/modification dates against configurable retention policies, identify data eligible for purge, execute secure deletion via soft-delete flags or physical removal, and generate audit logs. Legal holds prevent deletion during litigation or regulatory investigations. Data Factory pipelines coordinate with Fabric notebooks to identify eligible records using SQL queries (SELECT * WHERE modified_date < DATEADD(year,-3,GETDATE())), mark for deletion, execute deletion operations, and log to audit tables. Soft-delete approach (marking records inactive) preserves referential integrity and enables recovery. Hard-delete (physical removal from Delta Lake) reduces storage and improves query performance but requires careful execution.",
      "fabricComponents": [
        "Data Factory Pipeline",
        "Fabric Notebooks",
        "OneLake",
        "Delta Lake",
        "Lakehouse"
      ],
      "pros": [
        "Automates compliance with data retention regulations (GDPR right-to-erasure, employment law, SOX records retention).",
        "Reduces storage costs by removing obsolete data and improves query performance on smaller active datasets.",
        "Maintains complete audit trail of deleted records and reasons, supporting forensic investigations and compliance audits."
      ],
      "cons": [
        "Incorrectly configured retention rules can cause accidental data loss; requires thorough testing and change management approval.",
        "Soft-delete approaches maintain referential integrity but require query filters to exclude deleted records; hard-delete is faster but riskier.",
        "Legal holds must be tracked separately; coordination between legal, HR, and data teams is required to manage hold status."
      ],
      "usageInstructions": "1. Define retention policies in configuration table: entity type, retention period (years), jurisdiction, legal hold indicator. 2. Create Data Factory pipeline with Copy Activity to identify records eligible for purge using SQL: SELECT * WHERE datediff(year, modified_date, getdate()) >= retention_years AND legal_hold = 0. 3. Use Fabric Notebook to mark records as deleted (UPDATE table SET is_deleted = 1 WHERE id IN (...)) instead of hard delete initially. 4. Execute soft-delete daily/weekly per schedule. 5. After 30-day recovery period, run hard delete: VACUUM table_name RETAIN 0 HOURS; DELETE FROM table WHERE is_deleted = 1. 6. Log all operations: user, timestamp, record count, reasons. 7. Send audit report to compliance/legal team monthly.",
      "governanceConsiderations": "Establish data retention committee with legal, HR, compliance, and data teams. Document retention policies for each data entity type by jurisdiction. Implement approval workflow for legal hold changes. Set up audit alerts for large purge operations. Conduct quarterly audits of purge logs to verify correctness. Test disaster recovery to ensure deleted data cannot be recovered from backups without explicit authorization. Maintain retention policy version control.",
      "peopleAnalyticsUseCases": [
        "Automatically delete contingent worker records 2 years after separation date and contract end, enabling compliance with employment laws while reducing data footprint.",
        "Purge applicant data after hiring decision per FCRA requirements (typically 3 years), except for records under litigation hold from employment disputes.",
        "Soft-delete employee benefit elections from terminated employees after 7 years but preserve audit trail for pension calculations and benefit inquiries, with legal hold preventing deletion during claims."
      ],
      "complexity": "High",
      "maturity": "Emerging",
      "compatibleWith": [
        "medallion-architecture",
        "spark-notebook-etl",
        "delta-lake-partitioning",
        "audit-siem-integration"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "medallion-architecture"
      ],
      "tags": [
        "retention",
        "lifecycle",
        "compliance",
        "gdpr",
        "data-deletion"
      ],
      "referenceLinks": [
        {
          "label": "Azure Data Factory Scheduling and Triggers",
          "url": "https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers"
        },
        {
          "label": "GDPR Data Retention and Right to Erasure",
          "url": "https://learn.microsoft.com/en-us/compliance/regulatory/gdpr-data-subject-rights"
        },
        {
          "label": "Delta Lake VACUUM and DELETE Operations",
          "url": "https://learn.microsoft.com/en-us/fabric/onelake/delta-lake-overview"
        }
      ],
      "estimatedImplementationEffort": "5-7 weeks",
      "costImplications": "Data Factory pipeline runs: $0.50 per execution. Fabric compute for Notebook purge jobs: standard rates. Storage savings from deletion: 10-30% reduction in employee data volume."
    },
    {
      "id": "anonymization-k-anonymity",
      "name": "Statistical Anonymization for HR Analytics",
      "domain": "Data Governance and Security",
      "domainId": 3,
      "category": "Privacy Engineering",
      "summary": "Implements k-anonymity and differential privacy techniques to ensure published HR analytics cannot identify individuals through aggregation attacks or frequency analysis.",
      "description": "K-anonymity ensures that any combination of quasi-identifiers (age, department, salary band) appears in at least k records, preventing record linkage attacks. Differential privacy adds carefully calibrated noise to query results, enabling accurate aggregate statistics while preventing inference of individual values. For HR analytics, aggregation-only views enforce k-anonymity by requiring HAVING COUNT(*) >= 5 clauses on all queries, preventing disclosure of rare populations (e.g., the sole executive in a location). Noise injection adds Laplace or Gaussian noise proportional to sensitivity, with epsilon (privacy budget) controlling noise magnitude. Views in Fabric Warehouse or SQL Analytics Endpoint implement these controls. PySpark notebooks implement noise injection for ad-hoc analyses. Practical applications include publishing salary bands by role (not individuals), showing headcount by department (minimum 5 per group), and publishing engagement scores by location-function groups.",
      "fabricComponents": [
        "Fabric Warehouse",
        "SQL Analytics Endpoint",
        "PySpark Notebooks",
        "Lakehouse"
      ],
      "pros": [
        "Provides formal mathematical guarantees against re-identification attacks, meeting GDPR and other privacy regulations' proportionality requirements.",
        "Enables publishing aggregate HR analytics to broad audiences (all employees, board, public) without risk of individual inference.",
        "Can be implemented as database views and functions, integrating seamlessly into existing analytics without changing consuming applications."
      ],
      "cons": [
        "K-anonymity vulnerable to semantic attacks if quasi-identifiers are not carefully chosen; requires domain expertise to identify all identifying combinations.",
        "Noise injection reduces statistical utility; high privacy budgets (epsilon) may allow inference while low budgets (epsilon <0.1) add significant noise.",
        "Maintaining k-anonymity becomes harder as dataset grows and rare populations appear; may force conservative suppression of legitimate insights."
      ],
      "usageInstructions": "1. Identify quasi-identifiers in HR data (age, salary band, department, location, tenure, role). 2. Create aggregation-only views with HAVING COUNT(*) >= 5. 3. Test for k=5 anonymity: SELECT department, role, COUNT(*) FROM employees GROUP BY department, role HAVING COUNT(*) >= 5. 4. For salary analytics, implement noise injection in Python: create base aggregate (SELECT department, avg(salary) as avg_sal FROM employees GROUP BY department), calculate sensitivity (max salary difference), generate Laplace noise with epsilon=0.5, add noise to aggregates. 5. Create security principal with view-only permissions to anonymized views. 6. Publish anonymized views via Power BI semantic model with field-level security. 7. Document epsilon budgets and k values for each published analysis.",
      "governanceConsiderations": "Establish privacy analytics working group with data scientists, privacy officers, and HR stakeholders. Document quasi-identifier sets and privacy threat model per analysis. Set epsilon budgets based on acceptable privacy-utility tradeoff. Review all published aggregates for k-anonymity before release. Track total epsilon consumption across all published analyses to prevent privacy budget exhaustion. Implement technical controls preventing ad-hoc SQL query access to raw employee data.",
      "peopleAnalyticsUseCases": [
        "Publishing company-wide salary equity analysis to all employees showing average salary by department-level-location, with k=10 minimum group size, enabling transparency without revealing individual salaries.",
        "Generating anonymized performance distribution histograms for board review with noise injection ensuring no distribution shape discloses individual outliers.",
        "Creating recruitment funnel analytics published externally showing aggregate conversion rates by source-role with k=5 minimums, preventing inference of individual candidate outcomes."
      ],
      "complexity": "High",
      "maturity": "Emerging",
      "compatibleWith": [
        "medallion-architecture",
        "row-level-security",
        "dynamic-data-masking",
        "purview-data-map"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "medallion-architecture",
        "row-level-security"
      ],
      "tags": [
        "anonymization",
        "k-anonymity",
        "differential-privacy",
        "privacy",
        "compliance"
      ],
      "referenceLinks": [
        {
          "label": "K-Anonymity and Privacy Techniques",
          "url": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/privacy-techniques"
        },
        {
          "label": "Differential Privacy in Azure",
          "url": "https://learn.microsoft.com/en-us/purview/compliance-privacy-basics"
        },
        {
          "label": "GDPR Data Minimization Principles",
          "url": "https://learn.microsoft.com/en-us/compliance/regulatory/gdpr-data-minimization"
        }
      ],
      "estimatedImplementationEffort": "6-8 weeks",
      "costImplications": "No licensing cost; implemented in Fabric Warehouse at standard query rates. Noise injection PySpark notebooks at standard compute rates."
    },
    {
      "id": "audit-siem-integration",
      "name": "Audit Log Export and SIEM Integration",
      "domain": "Data Governance and Security",
      "domainId": 3,
      "category": "Monitoring",
      "summary": "Forwards Fabric activity logs and Power BI audit logs to Azure Sentinel for centralized SOC monitoring, anomaly detection, and forensic investigation.",
      "description": "Audit log integration ensures all Fabric and Power BI activities (data access, semantic model changes, sharing, export, deletion) are captured, centralized, and analyzed for security incidents. Fabric activity logs (workspace creation, item access, refresh execution, query patterns) flow to Log Analytics Workspace. Power BI activity logs (report view, dataset refresh, sharing changes, export) are forwarded to the Unified Audit Log via the Office 365 Management API. Azure Sentinel consumes these logs, applies correlation rules to detect anomalies (bulk data export, after-hours access, privilege escalation), enriches with identity and threat intelligence, and triggers incidents for SOC investigation. Custom KQL (Kusto Query Language) queries detect HR-specific threats: unusual bulk report exports, access to sensitive employee data by non-HR users, policy violation patterns. Retention ensures logs are available for 1-2 years for forensic analysis and compliance audits.",
      "fabricComponents": [
        "Azure Sentinel",
        "Log Analytics Workspace",
        "Power BI Activity Log",
        "Unified Audit Log",
        "Azure Monitor"
      ],
      "pros": [
        "Centralizes audit logs from Fabric and Power BI in single searchable repository enabling cross-system correlation and forensic investigations.",
        "Detects insider threats and compliance violations in real-time through rule-based alerting and anomaly detection, enabling rapid response.",
        "Provides evidence trail for regulatory audits and incident response, meeting SOX, HIPAA, and other audit requirements."
      ],
      "cons": [
        "Azure Sentinel ingestion costs scale with log volume; busy Fabric environments can generate 1000+ GB/month of logs, increasing licensing costs.",
        "Lag between activity and Sentinel processing (typically 1-5 minutes) means real-time detection is limited; historical detection takes longer.",
        "False positive rates in correlation rules can overwhelm SOC; requires significant tuning and stakeholder coordination."
      ],
      "usageInstructions": "1. Create Log Analytics Workspace in Azure. 2. Enable Fabric activity logging: Workspace Settings > Audit and Compliance > Enable Activity Logging. 3. Create Data Connector in Log Analytics: Fabric Activity > Diagnostic Settings > Send to Log Analytics Workspace. 4. Enable Power BI audit logging: Power BI Admin Portal > Audit and Compliance > Turn on audit log search. 5. Configure Office 365 Management API in Sentinel to consume Unified Audit Log. 6. In Azure Sentinel, create Data Connectors for Office 365 and Log Analytics. 7. Create KQL detection rules: query for bulk exports (>1000 rows), access to SSN columns, after-hours queries. 8. Configure Incidents from detections with SOAR playbook triggers. 9. Create Power BI dashboard for audit KPIs: daily active users, top data consumers, failed access attempts.",
      "governanceConsiderations": "Establish Security Operations Center (SOC) workflow for Sentinel incidents related to HR data. Define escalation procedures for confirmed security incidents (e.g., data exfiltration). Implement log retention policies: 6 months hot storage, 2 years cold archive. Conduct quarterly reviews of detection rules to reduce false positives. Require formal change management approval for queries accessing sensitive audit data. Ensure audit log access is restricted to security team and select compliance officers.",
      "peopleAnalyticsUseCases": [
        "Real-time detection of bulk export of executive compensation reports (>100 rows) to external email domains, triggering SOC investigation into potential salary data exfiltration.",
        "Anomaly detection identifying that a non-HR user (e.g., IT service account) is accessing employee SSN and salary columns during off-hours, indicating potential credential compromise.",
        "Forensic investigation of a departing manager's last-day activities: queried employee performance data, shared compensation history report with personal email, then deleted workspace link."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "row-level-security",
        "workspace-permission-governance",
        "dlp-policy-enforcement",
        "privileged-access-management"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "audit",
        "siem",
        "sentinel",
        "monitoring",
        "security"
      ],
      "referenceLinks": [
        {
          "label": "Fabric Activity Logging and Monitoring",
          "url": "https://learn.microsoft.com/en-us/fabric/admin/monitoring-workspace"
        },
        {
          "label": "Azure Sentinel Overview",
          "url": "https://learn.microsoft.com/en-us/azure/sentinel/overview"
        },
        {
          "label": "Power BI Audit Logging",
          "url": "https://learn.microsoft.com/en-us/power-bi/admin/service-admin-auditing"
        }
      ],
      "estimatedImplementationEffort": "3-4 weeks",
      "costImplications": "Log Analytics Workspace: $0.70/GB ingested. Azure Sentinel: $100+ per GB for first 100GB/day. Detection rule tuning requires SOC analyst time."
    },
    {
      "id": "dsr-fulfillment",
      "name": "Data Subject Request Fulfillment Pipeline",
      "domain": "Data Governance and Security",
      "domainId": 3,
      "category": "Privacy Compliance",
      "summary": "Automates GDPR/PIPEDA data subject rights (access and erasure) with workflows using Purview data lineage, soft-delete mechanisms, and verification processes.",
      "description": "Data Subject Requests (DSR) are formal legal requests from individuals to access (GDPR Article 15) or erase (GDPR Article 17, right-to-be-forgotten) their personal data. Manual DSR handling is error-prone, slow, and costly. Automated DSR fulfillment pipelines use Microsoft Purview Data Map to identify all systems containing an individual's data, orchestrate retrieval or deletion via Data Factory, perform soft-delete in Fabric Lakehouse with audit trail, notify requestor, and verify completion. For HR analytics, DSR requests require identifying employee records across Lakehouse, data warehouse, Power BI datasets, and archived systems. Soft-delete (marking records inactive) enables 30-day appeal period before hard-delete. Lineage mapping shows which analytics and reports are impacted by erasure. Workflow includes validation steps preventing accidental bulk deletion.",
      "fabricComponents": [
        "Microsoft Purview Data Map",
        "Data Factory Pipeline",
        "OneLake",
        "Fabric Notebooks",
        "Lakehouse"
      ],
      "pros": [
        "Automates time-consuming manual search and deletion, reducing DSR fulfillment time from weeks to days and reducing legal/compliance costs.",
        "Uses Purview lineage to ensure complete erasure across all systems; manual processes frequently miss copies or derived data.",
        "Provides audit trail proving compliance with GDPR/PIPEDA deadlines (30 days for access, 30 days for erasure), supporting regulatory defense."
      ],
      "cons": [
        "Incomplete or inaccurate Purview Data Map can result in missed data systems and non-compliance; requires continuous curation.",
        "Soft-delete relies on application logic filtering deleted records; risk of query bugs exposing deleted data if filters are incorrect.",
        "Determining which analytic derivatives (e.g., aggregates, models, exports) must be updated/deleted is complex and data-dependent."
      ],
      "usageInstructions": "1. Create DSR request intake form (SharePoint form or Power Apps) capturing: requestor name, email, request type (access/erasure), date. 2. Create Data Factory pipeline: Step 1 - Query Purview Data Map API for assets containing individual's data (e.g., Employee.EmployeeId = 12345). Step 2 - Generate list of impacted systems/tables. Step 3 - For access requests, export data to encrypted file in secure SharePoint. For erasure requests, mark records with is_dsr_deleted=1, log deletion timestamp/reason. Step 4 - Wait 30 days for appeals. Step 5 - Hard-delete (VACUUM, UPDATE DELETE). Step 6 - Notify requestor and compliance team. 7. Implement controls: require approvals for bulk erasure, audit all DSR operations, verify deletion completeness.",
      "governanceConsiderations": "Establish DSR response team with legal, HR, privacy, and data teams. Document DSR procedures and compliance timelines per jurisdiction. Implement technical access controls preventing unauthorized DSR request manipulation. Maintain DSR request audit log with decision and verification steps. Conduct quarterly audits to verify soft-deleted records are truly inaccessible to analytics. Maintain appeal process for 30-day grace period post-deletion request.",
      "peopleAnalyticsUseCases": [
        "Automated fulfillment of GDPR access request from Canadian employee: Purview identifies data in Lakehouse employee table, warehouse HRIS system, Power BI reports. Pipeline exports to encrypted PDF in 5 days, within GDPR 30-day SLA.",
        "Right-to-erasure request from terminated EU employee: soft-delete removes from all active tables, historical audit trails remain but employee salary/SSN are masked from all analytics, 30-day appeal period prevents accidental permanent deletion.",
        "Cross-border DSR for US employee working in EU office: pipeline identifies data in US Lake House capacity (compliant with GDPR data residency because US subject to standard contractual clauses), executes soft-delete with audit trail."
      ],
      "complexity": "High",
      "maturity": "Emerging",
      "compatibleWith": [
        "purview-data-map",
        "medallion-architecture",
        "audit-siem-integration"
      ],
      "incompatibleWith": [],
      "prerequisites": [
        "purview-data-map",
        "medallion-architecture"
      ],
      "tags": [
        "gdpr",
        "pipeda",
        "dsr",
        "privacy",
        "right-to-erasure"
      ],
      "referenceLinks": [
        {
          "label": "GDPR Data Subject Rights",
          "url": "https://learn.microsoft.com/en-us/compliance/regulatory/gdpr-data-subject-rights"
        },
        {
          "label": "Microsoft Purview Data Map",
          "url": "https://learn.microsoft.com/en-us/purview/purview-data-catalog"
        },
        {
          "label": "Data Subject Request Management",
          "url": "https://learn.microsoft.com/en-us/compliance/regulatory/manage-gdpr-data-subject-requests-summary"
        }
      ],
      "estimatedImplementationEffort": "6-8 weeks",
      "costImplications": "Purview Data Map: included in premium licensing. Data Factory DSR pipelines: $0.50 per execution. Estimated 100-200 DSR requests/year at $50 cost to process each."
    },
    {
      "id": "disaster-recovery-geo",
      "name": "Disaster Recovery and Geo-Redundancy",
      "domain": "Data Sharing and Distribution",
      "domainId": 8,
      "category": "Business Continuity",
      "summary": "Configures OneLake with zone-redundant storage (ZRS) and geo-replication to paired Azure region, with manual failover procedures and multi-geo capacity for data residency.",
      "description": "Disaster recovery ensures business continuity when regional outages occur. OneLake Zone-Redundant Storage (ZRS) replicates data across three availability zones within a region, protecting against zone-level failures. Geo-replication asynchronously copies data to a paired region (e.g., US East 2 to US Central 1), protecting against regional disasters. Fabric Capacity can be provisioned in multiple regions to enable manual failover. For HR analytics, disaster recovery ensures employee data is always available and compliant with data residency laws (e.g., Canadian data stays in Canada region). Multi-geo capacity configuration enables capacity to be deployed in specific regions. Backup procedures include daily snapshots exported to secure storage. Manual failover involves updating connection strings and repointing workspaces to failover capacity in alternate region. Recovery Time Objective (RTO) for manual failover is 4-8 hours; automated failover via service mesh future capability targets RTO <15 minutes.",
      "fabricComponents": [
        "OneLake",
        "Fabric Capacity",
        "Azure Region Pairs",
        "Zone-Redundant Storage"
      ],
      "pros": [
        "ZRS provides automatic replication without application changes, protecting against zone and data center failures with no additional cost.",
        "Geo-replication enables regional compliance (e.g., Canadian data in Canada) and disaster recovery to another region, meeting business continuity SLAs.",
        "Manual failover procedures are tested and documented, enabling rapid recovery with minimal training."
      ],
      "cons": [
        "Geo-replication introduces asynchronous delay (minutes to hours); recent data changes may not be replicated if failover occurs immediately after write.",
        "Manual failover requires human action and coordination, typically taking 4-8 hours; automated failover is not yet available.",
        "Multi-region Fabric Capacity costs increase significantly; maintaining hot-hot failover doubles capacity costs vs. cold standby."
      ],
      "usageInstructions": "1. Enable ZRS on OneLake workspace: Workspace Settings > Storage Redundancy > Zone-Redundant Storage. 2. Configure geo-replication: Workspace Settings > Disaster Recovery > Enable Geo-Replication to paired region (e.g., East 2 to Central 1). 3. Create Data Factory pipeline for daily snapshot export: copy all Lakehouse tables to Azure Storage Account in alternate region. 4. Document failover runbook: 1) Declare disaster, 2) Notify stakeholders, 3) Verify failover region readiness, 4) Update Fabric workspace connection to failover capacity, 5) Repoint Power BI datasets to failover semantic models, 6) Validate data integrity in failover region. 5. Conduct quarterly disaster recovery drill: simulate regional failure, execute failover, validate analytics in alternate region, measure RTO. 6. Maintain failover capacity in standby mode or scale down when not in use.",
      "governanceConsiderations": "Establish disaster recovery committee with IT, business continuity, and HR stakeholders. Define RTO (Recovery Time Objective) and RPO (Recovery Point Objective) per service level agreement. Document failover procedures and roles. Test failover procedures quarterly with full team participation. Monitor geo-replication lag and alert if lag exceeds SLA. Maintain separate credentials for failover region. Ensure external audit of disaster recovery controls annually.",
      "peopleAnalyticsUseCases": [
        "Regional outage in US East region: automatic ZRS failover within same region (1-2 minutes). If both East zones unavailable, manual failover to US Central capacity (4 hours RTO) ensures payroll analytics continue.",
        "Compliance requirement for Canadian employee data to remain in Canada: Canadian Fabric Capacity with geo-replication to alternate Canadian region, ensuring data never leaves Canada even during failover.",
        "Financial services regulatory requirement for near-zero RPO: async geo-replication has 30-minute lag, so daily snapshot export to alternate region ensures RPO <24 hours."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "medallion-architecture",
        "encryption-at-rest-cmk",
        "audit-siem-integration"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "disaster-recovery",
        "business-continuity",
        "geo-redundancy",
        "zrs",
        "failover"
      ],
      "referenceLinks": [
        {
          "label": "OneLake Disaster Recovery and Geo-Redundancy",
          "url": "https://learn.microsoft.com/en-us/fabric/onelake/redundancy"
        },
        {
          "label": "Azure Region Pairs for Business Continuity",
          "url": "https://learn.microsoft.com/en-us/azure/reliability/cross-region-replication-azure"
        },
        {
          "label": "Fabric Capacity Provisioning and Failover",
          "url": "https://learn.microsoft.com/en-us/fabric/admin/capacity-admin"
        }
      ],
      "estimatedImplementationEffort": "4-6 weeks",
      "costImplications": "ZRS: no additional cost. Geo-replication: $0.02/GB/month for replication. Failover capacity: $16/hour standby, $96/hour active (Premium P1)."
    },
    {
      "id": "network-isolation-private-links",
      "name": "Network Isolation with Private Endpoints",
      "domain": "Data Governance and Security",
      "domainId": 3,
      "category": "Network Security",
      "summary": "Implements Azure Private Endpoints for Fabric Capacity and managed private endpoints for Spark/compute, ensuring traffic flows only over private VNet and preventing internet exposure.",
      "description": "Private Endpoints eliminate internet routes for Fabric services, instead routing all traffic through Azure VNet private network. Fabric Capacity (compute and storage) uses Private Endpoints to ensure no data traverses public internet. Managed Private Endpoints for Spark Compute enable secure communication with private data sources (SQL Database in VNet, storage accounts with firewall enabled). For HR analytics in regulated environments (financial services, healthcare), network isolation ensures employee data never transits public internet. Network security groups (NSGs) can further restrict traffic by source/destination IP. VNet integration enables hybrid connectivity: on-premises HR systems can securely connect to Fabric via ExpressRoute. Outbound traffic can be forced through proxy/firewall for inspection. Azure Private Link services provide 99.99% availability.",
      "fabricComponents": [
        "Azure Private Link",
        "Azure VNet",
        "Managed Private Endpoints",
        "Fabric Capacity"
      ],
      "pros": [
        "Eliminates internet exposure of Fabric endpoints, significantly reducing attack surface and complying with network isolation requirements.",
        "Enables hybrid connectivity to on-premises systems without VPN, improving performance and security.",
        "Provides network-layer segmentation complementing identity and data-layer security controls, following defense-in-depth principle."
      ],
      "cons": [
        "Private Endpoints add operational complexity: VNet design, routing, DNS configuration, and troubleshooting require network expertise.",
        "Cost increases: Fabric Capacity pricing unchanged but Private Endpoints charge $0.50/hour per endpoint, adding $360/month per capacity.",
        "Hybrid connectivity requires ExpressRoute or Site-to-Site VPN; on-premises connectivity adds complexity and latency."
      ],
      "usageInstructions": "1. Create Azure VNet in the same region as Fabric Capacity. 2. In Fabric Workspace Settings, enable Private Endpoints: select Capacity > Network > Enable Private Endpoint. 3. Azure creates Private Link Service and provides Private Endpoint Connection. 4. In your VNet, create Private Endpoint resource: +Create > Private Endpoint, select Fabric service, select subnet, configure DNS integration. 5. Update DNS resolver to map fabric.microsoft.com to private IP (e.g., 10.0.0.5). 6. Test connectivity: connect VM in VNet, query nslookup fabric.microsoft.com (should resolve to private IP). 7. Configure Managed Private Endpoints for Spark: Workspace > Managed Private Endpoints > +New, select target resource (SQL Database, storage), approve in target resource. 8. Configure Network Security Group (NSG): allow only VNet subnets to reach Fabric Private Endpoint, deny internet-routed traffic.",
      "governanceConsiderations": "Establish network security architecture review with IT security and compliance teams. Document VNet design and Private Endpoint locations. Implement bastion hosts for secure VM access instead of public IPs. Enforce NSG rules preventing traffic to public internet. Monitor Private Endpoint connections and deny suspicious outbound attempts. Require change management approval for VNet/NSG changes. Conduct quarterly network segmentation audits.",
      "peopleAnalyticsUseCases": [
        "Financial services firm isolates HR analytics Fabric Capacity on private VNet, restricts outbound traffic to approved SIEM sink via proxy, ensuring no employee data transits internet.",
        "Healthcare provider connects on-premises HRIS system via ExpressRoute to Fabric over private endpoint, enabling analytics without routing PII through public internet or VPN.",
        "Multi-tenant SaaS provider isolates each customer's Fabric Capacity on separate VNet, using managed private endpoints to connect to customer-specific SQL databases on-premises."
      ],
      "complexity": "High",
      "maturity": "GA",
      "compatibleWith": [
        "encryption-at-rest-cmk",
        "workspace-permission-governance",
        "audit-siem-integration"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "network-security",
        "private-endpoints",
        "vnet",
        "isolation",
        "compliance"
      ],
      "referenceLinks": [
        {
          "label": "Azure Private Endpoints for Fabric",
          "url": "https://learn.microsoft.com/en-us/fabric/security/security-private-endpoints-overview"
        },
        {
          "label": "Azure VNet and NSG Configuration",
          "url": "https://learn.microsoft.com/en-us/azure/virtual-network/manage-network-security-group"
        },
        {
          "label": "Managed Private Endpoints for Spark",
          "url": "https://learn.microsoft.com/en-us/fabric/security/security-managed-private-endpoints-overview"
        }
      ],
      "estimatedImplementationEffort": "4-6 weeks",
      "costImplications": "Private Endpoint: $0.50/hour ($360/month per endpoint). VNet/NSG: no cost. ExpressRoute (if used): $0.30-0.50/hour depending on bandwidth."
    },
    {
      "id": "privileged-access-management",
      "name": "Just-in-Time Privileged Access Management",
      "domain": "Data Governance and Security",
      "domainId": 3,
      "category": "Identity Security",
      "summary": "Implements Azure Entra ID Privileged Identity Management (PIM) for just-in-time elevation of workspace admin roles, with approval workflows and MFA enforcement.",
      "description": "Privileged Identity Management (PIM) reduces standing permissions, requiring users to request temporary elevation to admin roles. For HR analytics, workspace admins manage data governance policies, sharing, and sensitive configurations. Rather than making users permanent admins (standing privilege), PIM makes them 'Eligible Admins' who must request activation. Requests require approval from compliance officer and MFA authentication. Activation is time-limited (1-8 hours) and audited. Break-glass emergency accounts provide fallback access if primary admins are unavailable. Multi-factor authentication (MFA) is required for all privileged actions. Audit logs in Entra ID capture all activation requests, approvals, and actions performed during elevated access. For compliance, PIM demonstrates least-privilege principle and immediate detection of unauthorized privilege escalation attempts.",
      "fabricComponents": [
        "Azure Entra ID PIM",
        "Fabric Workspace Roles",
        "Azure Key Vault",
        "Azure Sentinel"
      ],
      "pros": [
        "Reduces standing privilege, dramatically lowering risk of compromise: admin account hack affects only active activation window (1-8 hours) not 365 days.",
        "Requires approval for every elevation, creating human checkpoint preventing unauthorized access.",
        "Complete audit trail of privilege escalation enables detection of unusual elevation patterns and supports compliance audits."
      ],
      "cons": [
        "PIM adds friction to emergency access scenarios, requiring approval workflow adds 15-30 minutes to incident response.",
        "Admin users must have Entra ID Premium P2 license, adding cost ~$30-50/user/month.",
        "Misconfigured break-glass accounts (credentials leaked, not rotated) undermine PIM's security benefits."
      ],
      "usageInstructions": "1. Ensure Entra ID Premium P2 licensed. 2. In Entra ID > Privileged Identity Management > Fabric Workspace Roles, select 'Workspace Admin' role. 3. Set eligible users: add workspace managers as Eligible (not Permanent) members. 4. Configure activation: require approval, require MFA, set max activation duration to 4 hours. 5. Select approval delegator: compliance officer or security team. 6. Configure notifications: alert on elevations, email approvers. 7. Create break-glass account: dedicate account for emergency access, store credentials in physical safe (not digital). 8. Test activation: Eligible Admin requests activation, approver receives email, activates with MFA. 9. Monitor in Azure Sentinel: create alert for approval denials, unusual activation times, after-hours elevations. 10. Quarterly review: audit PIM logs, revoke unused eligible access.",
      "governanceConsiderations": "Establish PIM governance committee with security, HR, and compliance stakeholders. Document approval process and SLA for activation requests (should be <15 minutes). Implement technical controls enforcing MFA (hardware keys preferred). Rotate break-glass credentials quarterly with multiple stakeholders witnessing. Test break-glass emergency access annually. Review PIM logs monthly for suspicious patterns. Enforce organization-wide policy: no permanent privileged roles, all admins must use PIM.",
      "peopleAnalyticsUseCases": [
        "HR analytics workspace admin (normally ineligible) requests elevation to investigate unauthorized data export, approver grants 4-hour activation via MFA approval, admin investigates access logs with limited time window.",
        "Data governance officer regularly activates to review sensitivity label assignments and RLS policies, must approve each activation with business justification.",
        "Emergency incident: production Fabric capacity is down, break-glass account holder activates with MFA, gains temporary admin access to restart services, audited and revoked within 1 hour."
      ],
      "complexity": "Medium",
      "maturity": "GA",
      "compatibleWith": [
        "workspace-permission-governance",
        "audit-siem-integration",
        "encryption-at-rest-cmk"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "pim",
        "privileged-access",
        "identity",
        "mfa",
        "compliance"
      ],
      "referenceLinks": [
        {
          "label": "Azure Entra ID Privileged Identity Management",
          "url": "https://learn.microsoft.com/en-us/entra/id-governance/privileged-identity-management/pim-configure"
        },
        {
          "label": "PIM for Fabric Workspace Roles",
          "url": "https://learn.microsoft.com/en-us/fabric/admin/pim-setup"
        },
        {
          "label": "Multi-Factor Authentication Best Practices",
          "url": "https://learn.microsoft.com/en-us/entra/identity/authentication/concept-mfa-licensing"
        }
      ],
      "estimatedImplementationEffort": "2-3 weeks",
      "costImplications": "Entra ID Premium P2: $30-50/user/month (per admin). PIM: included in Premium P2. Hardware MFA keys: $20-50 per unit."
    },
    {
      "id": "change-management-four-eyes",
      "name": "Dual-Approval Change Management Pipeline",
      "domain": "Data Transformation and Processing",
      "domainId": 2,
      "category": "Change Control",
      "summary": "Enforces dual-approval (business and governance) before production pipeline deployment, implementing four-eyes principle via Azure DevOps gates.",
      "description": "Four-eyes principle requires two independent approvals before changes to production systems. For HR analytics, changes to data pipelines (transformations, data quality rules, refresh schedules) can affect payroll, benefits, or compliance analytics. Requiring dual approval ensures both business correctness (HR manager) and governance compliance (data governance officer) are verified. Azure DevOps Deployment Pipelines implement gates between environments: development -> staging -> production. Staging gate requires business approval from HR analytics owner confirming transformations are correct. Production gate requires data governance approval confirming RLS policies, data lineage, and compliance are maintained. Pull request reviews enforce code quality and documentation before merge to main. Git branching strategy separates features, requiring peer review. Change log automatically documents approvers, timestamp, and change summary. Rejection of changes includes audit trail for compliance.",
      "fabricComponents": [
        "Azure DevOps",
        "Data Factory Pipeline",
        "Fabric Deployment Pipelines",
        "Git Integration"
      ],
      "pros": [
        "Enforces dual approval, preventing single-person errors and unauthorized changes to critical analytics.",
        "Creates audit trail proving compliance with change control requirements, supporting SOX, HIPAA audits.",
        "Improves quality by requiring peer review before production deployment."
      ],
      "cons": [
        "Adds cycle time: waiting for two approvers can delay urgent fixes (typical cycle time 24-48 hours).",
        "Requires both approvers to be available; absence of approver blocks deployment.",
        "False sense of security if approvers don't actually review changes carefully."
      ],
      "usageInstructions": "1. Set up Azure DevOps project with Git repo for Fabric pipeline definitions. 2. Configure branch policy on main: require pull request reviews, minimum 2 approvers (business + governance), status checks passing. 3. Create staging deployment pipeline: trigger on PR approval, deploy to staging environment. 4. Add pre-deployment gate before staging: auto-approve (runs tests, validates syntax). 5. Add pre-deployment gate before production: manual approval, allow only specific users (data governance team). Require justification/description. 6. Create business approval gate: HR manager reviews transformations, confirms correctness. 7. Track approvals: Azure DevOps automatically logs timestamp, approver identity, comments. 8. Reject approvals include mandatory reason. 9. Create dashboard: count deployments, approval time metrics, rejection rates. 10. Quarterly review: analyze approval bottlenecks, optimize process.",
      "governanceConsiderations": "Define who can request, approve, and reject changes: business sponsor (business approval), data governance (compliance approval). Document approval criteria: business approval verifies transformations match requirements, governance approval verifies RLS, lineage, data quality, compliance. Implement escalation path for urgent changes (e.g., 24-hour SLA for critical bug fixes). Audit approval logs monthly. Require documented change rationale in pull request. Disallow approval from same person who submitted change (dual-approval enforced technically).",
      "peopleAnalyticsUseCases": [
        "Data engineer submits PR changing employee salary aggregation logic (e.g., fixing bonus calculation). Business sponsor (payroll manager) approves confirming calculation is correct. Data governance approves confirming aggregation preserves privacy (k>=5). Production deployment occurs only after both approvals.",
        "New HR dataset onboarded to Lakehouse: ingestion pipeline PR submitted. Business sponsor approves confirming data matches HRIS system. Data governance approves confirming sensitivity labels assigned, lineage documented. Both approvals required before prod activation.",
        "Urgent hotfix: salary dashboard showing incorrect totals. Change submitted with 'urgent' flag. Both approvers pinged, typically respond within 2-4 hours. Production deployment after both approvals."
      ],
      "complexity": "Medium",
      "maturity": "Emerging",
      "compatibleWith": [
        "deployment-pipelines",
        "audit-siem-integration"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "change-management",
        "devops",
        "approval",
        "governance",
        "four-eyes"
      ],
      "referenceLinks": [
        {
          "label": "Azure DevOps Release Gates and Approvals",
          "url": "https://learn.microsoft.com/en-us/azure/devops/pipelines/release/approvals/approvals"
        },
        {
          "label": "Fabric Deployment Pipelines",
          "url": "https://learn.microsoft.com/en-us/fabric/cicd/deployment-pipelines/intro"
        },
        {
          "label": "Git Branching Strategy and Pull Request Reviews",
          "url": "https://learn.microsoft.com/en-us/azure/devops/repos/git/branch-policies"
        }
      ],
      "estimatedImplementationEffort": "3-4 weeks",
      "costImplications": "Azure DevOps: free for up to 5 users/project. Git: no cost. Slack/Teams notifications: included. Process overhead: ~1-2 hours per change for approvals."
    },
    {
      "id": "cross-border-data-residency",
      "name": "Cross-Border Data Residency Isolation",
      "domain": "Data Sharing and Distribution",
      "domainId": 8,
      "category": "Data Sovereignty",
      "summary": "Isolates employee data by geography using multi-geo Fabric capacities, ensuring Canadian employee data remains in Canada and US data in US, with aggregate-only cross-border reporting.",
      "description": "Cross-border data residency ensures that employee personal data is stored and processed only within specified geographies per regulatory requirements. For multinational HR analytics, Canadian employees' data (salary, SSN, benefits) must remain in Canada due to PIPEDA. US data must remain in US due to state regulations and employer obligations. Fabric multi-geo capacity configuration assigns workspaces to specific regions; OneLake data is geo-pinned to that region. Cross-border reporting uses aggregate-only views (k-anonymity approach) ensuring individual records cannot be queried across borders. Data sharing uses Shortcuts with filtering, preventing cross-border record-level export. Periodic reviews ensure no data has migrated across borders. Audit logs track cross-border access attempts.",
      "fabricComponents": [
        "Fabric Capacity",
        "OneLake",
        "Workspace Assignment",
        "Multi-Geo Configuration"
      ],
      "pros": [
        "Meets regulatory data residency requirements (PIPEDA, GDPR, local laws) preventing costly compliance violations.",
        "Enables multi-country operations with confidence that data stays in authorized regions.",
        "Provides operational resilience: country-level outage affects only that region's operations, not global."
      ],
      "cons": [
        "Multi-region capacities increase costs ~2-3x vs. single-region (separate capacity per region).",
        "Analytics across regions requires federated queries or cross-border aggregates; complex joins are impossible.",
        "Data migration for employee moves (e.g., employee relocates from Canada to US) requires careful handling: old records deletion or transfer."
      ],
      "usageInstructions": "1. Create separate Fabric Capacity per geography: Canadian Capacity (Canada Central region), US Capacity (US East 2 region). 2. Create separate workspaces per geography: 'HR-Analytics-CA' in Canadian capacity, 'HR-Analytics-US' in US capacity. 3. Ingest employee data to geo-pinned Lakehouse: Canadian employee table in Canada workspace, US employee table in US workspace. 4. For cross-border reporting, create aggregate-only views: SELECT department, YEAR(dob) as year_of_birth, COUNT(*) as employee_count FROM employees WHERE country='CA' GROUP BY department, YEAR(dob) HAVING COUNT(*) >= 5. 5. Create federated semantic models: Power BI connects to Canadian and US aggregate views, combines aggregates (no record-level data). 6. Prevent cross-border shortcuts: Workspace Sharing > restrict shortcuts to same-region workspaces. 7. Audit cross-border access: Log Analytics tracks queries across regions, alert on suspicious activity. 8. Data migration procedure: terminating employee, update country field, archive to historical table in original region, do not migrate raw records.",
      "governanceConsiderations": "Establish data residency governance committee with legal, compliance, and HR stakeholders per country. Document residency requirements by jurisdiction (PIPEDA, GDPR, state laws). Implement technical enforcement: prevent shortcuts crossing regions, audit logs for attempted cross-border access. Quarterly audit: verify no employee data exists in wrong region. Data transfer agreements: document how employee relocations are handled. Secure deletion: ensure migrated data is permanently deleted from source region.",
      "peopleAnalyticsUseCases": [
        "Canadian bank with Canadian HQ + US subsidiary: employees are separated by capacity/workspace. Canadian headquarters views all Canadian employee analytics in Canadian workspace (compliant with PIPEDA). US subsidiary views US employee analytics in US workspace. Joint reporting uses aggregates: total Canadian headcount + total US headcount, no cross-border record-level joins.",
        "Multinational tech company with employees in Canada, US, and EU: separate capacities for each region. Annual global report aggregates at country-group level: 'Canada 5000 employees, US 10000 employees, EU 3000 employees' without exposing individual records.",
        "Employee relocation: Canadian employee transfers to US. Old record in Canadian Lakehouse is soft-deleted (marked inactive), no migration to US workspace. US onboarding creates new record in US workspace for same employee."
      ],
      "complexity": "High",
      "maturity": "GA",
      "compatibleWith": [
        "medallion-architecture",
        "encryption-at-rest-cmk",
        "disaster-recovery-geo"
      ],
      "incompatibleWith": [],
      "prerequisites": [],
      "tags": [
        "data-residency",
        "compliance",
        "gdpr",
        "pipeda",
        "sovereignty"
      ],
      "referenceLinks": [
        {
          "label": "Microsoft Fabric Multi-Geo Capabilities",
          "url": "https://learn.microsoft.com/en-us/fabric/admin/capacity-multi-geo"
        },
        {
          "label": "PIPEDA Data Residency Requirements",
          "url": "https://learn.microsoft.com/en-us/compliance/regulatory/offering-pipeda"
        },
        {
          "label": "GDPR Data Residency and Transfers",
          "url": "https://learn.microsoft.com/en-us/compliance/regulatory/gdpr-data-location"
        }
      ],
      "estimatedImplementationEffort": "6-8 weeks",
      "costImplications": "Multi-region Fabric Capacity: 2-3x capacity cost (separate capacity per region). Premium P1 capacity: $16/hour per region. Cross-border OneLake replication: $0.02/GB/month."
    }
  ],
  "useCaseBlueprints": [
    {
      "id": "attrition-risk-scoring",
      "name": "Attrition Risk Scoring & Fairness Monitoring",
      "category": "Talent Management",
      "description": "Weekly ML-based attrition risk scoring that identifies high-risk employees before they leave. Incorporates fairness monitoring to ensure predictions are unbiased across demographics, with automated alerts to managers for at-risk talent.",
      "businessValue": "Reduces unwanted attrition by enabling proactive retention interventions. Fairness monitoring ensures equity in talent management decisions across all employee demographics, reducing legal and reputational risk.",
      "patternFlow": [
        {
          "order": 1,
          "patternId": "medallion-architecture",
          "role": "Establishes Bronze-Silver-Gold data layers for clean, aggregated HR data",
          "dataFlow": "Raw HR systems \u2192 Bronze \u2192 enriched employee records \u2192 Silver"
        },
        {
          "order": 2,
          "patternId": "feature-store-delta",
          "role": "Creates reusable ML features (tenure, engagement, salary growth) for model training",
          "dataFlow": "Silver layer \u2192 feature engineering \u2192 feature store with Delta versioning"
        },
        {
          "order": 3,
          "patternId": "batch-inference-pipeline",
          "role": "Runs weekly ML inference to score attrition risk for all employees",
          "dataFlow": "Feature store \u2192 ML model \u2192 risk scores per employee"
        },
        {
          "order": 4,
          "patternId": "fairness-bias-evaluation",
          "role": "Evaluates predictions across demographic groups to identify and flag bias",
          "dataFlow": "Risk scores + demographics \u2192 fairness metrics \u2192 bias report"
        },
        {
          "order": 5,
          "patternId": "row-level-security",
          "role": "Restricts manager access to only their direct reports' risk scores",
          "dataFlow": "Risk scores filtered by manager org hierarchy in semantic layer"
        },
        {
          "order": 6,
          "patternId": "directlake-power-bi",
          "role": "Enables real-time Power BI dashboards on risk scores with no copy",
          "dataFlow": "Gold layer tables \u2192 DirectLake \u2192 Power BI dashboards"
        },
        {
          "order": 7,
          "patternId": "data-activator-reflex",
          "role": "Triggers automated alerts when risk scores exceed thresholds",
          "dataFlow": "Risk scores \u2192 Reflex rules \u2192 alerts to HR and managers"
        }
      ],
      "architectureNarrative": "This blueprint orchestrates a complete ML-powered attrition prevention system. Employee data flows through the medallion architecture's Bronze and Silver layers where it's enriched and deduplicated. The feature store then engineers critical ML features (tenure, compensation trends, engagement indicators, promotion history) with full Delta version control for reproducibility. A weekly batch inference pipeline scores all employees using a trained attrition model, producing a risk percentile for each employee. Critically, the fairness evaluation pattern runs post-inference to audit the model's predictions across demographic groups (gender, age, ethnicity) and flags any disparate impact. These risk scores are modeled in the semantic layer with row-level security (RLS) enforcing that managers see only their team members' scores, while HR leadership sees organization-wide patterns. Power BI dashboards built on DirectLake provide zero-latency visualization of risk distributions, trends, and demographic breakdowns. Finally, Data Activator Reflex rules trigger real-time alerts whenever an employee's risk score crosses critical thresholds (e.g., >80th percentile), sending notifications to managers and HR business partners with recommended retention actions.",
      "requiredGovernance": [
        "Model validation and testing for bias before deployment",
        "Quarterly fairness audits across protected classes",
        "Data residency compliance for sensitive PII in feature store",
        "Access logging for all users viewing risk scores",
        "Regular model retraining and performance monitoring",
        "Documentation of fairness thresholds and intervention triggers"
      ],
      "estimatedTimeline": "8-12 weeks for initial deployment including model training and fairness validation",
      "stakeholders": [
        "Chief HR Officer",
        "Head of Talent",
        "HR Analytics Team",
        "Data Science Team",
        "Compliance & Legal",
        "IT Security"
      ],
      "editable": true
    },
    {
      "id": "compensation-benchmarking",
      "name": "Secure Compensation Analytics & Market Benchmarking",
      "category": "Compensation & Benefits",
      "description": "Enables compensation analysis and market benchmarking while protecting individual salary data through k-anonymity and dynamic masking. Allows cross-department sharing of anonymized salary insights for equitable pay decisions.",
      "businessValue": "Supports competitive compensation strategy and pay equity initiatives without exposing individual salaries. Reduces legal risk from inadvertent salary disclosure while enabling data-driven compensation decisions across the organization.",
      "patternFlow": [
        {
          "order": 1,
          "patternId": "medallion-architecture",
          "role": "Layers salary, role, and demographics data for secure processing",
          "dataFlow": "Payroll systems \u2192 Bronze \u2192 normalized compensation records \u2192 Silver"
        },
        {
          "order": 2,
          "patternId": "scd-type-2",
          "role": "Tracks salary history and job changes over time with effective dating",
          "dataFlow": "Salary updates \u2192 SCD Type 2 \u2192 historical snapshots with valid_from/valid_to"
        },
        {
          "order": 3,
          "patternId": "anonymization-k-anonymity",
          "role": "Generalizes data so groups of 5+ employees are indistinguishable",
          "dataFlow": "Detailed salary records \u2192 quasi-identifiers generalization \u2192 anonymized salary groups"
        },
        {
          "order": 4,
          "patternId": "dynamic-data-masking",
          "role": "Real-time masking of exact salaries based on viewer permissions in Power BI",
          "dataFlow": "Salary values \u2192 masking rules applied per user role \u2192 aggregated view only"
        },
        {
          "order": 5,
          "patternId": "row-level-security",
          "role": "Ensures users see only compensation data for their department",
          "dataFlow": "Salary data filtered by org hierarchy in semantic model"
        },
        {
          "order": 6,
          "patternId": "certified-semantic-model",
          "role": "Defines trusted salary metrics (median, quartiles, benchmarks) for consistent reporting",
          "dataFlow": "Anonymized salary data \u2192 certified model \u2192 trusted metrics"
        },
        {
          "order": 7,
          "patternId": "directlake-power-bi",
          "role": "Delivers interactive compensation dashboards with benchmark comparisons",
          "dataFlow": "Semantic model \u2192 DirectLake \u2192 Power BI dashboards"
        }
      ],
      "architectureNarrative": "This blueprint creates a secure compensation analytics platform that protects individual privacy while enabling strategic insights. Payroll data enters the medallion architecture and is normalized in the Silver layer, then SCD Type 2 tracking captures salary history with effective dates to support trend analysis. The k-anonymity pattern generalizes quasi-identifiers (job level, department, age group, tenure band) so that no group of fewer than 5 employees can be uniquely identified, enabling safe sharing between departments. Dynamic data masking then provides an additional layer by allowing department leaders to see aggregate compensation metrics and benchmarks without viewing exact individual salaries. The semantic layer enforces row-level security so leaders see only their department's anonymized compensation data, while HR compensation specialists see organization-wide patterns. The certified semantic model codifies trusted salary metrics (median, 25th/75th percentiles, market benchmarks) that drive consistent reporting across the organization. Power BI dashboards leverage DirectLake to deliver interactive compensation analysis including pay equity ratios, market positioning by level, and salary distribution trends, all on anonymized data.",
      "requiredGovernance": [
        "Annual k-anonymity validation to ensure sufficient group sizes",
        "Compensation committee review of benchmark decisions",
        "Audit logging for all compensation data access",
        "Regular review of masking rules to ensure appropriate confidentiality levels",
        "Compliance with pay equity regulations (e.g., UK Gender Pay Gap, California pay transparency)",
        "Data retention policies for historical salary records"
      ],
      "estimatedTimeline": "6-8 weeks for initial setup including anonymization validation and masking configuration",
      "stakeholders": [
        "Chief HR Officer",
        "Compensation & Benefits Lead",
        "HR Analytics Team",
        "HR Business Partners",
        "Finance",
        "Legal & Compliance"
      ],
      "editable": true
    },
    {
      "id": "regulatory-dei-reporting",
      "name": "Automated Regulatory DEI Reporting",
      "category": "DEI & Reporting",
      "description": "Automates diversity, equity, and inclusion reporting for regulatory compliance (EEO-1, pay equity audits). Generates paginated compliance reports with sensitivity labeling and enforces data loss prevention policies.",
      "businessValue": "Reduces compliance risk and reporting burden by automating DEI metrics generation. Ensures data accuracy and timeliness for regulatory submissions while protecting sensitive diversity data from unauthorized access.",
      "patternFlow": [
        {
          "order": 1,
          "patternId": "medallion-architecture",
          "role": "Consolidates HR data from multiple sources into trusted layers",
          "dataFlow": "Multiple HRIS systems \u2192 Bronze \u2192 deduplicated employee records \u2192 Silver"
        },
        {
          "order": 2,
          "patternId": "data-quality-validation",
          "role": "Validates demographic data completeness and accuracy before reporting",
          "dataFlow": "Silver layer records \u2192 quality checks \u2192 flagged missing/invalid demographics"
        },
        {
          "order": 3,
          "patternId": "anonymization-k-anonymity",
          "role": "Generalizes demographic data for internal sharing while protecting individual privacy",
          "dataFlow": "Raw demographics \u2192 quasi-identifier generalization \u2192 anonymized diversity metrics"
        },
        {
          "order": 4,
          "patternId": "paginated-reports",
          "role": "Generates formatted EEO-1 and pay equity audit reports with regulatory structure",
          "dataFlow": "Anonymized metrics \u2192 template-driven report generation \u2192 paginated PDF/Excel"
        },
        {
          "order": 5,
          "patternId": "sensitivity-labels",
          "role": "Marks reports as containing regulated diversity data requiring protection",
          "dataFlow": "DEI metrics \u2192 sensitivity label application \u2192 high/medium classification"
        },
        {
          "order": 6,
          "patternId": "dlp-policy-enforcement",
          "role": "Prevents accidental sharing or export of DEI reports outside secure channels",
          "dataFlow": "Reports with sensitivity labels \u2192 DLP rules \u2192 block unauthorized export/sharing"
        }
      ],
      "architectureNarrative": "This blueprint automates compliance reporting for diversity and equity metrics required by regulators like the EEOC (EEO-1) and pay equity audit bodies. Employee demographic and compensation data flows through the medallion architecture's Bronze and Silver layers where it's deduplicated and standardized. Data quality validation patterns ensure demographic fields are complete and accurately coded before being included in compliance reports. The anonymization pattern generalizes demographic categories to safe levels for internal sharing (e.g., age bands, tenure bands) while preserving aggregate diversity metrics. Paginated report generation creates formatted reports matching EEO-1 structure and pay equity audit requirements, with automated calculations of diversity percentages, pay ratios, and representation metrics. Sensitivity labels mark these reports as containing regulated data, and DLP policies prevent unauthorized copying, printing, or external sharing of the reports, ensuring only intended recipients (legal, CHRO, board) access compliance documentation.",
      "requiredGovernance": [
        "Annual validation of demographic data against source systems",
        "Legal review of compliance report templates before deployment",
        "Audit trail of all report generations and distributions",
        "Secure storage and retention of compliance reports",
        "Regular DLP policy updates to align with evolving regulations",
        "Executive sign-off on DEI metrics before external submission"
      ],
      "estimatedTimeline": "8-10 weeks including regulatory template validation and DLP policy configuration",
      "stakeholders": [
        "Chief Legal Officer",
        "Chief HR Officer",
        "Diversity & Inclusion Leader",
        "HR Analytics Team",
        "Compliance Officer",
        "Board Secretary"
      ],
      "editable": true
    },
    {
      "id": "secure-hr-chatbot",
      "name": "Conversational HR Analytics with Secure Retrieval",
      "category": "Employee Experience",
      "description": "Deploys a conversational AI agent that answers HR analytics questions grounded in Fabric data, with row-level security enforced per user. Employees can ask questions about benefits, compensation benchmarks, and organizational metrics with confidence their data access is governed.",
      "businessValue": "Improves HR service delivery and employee self-service by enabling natural language access to HR analytics. Reduces HR team support tickets while maintaining strict data governance and ensuring users only see data they're authorized to access.",
      "patternFlow": [
        {
          "order": 1,
          "patternId": "medallion-architecture",
          "role": "Provides clean HR data foundation for RAG context retrieval",
          "dataFlow": "HR systems \u2192 Bronze \u2192 enriched records \u2192 Silver/Gold tables"
        },
        {
          "order": 2,
          "patternId": "certified-semantic-model",
          "role": "Defines consistent HR metrics and relationships for LLM grounding",
          "dataFlow": "Gold layer tables \u2192 semantic model definitions \u2192 trusted metrics"
        },
        {
          "order": 3,
          "patternId": "rag-fabric-grounded",
          "role": "Grounds LLM responses in actual Fabric data, preventing hallucinations",
          "dataFlow": "User question \u2192 semantic model query \u2192 LLM + context \u2192 grounded response"
        },
        {
          "order": 4,
          "patternId": "secure-chat-rls",
          "role": "Enforces row-level security within chat context, restricting data per user",
          "dataFlow": "Query context \u2192 RLS rules applied \u2192 filtered data \u2192 LLM response"
        },
        {
          "order": 5,
          "patternId": "hr-ai-guardrails",
          "role": "Validates LLM responses and blocks harmful outputs before delivery",
          "dataFlow": "LLM response \u2192 guardrail validation \u2192 approved/rejected"
        },
        {
          "order": 6,
          "patternId": "audit-siem-integration",
          "role": "Logs all user questions and data accessed through chatbot for compliance",
          "dataFlow": "Chat interactions \u2192 audit logs \u2192 SIEM \u2192 compliance reporting"
        }
      ],
      "architectureNarrative": "This blueprint creates a secure, governed conversational interface to HR analytics. The medallion architecture provides clean, trusted HR data across Bronze/Silver/Gold layers. A certified semantic model defines consistent HR metrics and relationships (e.g., salary bands, benefit eligibility rules, organizational structures) that serve as ground truth for the LLM. When a user asks a question like 'What's the compensation benchmark for my role?', the RAG pattern grounds the LLM with actual data from the semantic model, ensuring answers are factual rather than hallucinated. Critically, row-level security is enforced within the chatbot context\u2014a user only retrieves data they're authorized to see (e.g., their department's benchmarks, never peer salaries). HR AI guardrails validate the LLM response to ensure it's accurate, non-discriminatory, and policy-compliant before it's returned to the user. All interactions are logged in the audit SIEM system, creating a compliance trail that tracks which users asked what questions and what data was retrieved, enabling HR and security teams to audit chatbot activity.",
      "requiredGovernance": [
        "LLM response validation and guardrail testing before deployment",
        "Regular audit of chatbot interactions for policy violations",
        "Role-based access control for sensitive HR topics (e.g., executive compensation)",
        "Data residency compliance for all retrieved employee data",
        "Incident response procedure for misclassified or harmful responses",
        "User education on appropriate use of HR chatbot"
      ],
      "estimatedTimeline": "10-14 weeks including LLM fine-tuning, guardrail development, and security validation",
      "stakeholders": [
        "Chief HR Officer",
        "HR Analytics Lead",
        "AI/ML Engineering",
        "IT Security",
        "Data Governance",
        "Legal & Compliance"
      ],
      "editable": true
    },
    {
      "id": "employee-lifecycle-360",
      "name": "Complete Employee Lifecycle Tracking",
      "category": "Workforce Planning",
      "description": "Tracks employees through their complete lifecycle from hire to separation with full historical audit trail. Uses slowly changing dimensions to capture career progression, compensation changes, and organizational moves, enabling cohort analysis and retention studies.",
      "businessValue": "Provides complete visibility into employee tenure and progression, enabling analytics on retention, promotion fairness, and career path effectiveness. Historical tracking supports litigation defense and succession planning.",
      "patternFlow": [
        {
          "order": 1,
          "patternId": "incremental-watermark",
          "role": "Captures only new/changed records from HR systems for efficient daily loading",
          "dataFlow": "HR system changes \u2192 watermark tracking \u2192 incremental extraction"
        },
        {
          "order": 2,
          "patternId": "scd-type-2",
          "role": "Maintains complete history of role, level, department, and salary changes",
          "dataFlow": "Employee updates \u2192 SCD Type 2 \u2192 historical snapshots with effective dates"
        },
        {
          "order": 3,
          "patternId": "medallion-architecture",
          "role": "Layers lifecycle data for analytics with audit trail",
          "dataFlow": "SCD tables \u2192 Bronze \u2192 enriched employee journey \u2192 Silver \u2192 analytics ready \u2192 Gold"
        },
        {
          "order": 4,
          "patternId": "data-quality-validation",
          "role": "Validates date sequences and data consistency across lifecycle events",
          "dataFlow": "Lifecycle records \u2192 quality checks \u2192 flagged date anomalies/gaps"
        },
        {
          "order": 5,
          "patternId": "hub-spoke-workspace",
          "role": "Organizes lifecycle analytics into shared workspace for cross-functional teams",
          "dataFlow": "Gold layer tables \u2192 shared semantic models \u2192 distributed teams"
        },
        {
          "order": 6,
          "patternId": "directlake-power-bi",
          "role": "Visualizes employee journey and cohort trends in interactive dashboards",
          "dataFlow": "Gold tables \u2192 DirectLake \u2192 Power BI career path visualizations"
        },
        {
          "order": 7,
          "patternId": "metrics-scorecard",
          "role": "Tracks key lifecycle metrics (time-to-productivity, promotion rate, retention rate)",
          "dataFlow": "Lifecycle events \u2192 metric calculations \u2192 scorecard updates"
        }
      ],
      "architectureNarrative": "This blueprint creates a comprehensive employee lifecycle analytics system with full audit trail. Daily extractions use incremental watermarking to efficiently capture only changes from the HR system (new hires, promotions, transfers, separations). Slowly Changing Dimension Type 2 modeling captures every career event with effective dates, preserving complete history\u2014an employee's SCD table might show them as 'Junior Analyst' in Boston from 2020-2021, 'Senior Analyst' in Boston from 2021-2023, 'Manager' in New York from 2023-present. This historical data flows through the medallion architecture's Bronze layer where it's deduplicated, then the Silver layer where career progression is enriched with performance data and compensation history, and finally the Gold layer where it's structured for analytics. Data quality validation ensures date sequences are logical (e.g., no negative tenure, start dates before hire dates). A hub-spoke workspace structure organizes these analytics so that talent leaders, compensation teams, and succession planners can each build on shared, trusted Gold layer tables. Power BI dashboards visualize employee journeys including time-in-role, promotion velocity, and departmental migration patterns. Finally, a metrics scorecard continuously calculates lifecycle KPIs (time-to-full-productivity for new hires, average promotion cycle, voluntary vs. involuntary separation rates), enabling leaders to monitor workforce health.",
      "requiredGovernance": [
        "Audit trail preservation for separation events (legal hold periods)",
        "Data retention policies aligned with employment law requirements",
        "Access controls for sensitive lifecycle stages (medical leave, disciplinary action)",
        "Validation of date fields against HR system source of truth",
        "Regular reconciliation of headcount snapshots against official counts",
        "Documentation of SCD effective date logic for reproducibility"
      ],
      "estimatedTimeline": "8-10 weeks for SCD modeling, incremental load setup, and validation",
      "stakeholders": [
        "Chief HR Officer",
        "Talent Director",
        "HR Analytics Team",
        "Succession Planning Lead",
        "Legal & Compliance",
        "Finance"
      ],
      "editable": true
    },
    {
      "id": "executive-comp-isolation",
      "name": "Executive Compensation Data with Maximum Security Isolation",
      "category": "Compensation & Benefits",
      "description": "Isolates executive compensation data (C-suite, board) with maximum security controls including encryption, network isolation, privileged access management, and comprehensive audit logging. Only pre-approved executives and compensation committee members access this data.",
      "businessValue": "Protects highly sensitive executive data from unauthorized access, theft, or disclosure while enabling legitimate compensation governance and board oversight. Prevents insider trading through audit trails and demonstrates security controls to boards and regulators.",
      "patternFlow": [
        {
          "order": 1,
          "patternId": "encryption-at-rest-cmk",
          "role": "Encrypts executive compensation data using customer-managed keys",
          "dataFlow": "Executive salary/bonus data \u2192 CMK encryption \u2192 encrypted storage"
        },
        {
          "order": 2,
          "patternId": "network-isolation-private-links",
          "role": "Routes executive compensation workspace through private network endpoints",
          "dataFlow": "Authorized users \u2192 private network links \u2192 isolated workspace"
        },
        {
          "order": 3,
          "patternId": "row-level-security",
          "role": "Restricts executive records to only approved compensation committee members",
          "dataFlow": "Executive comp data \u2192 RLS filter \u2192 authorized users only"
        },
        {
          "order": 4,
          "patternId": "dynamic-data-masking",
          "role": "Masks specific compensation elements (base, bonus, equity) based on role",
          "dataFlow": "Full compensation records \u2192 masking rules \u2192 redacted view"
        },
        {
          "order": 5,
          "patternId": "privileged-access-management",
          "role": "Requires just-in-time approval and audit for access to executive compensation",
          "dataFlow": "Access request \u2192 approval workflow \u2192 time-limited access \u2192 audit log"
        },
        {
          "order": 6,
          "patternId": "audit-siem-integration",
          "role": "Logs all executive compensation access and data changes with SIEM monitoring",
          "dataFlow": "All actions on executive data \u2192 detailed audit logs \u2192 SIEM alerts"
        },
        {
          "order": 7,
          "patternId": "paginated-reports",
          "role": "Generates formal executive compensation reports for board review",
          "dataFlow": "Authorized access \u2192 report generation \u2192 signed PDFs to board"
        }
      ],
      "architectureNarrative": "This blueprint implements maximum security isolation for executive compensation data, recognizing its strategic and sensitive nature. Executive compensation data is encrypted at rest using customer-managed keys (CMK) controlled by the organization's security team, not Microsoft. Network isolation routes access through private network endpoints (Azure Private Link), so authorized users connect through a private network backbone rather than the public internet, preventing packet sniffing or man-in-the-middle attacks. The semantic layer applies strict row-level security that ensures even Finance VP cannot see another executive's compensation. Dynamic masking layers on top, allowing some approved users to see aggregated benchmarks without seeing individual amounts. Privileged access management (PAM) wraps all of this in a just-in-time approval workflow\u2014a board member requesting executive compensation data must have that request approved by the board chair, and access is time-limited (e.g., 1 hour). Every action (access grant, data view, download) is logged with full audit detail (timestamp, user, IP, action) and streamed to the SIEM for monitoring and alerting on anomalous behavior (e.g., after-hours access). Finally, formal paginated reports are generated for board compensation committee review, with digital signatures and distribution tracking ensuring legal defensibility.",
      "requiredGovernance": [
        "CMK key rotation policies and segregation from standard keys",
        "Documented executive compensation access approval process",
        "Quarterly SIEM audit reviews for anomalous access patterns",
        "Incident response plan for unauthorized executive data access",
        "Board-level governance of executive compensation data policies",
        "Compliance with SOX auditor requirements for executive data controls"
      ],
      "estimatedTimeline": "10-12 weeks for security infrastructure, PAM setup, and control validation",
      "stakeholders": [
        "Chief Security Officer",
        "Chief HR Officer",
        "Board Compensation Committee",
        "General Counsel",
        "IT Infrastructure",
        "Internal Audit"
      ],
      "editable": true
    },
    {
      "id": "gdpr-pipeda-compliance",
      "name": "Privacy Compliance & Data Subject Request Fulfillment",
      "category": "Compliance & Risk",
      "description": "Implements full privacy compliance pipeline including data mapping, sensitivity labeling, DLP enforcement, and automated Data Subject Request (DSR) fulfillment. Ensures GDPR, PIPEDA, and other privacy regulations are met.",
      "businessValue": "Reduces compliance risk and legal exposure from privacy violations. Enables rapid, accurate response to data subject requests, demonstrating regulatory compliance and building customer/employee trust in data handling practices.",
      "patternFlow": [
        {
          "order": 1,
          "patternId": "purview-data-map",
          "role": "Catalogs all HR data sources and their lineage for privacy compliance",
          "dataFlow": "HR systems \u2192 Purview scanning \u2192 data asset catalog with classifications"
        },
        {
          "order": 2,
          "patternId": "sensitivity-labels",
          "role": "Tags PII, health data, and other regulated information with privacy labels",
          "dataFlow": "Catalog assets \u2192 sensitivity classification \u2192 labels applied"
        },
        {
          "order": 3,
          "patternId": "dlp-policy-enforcement",
          "role": "Blocks unauthorized export, copy, or sharing of privacy-labeled data",
          "dataFlow": "Labeled data \u2192 DLP rules \u2192 prevent exfiltration"
        },
        {
          "order": 4,
          "patternId": "data-retention-lifecycle",
          "role": "Automatically deletes data per privacy retention schedules after employee separation",
          "dataFlow": "Retention dates per regulation \u2192 automated deletion schedule \u2192 purged records"
        },
        {
          "order": 5,
          "patternId": "dsr-fulfillment",
          "role": "Orchestrates automated fulfillment of data subject access requests",
          "dataFlow": "DSR request received \u2192 data query \u2192 subject data compiled \u2192 encrypted delivery"
        },
        {
          "order": 6,
          "patternId": "audit-siem-integration",
          "role": "Logs all privacy-related data access and deletion for compliance audits",
          "dataFlow": "All privacy actions \u2192 audit logs \u2192 regulatory audit trail"
        }
      ],
      "architectureNarrative": "This blueprint implements a comprehensive privacy compliance system aligned with GDPR, PIPEDA, CCPA, and other global privacy regulations. Purview scans all HR data sources and builds a complete data asset catalog, identifying what employee data exists, where it's stored, and how it flows through the organization. Sensitivity labels are applied to all personal data (names, IDs, health information, biometrics), marking them as PII or regulated health data. DLP policies leverage these labels to prevent unauthorized exfiltration\u2014attempting to copy a table with PII-labeled columns triggers a policy block, and attempting to download or email sensitive data triggers alerts and blocks. Data retention lifecycle policies encode the legal retention requirements for different data types (e.g., payroll records retained 3 years, health records deleted upon separation after applicable hold periods), and automated deletion tasks purge data when retention periods expire. When a data subject (employee or applicant) submits a DSR request asking 'give me all my data,' the DSR fulfillment pattern orchestrates rapid compliance: it queries all systems in the catalog, compiles relevant employee records, creates a secure export, and delivers it to the requester via encrypted link within the required timeframe (typically 30 days). Comprehensive audit logging tracks all sensitive data access, modifications, and deletions, creating an audit trail that demonstrates compliance during regulatory inspections.",
      "requiredGovernance": [
        "Annual privacy impact assessment (DPIA) on HR data processing",
        "Documented data retention schedules aligned with jurisdiction requirements",
        "Privacy policy documentation specifying employee rights and procedures",
        "DSR fulfillment SLA and escalation procedures",
        "Regular DLP policy updates as privacy regulations evolve",
        "Privacy training for all HR and IT staff handling employee data"
      ],
      "estimatedTimeline": "10-14 weeks including data mapping, DLP configuration, and DSR automation setup",
      "stakeholders": [
        "Chief Privacy Officer",
        "General Counsel",
        "Chief HR Officer",
        "Data Governance",
        "IT Security",
        "Compliance Officer"
      ],
      "editable": true
    },
    {
      "id": "workforce-planning-ml",
      "name": "ML-Powered Workforce Planning & Headcount Forecasting",
      "category": "Workforce Planning",
      "description": "Uses ML to forecast future headcount needs, attrition, and hiring by role and department. Captures historical patterns and business drivers to enable data-driven workforce planning with confidence intervals.",
      "businessValue": "Improves strategic workforce planning accuracy and enables proactive hiring. Reduces over/under staffing costs and supports budget negotiations with data-driven headcount projections.",
      "patternFlow": [
        {
          "order": 1,
          "patternId": "medallion-architecture",
          "role": "Structures historical HR and business data for ML training",
          "dataFlow": "HR + business data \u2192 Bronze \u2192 enriched records \u2192 Silver \u2192 features \u2192 Gold"
        },
        {
          "order": 2,
          "patternId": "feature-store-delta",
          "role": "Engineers and versions time-series features (headcount, attrition, hiring rates, business drivers)",
          "dataFlow": "Silver records \u2192 feature engineering \u2192 feature store with Delta versioning"
        },
        {
          "order": 3,
          "patternId": "mlflow-model-registry",
          "role": "Manages model versions, tracks experiments, and promotes best-performing forecast model",
          "dataFlow": "Trained models \u2192 MLflow registry \u2192 versioned, promoted models"
        },
        {
          "order": 4,
          "patternId": "batch-inference-pipeline",
          "role": "Runs monthly forecast inference to predict headcount 12 months forward",
          "dataFlow": "Latest features \u2192 forecast model \u2192 12-month predictions per role"
        },
        {
          "order": 5,
          "patternId": "model-drift-detection",
          "role": "Monitors forecast accuracy and alerts when model performance degrades",
          "dataFlow": "Monthly actuals vs. forecasts \u2192 drift detection \u2192 retraining triggers"
        },
        {
          "order": 6,
          "patternId": "directlake-power-bi",
          "role": "Visualizes headcount forecasts with confidence intervals and scenario analysis",
          "dataFlow": "Forecast results \u2192 DirectLake \u2192 Power BI planning dashboards"
        },
        {
          "order": 7,
          "patternId": "data-activator-reflex",
          "role": "Alerts when forecasts differ significantly from plans",
          "dataFlow": "Forecast vs. budget \u2192 Reflex alerts \u2192 planning team notification"
        }
      ],
      "architectureNarrative": "This blueprint implements end-to-end ML-powered workforce planning. Historical HR data (headcount by role/level/department, attrition rates, hiring rates) flows through the medallion architecture to create a clean, enriched Silver layer. The feature store engineers time-series features capturing headcount trends, attrition seasonality, and business driver relationships (e.g., revenue growth correlation with hiring). MLflow manages multiple forecast model experiments (regression, time-series, ensemble), tracking hyperparameters, performance metrics, and artifacts. The best-performing model is promoted to production in the MLflow registry. A monthly batch inference pipeline applies this model to the latest feature set, generating 12-month headcount forecasts at the role/department level, including confidence intervals (e.g., 'we forecast 45 Software Engineers in 2027 Q2, 95% confidence interval 40-50'). Model drift detection continuously compares actual hiring and attrition against prior forecasts, alerting data scientists when forecast accuracy drops below acceptable thresholds, triggering retraining. Power BI dashboards layer forecasts over actual headcount, showing how well recent predictions performed and enabling scenario analysis (e.g., 'if attrition increases to 15%, what headcount impact?'). Data Activator rules alert the workforce planning team when actual headcount diverges meaningfully from forecasts, enabling them to adjust hiring plans or investigate root causes of variance.",
      "requiredGovernance": [
        "Quarterly model performance review and validation against actuals",
        "Documentation of forecast methodology for HR leadership and budget teams",
        "Confidence interval guidance to prevent over-interpretation of point forecasts",
        "Scenario planning governance and approval of 'what-if' analyses",
        "Regular retraining schedule (monthly or quarterly based on drift)",
        "Stakeholder alignment on forecast use in budget planning vs. commitments"
      ],
      "estimatedTimeline": "10-14 weeks including feature engineering, model experimentation, and validation",
      "stakeholders": [
        "Chief HR Officer",
        "Workforce Planning Lead",
        "Data Science Team",
        "Finance/Budget Planning",
        "Department Leaders"
      ],
      "editable": true
    },
    {
      "id": "cross-border-hr-analytics",
      "name": "Multi-Country HR Analytics with Data Residency Compliance",
      "category": "Compliance & Risk",
      "description": "Enables HR analytics across multiple countries while respecting data residency regulations. Routes employee data to country-specific storage regions, applies anonymization for cross-border aggregation, and maintains localized reporting.",
      "businessValue": "Supports global HR strategy while maintaining regulatory compliance in each jurisdiction. Enables scale without violating data localization laws, reducing risk of fines and operational restrictions.",
      "patternFlow": [
        {
          "order": 1,
          "patternId": "cross-border-data-residency",
          "role": "Routes employee data to country-specific Azure regions per regulation",
          "dataFlow": "France employee data \u2192 France region; Germany \u2192 Germany region; US \u2192 US region"
        },
        {
          "order": 2,
          "patternId": "medallion-architecture",
          "role": "Layers HR data independently in each country-specific workspace",
          "dataFlow": "Local HR systems \u2192 country workspace Bronze \u2192 Silver \u2192 Gold"
        },
        {
          "order": 3,
          "patternId": "anonymization-k-anonymity",
          "role": "Anonymizes data for safe cross-border aggregation to global headquarters",
          "dataFlow": "Country-level detailed data \u2192 anonymization \u2192 safe for aggregation"
        },
        {
          "order": 4,
          "patternId": "onelake-shortcuts",
          "role": "Creates virtual links to country-specific medallion tables without copying data",
          "dataFlow": "Local storage \u2192 OneLake shortcuts \u2192 global aggregation without duplication"
        },
        {
          "order": 5,
          "patternId": "certified-semantic-model",
          "role": "Defines consistent global HR metrics while respecting country models",
          "dataFlow": "Country semantic models \u2192 global model \u2192 consistent metrics"
        },
        {
          "order": 6,
          "patternId": "row-level-security",
          "role": "Ensures users see only data from their country or anonymized global summaries",
          "dataFlow": "User country context \u2192 RLS filter \u2192 country-scoped or anonymized data"
        },
        {
          "order": 7,
          "patternId": "directlake-power-bi",
          "role": "Delivers localized dashboards per country and global rollup dashboards",
          "dataFlow": "Country + global semantic models \u2192 DirectLake \u2192 localized Power BI"
        }
      ],
      "architectureNarrative": "This blueprint enables global HR analytics while respecting data residency regulations that require employee data to stay within specific countries (GDPR in EU, data localization in China, etc.). Each country maintains its own Azure workspace in a region that meets legal requirements\u2014France employee data stays in the France region, Germany employee data in the Germany region, etc. Within each country workspace, the medallion architecture (Bronze-Silver-Gold) independently layers HR data. For cross-border aggregation to global headquarters, the anonymization pattern generalizes employee records so that no individual is identifiable, enabling safe sharing of aggregate diversity metrics, compensation benchmarks, and organizational structures across borders. OneLake shortcuts create virtual links to country-specific Gold layer tables without physically moving data, allowing global dashboards to aggregate anonymized insights. The certified semantic model defines consistent global HR metrics (e.g., total headcount, attrition rate, average tenure) while each country maintains local model variations reflecting local employment law requirements. Row-level security ensures a France-based user sees detailed France data plus anonymized global rollups, while HQ sees only anonymized country-level summaries. Power BI dashboards are delivered in multiple flavors: country leaders use localized dashboards with detailed data; global HR leadership uses dashboards with anonymized country rollups and global trends.",
      "requiredGovernance": [
        "Legal review of data residency requirements per jurisdiction",
        "Documentation of data flow and storage locations for compliance audits",
        "Validation that cross-border anonymization meets local privacy standards",
        "Regular review of country-specific employment law changes",
        "Data transfer agreements (SCCs) for any cross-border metadata",
        "Incident response procedures for data residency violations"
      ],
      "estimatedTimeline": "12-16 weeks including workspace setup, region configuration, and compliance validation",
      "stakeholders": [
        "Chief HR Officer",
        "Chief Privacy Officer",
        "General Counsel",
        "Global HR Leaders",
        "IT Infrastructure",
        "Compliance Officer"
      ],
      "editable": true
    },
    {
      "id": "real-time-org-health",
      "name": "Real-Time Organizational Health Monitoring & Automated Escalation",
      "category": "Executive Analytics",
      "description": "Monitors organizational health metrics in real-time (headcount, engagement, turnover) with automated alerts and escalation routing. Executives see live dashboards with automatic notification of critical changes.",
      "businessValue": "Enables rapid response to organizational health crises (sudden attrition spike, engagement drop) before they become serious. Reduces time spent in status meetings by automating metric delivery and flagging critical issues.",
      "patternFlow": [
        {
          "order": 1,
          "patternId": "cdc-change-capture",
          "role": "Captures real-time employee data changes (terminations, promotions, absences) from HR system",
          "dataFlow": "HR system changes \u2192 CDC \u2192 streaming events"
        },
        {
          "order": 2,
          "patternId": "medallion-architecture",
          "role": "Ingests real-time changes into medallion architecture for aggregation",
          "dataFlow": "CDC events \u2192 Bronze real-time layer \u2192 Silver aggregated metrics \u2192 Gold health scorecards"
        },
        {
          "order": 3,
          "patternId": "data-quality-validation",
          "role": "Validates real-time data quality and flags anomalies",
          "dataFlow": "Real-time metrics \u2192 quality checks \u2192 flagged inconsistencies"
        },
        {
          "order": 4,
          "patternId": "metrics-scorecard",
          "role": "Calculates org health KPIs (headcount, attrition rate, engagement score) in real-time",
          "dataFlow": "Real-time employee events \u2192 metric aggregation \u2192 live scorecard"
        },
        {
          "order": 5,
          "patternId": "data-activator-reflex",
          "role": "Evaluates health metrics against thresholds and triggers alerts",
          "dataFlow": "Real-time metrics \u2192 threshold evaluation \u2192 alert triggers"
        },
        {
          "order": 6,
          "patternId": "power-automate-triggers",
          "role": "Initiates notification and escalation workflows based on alert severity",
          "dataFlow": "Alert \u2192 severity assessment \u2192 notification workflow"
        },
        {
          "order": 7,
          "patternId": "escalation-routing",
          "role": "Routes critical alerts to appropriate leaders based on escalation rules",
          "dataFlow": "Alert + severity + scope \u2192 routing rules \u2192 CHRO, CFO, COO notification"
        }
      ],
      "architectureNarrative": "This blueprint creates a real-time organizational health monitoring system that keeps executives informed of critical workforce changes. Change Data Capture (CDC) from the HR system streams real-time events\u2014employee terminations, promotions, unplanned absences, new hires\u2014creating a real-time feed of workforce changes. These events feed into the medallion architecture's Bronze layer in real-time, where they're deduplicated and enriched. The Silver layer continuously aggregates these events to maintain real-time metrics: current headcount by department, rolling 30-day attrition count, unplanned absence rate. The Gold layer calculates high-level org health scorecards combining multiple metrics into composite health signals (e.g., org health score from 0-100 factoring headcount stability, attrition trends, and engagement indicators). A metrics scorecard layer exposes key metrics (total headcount, monthly attrition count, engagement index) for real-time consumption. Data Activator continuously evaluates these metrics against business thresholds (e.g., 'alert if daily attrition count > 5', 'alert if any department headcount drops >10%'), and when thresholds are breached, fires alerts. Power Automate workflows respond to alerts, routing escalations based on severity and scope: a critical alert (e.g., VP-level termination) routes immediately to the CHRO and CEO; a moderate alert (e.g., headcount drop in a department) routes to the department head and HR business partner. Escalation routing ensures time-sensitive alerts reach the right people within minutes, enabling rapid response.",
      "requiredGovernance": [
        "Threshold definition and approval by HR leadership for all alerts",
        "Escalation routing governance and approval by executive team",
        "Data quality validation rules for real-time metrics before alerting",
        "Alert fatigue monitoring\u2014regular review of false positive rates",
        "Privacy controls for real-time termination alerts",
        "On-call rotation and response SLA for critical org health alerts"
      ],
      "estimatedTimeline": "8-10 weeks including CDC setup, threshold definition, and escalation automation",
      "stakeholders": [
        "Chief HR Officer",
        "Chief Financial Officer",
        "Chief Operations Officer",
        "IT Infrastructure",
        "HR Analytics Team",
        "Business Continuity"
      ],
      "editable": true
    }
  ]
}